{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29d3b3fe",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"./images/PallenceAI-Final.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccee792",
   "metadata": {},
   "source": [
    "# Transfer Learning: Leveraging Pretrained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d95557",
   "metadata": {},
   "source": [
    "### Transfer Learning can be done in many different ways depending on the task at hand\n",
    "\n",
    "1.  Extract the features from pretrained models and use that as a starting point for our training \n",
    "\n",
    "\n",
    "2.  Fine tune the pretrained model \n",
    "    * By freezing the convbase of the model\n",
    "    * **By freezing some layers and unfreezing some layers**\n",
    "    * By fine-tuning all the layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106309e4",
   "metadata": {},
   "source": [
    "## Fine tune on Select Layers of VGG16 Architecture below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0d5487",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"./images/vgg16-3.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162adc49",
   "metadata": {},
   "source": [
    "**Import needed libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a9babbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic Python packages for data wrangling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "#Tensorflow & Keras related packages\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "#from keras import layers\n",
    "\n",
    "from utils import plot_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50a78ca",
   "metadata": {},
   "source": [
    "### Load Cifar Dataset Preloaded in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24f0320c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14d20b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f76f9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the last 10,000 images from training to create a validation dataset\n",
    "val_images = train_images[40000:]\n",
    "val_labels = train_labels[40000:]\n",
    "\n",
    "# And use the first 40,000 images for training\n",
    "train_images = train_images[:40000]\n",
    "train_labels = train_labels[:40000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36d9f24",
   "metadata": {},
   "source": [
    "### Preprocess the image by\n",
    "1. Resizing the image and use preprocessing which the vgg16 model expectss\n",
    "2. convert our data (train_images, train_labels) into tf.data object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b45b573d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function preprocess the images as per the requirements of vgg16 pretrained model\n",
    "\n",
    "def preprocess_image(image, label):\n",
    "    image = tf.image.resize(image, [224, 224])  # Resize the image\n",
    "    image = keras.applications.vgg16.preprocess_input(image)\n",
    "    \n",
    "    #image = image / 255.0  # We were scaling our images like this before\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fce57b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function converts training data into a Tensorflow data objects, \n",
    "#...preprocesses and organizes data into batches \n",
    "# Ensures data is loaded efficiently through prefetching. \n",
    "\n",
    "def make_dataset(images, labels, batch_size=32):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images, labels)) #tf.data object for efficient data loading & preprocessing\n",
    "    dataset = dataset.map(preprocess_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "243b556d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparocess all the datasets\n",
    "train_dataset = make_dataset(train_images, train_labels, batch_size=32)\n",
    "test_dataset = make_dataset(test_images, test_labels, batch_size=32)\n",
    "val_dataset = make_dataset(val_images, val_labels, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c881a7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1), dtype=tf.uint8, name=None))>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6bc171",
   "metadata": {},
   "source": [
    "### Load  VGG16 Architecture trained on ImageNet dataset: 1.4 million images with 1000 different classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de19d23",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"./images/vgg16-3.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "deb2a1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the Convolution base of the architecture. We will ignore the last three layers\n",
    "convbase_model = keras.applications.vgg16.VGG16(\n",
    "    weights=\"imagenet\",\n",
    "    include_top=False,\n",
    "    input_shape=(224,224,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "914da574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "convbase_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63571af3",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"./images/vgg16-3.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d6f4f6",
   "metadata": {},
   "source": [
    "### Define the model architecture including VGG16 Convolution Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "158610a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1aeaf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b87cb1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are not extracting features from the convbase and then passing it as input as we did before\n",
    "\n",
    "# Instead \n",
    "# ..We will integrate vgg16 convbase architecture into ours.\n",
    "# ..Fine tune only those layers that we want to\n",
    "\n",
    "def cifar_vgg16_selectlayers(base_model): \n",
    "      \n",
    "    # choose which layers we want to freeze. We will freeze all layers except the last 4\n",
    "    base_model.trainable = True\n",
    "    for layer in base_model.layers[:-4]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Define Input shape\n",
    "    inputs = keras.Input(shape = (224,224,3)) \n",
    "    \n",
    "    # get Vgg16 base model output by passing in input \n",
    "    x = base_model(inputs) \n",
    "\n",
    "    # Flatten\n",
    "    x = layers.Flatten()(x) \n",
    "  \n",
    "    # Dense layer with 256 units\n",
    "    x = layers.Dense(256, activation=\"relu\")(x) \n",
    "    x = layers.Dropout(0.4)(x) # Dropout layer\n",
    "    \n",
    "    # Final dense output layer with 10 units\n",
    "    outputs = layers.Dense(10, activation=\"softmax\")(x) \n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9b5265",
   "metadata": {},
   "source": [
    "## Fine Tune Select Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02d4e3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " vgg16 (Functional)          (None, 7, 7, 512)         14714688  \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               6422784   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,140,042\n",
      "Trainable params: 13,504,778\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model by freezing weights of base model\n",
    "model = cifar_vgg16_selectlayers(convbase_model) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceab7cbb",
   "metadata": {},
   "source": [
    "### Compile & Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25db29b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sridh\\anaconda3\\envs\\tf2.10_env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:111: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Compile the Model by configuring the  optimizer, loss function & metrics \n",
    "\n",
    "sgd = tf.keras.optimizers.SGD(lr=0.001) # sgd optimizer\n",
    "\n",
    "model.compile(optimizer=sgd,loss='sparse_categorical_crossentropy',  metrics = [\"accuracy\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c408b7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CallBacks\n",
    "\n",
    "# call back for reducing learning rate as the training proceeds\n",
    "def lr_scheduler(epoch):\n",
    "    return 0.001 * (0.5 ** (epoch // 20))\n",
    "schedule_lr = keras.callbacks.LearningRateScheduler(lr_scheduler) \n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", \n",
    "                                           patience=3, \n",
    "                                           restore_best_weights=True)\n",
    "\n",
    "callbacks = [schedule_lr, early_stop] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5493de62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1250/1250 [==============================] - 225s 171ms/step - loss: 0.9599 - accuracy: 0.6826 - val_loss: 0.5377 - val_accuracy: 0.8098 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "1250/1250 [==============================] - 237s 189ms/step - loss: 0.4654 - accuracy: 0.8432 - val_loss: 0.4033 - val_accuracy: 0.8617 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "1250/1250 [==============================] - 211s 168ms/step - loss: 0.3076 - accuracy: 0.8953 - val_loss: 0.3542 - val_accuracy: 0.8836 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "1250/1250 [==============================] - 207s 166ms/step - loss: 0.2133 - accuracy: 0.9267 - val_loss: 0.3381 - val_accuracy: 0.8898 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "1250/1250 [==============================] - 201s 160ms/step - loss: 0.1493 - accuracy: 0.9488 - val_loss: 0.3499 - val_accuracy: 0.8927 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "1250/1250 [==============================] - 202s 162ms/step - loss: 0.1033 - accuracy: 0.9660 - val_loss: 0.3522 - val_accuracy: 0.8986 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "1250/1250 [==============================] - 203s 162ms/step - loss: 0.0763 - accuracy: 0.9751 - val_loss: 0.3663 - val_accuracy: 0.9016 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "1250/1250 [==============================] - 283s 226ms/step - loss: 0.0582 - accuracy: 0.9807 - val_loss: 0.3678 - val_accuracy: 0.9023 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "1250/1250 [==============================] - 199s 159ms/step - loss: 0.0433 - accuracy: 0.9868 - val_loss: 0.3690 - val_accuracy: 0.9044 - lr: 0.0010\n",
      "Epoch 10/30\n",
      "1250/1250 [==============================] - 200s 160ms/step - loss: 0.0340 - accuracy: 0.9896 - val_loss: 0.4420 - val_accuracy: 0.8967 - lr: 0.0010\n",
      "Epoch 11/30\n",
      "1250/1250 [==============================] - 191s 153ms/step - loss: 0.0279 - accuracy: 0.9923 - val_loss: 0.4033 - val_accuracy: 0.9064 - lr: 0.0010\n",
      "Epoch 12/30\n",
      "1250/1250 [==============================] - 203s 162ms/step - loss: 0.0240 - accuracy: 0.9932 - val_loss: 0.3948 - val_accuracy: 0.9069 - lr: 0.0010\n",
      "Epoch 13/30\n",
      "1250/1250 [==============================] - 204s 163ms/step - loss: 0.0202 - accuracy: 0.9947 - val_loss: 0.4330 - val_accuracy: 0.9047 - lr: 0.0010\n",
      "Epoch 14/30\n",
      "1250/1250 [==============================] - 200s 160ms/step - loss: 0.0162 - accuracy: 0.9953 - val_loss: 0.4177 - val_accuracy: 0.9100 - lr: 0.0010\n",
      "Epoch 15/30\n",
      "1250/1250 [==============================] - 200s 160ms/step - loss: 0.0133 - accuracy: 0.9966 - val_loss: 0.4259 - val_accuracy: 0.9106 - lr: 0.0010\n",
      "Epoch 16/30\n",
      "1250/1250 [==============================] - 201s 160ms/step - loss: 0.0143 - accuracy: 0.9960 - val_loss: 0.4180 - val_accuracy: 0.9112 - lr: 0.0010\n",
      "Epoch 17/30\n",
      "1250/1250 [==============================] - 212s 169ms/step - loss: 0.0114 - accuracy: 0.9970 - val_loss: 0.4462 - val_accuracy: 0.9114 - lr: 0.0010\n",
      "Epoch 18/30\n",
      "1250/1250 [==============================] - 204s 163ms/step - loss: 0.0094 - accuracy: 0.9976 - val_loss: 0.4393 - val_accuracy: 0.9097 - lr: 0.0010\n",
      "Epoch 19/30\n",
      "1250/1250 [==============================] - 205s 164ms/step - loss: 0.0096 - accuracy: 0.9973 - val_loss: 0.4381 - val_accuracy: 0.9129 - lr: 0.0010\n",
      "Epoch 20/30\n",
      "1250/1250 [==============================] - 214s 171ms/step - loss: 0.0088 - accuracy: 0.9977 - val_loss: 0.4402 - val_accuracy: 0.9128 - lr: 0.0010\n",
      "Epoch 21/30\n",
      "1250/1250 [==============================] - 212s 169ms/step - loss: 0.0054 - accuracy: 0.9989 - val_loss: 0.4514 - val_accuracy: 0.9123 - lr: 5.0000e-04\n",
      "Epoch 22/30\n",
      "1250/1250 [==============================] - 209s 167ms/step - loss: 0.0057 - accuracy: 0.9989 - val_loss: 0.4540 - val_accuracy: 0.9126 - lr: 5.0000e-04\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(train_dataset, epochs = 30, batch_size = 32, \n",
    "                    validation_data=val_dataset,callbacks=callbacks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc00ff32",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c231029",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_dataset, batch_size=32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8bfb01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

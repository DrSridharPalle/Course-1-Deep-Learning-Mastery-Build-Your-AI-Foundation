{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29d3b3fe",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"./images/PallenceAI-Final.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccee792",
   "metadata": {},
   "source": [
    "# Machine Translation: Sequence to Sequence Models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2b3807",
   "metadata": {},
   "source": [
    "## Transformers: Encoder - Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa336972",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"./images/mt2.webp\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162adc49",
   "metadata": {},
   "source": [
    "### Import needed libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a9babbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic Python packages for data wrangling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "#Tensorflow & Keras related packages\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "from utils import plot_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ebcf38",
   "metadata": {},
   "source": [
    "### Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e81043b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First download the dataset from \n",
    "#...http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d885708",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"data/spa-eng/spa.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e702b075",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path, encoding=\"utf-8\") as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "\n",
    "sentence_pairs = []\n",
    "for line in lines:\n",
    "    english, spanish = line.split(\"\\t\")\n",
    "    sentence_pairs.append((english, spanish))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8805920b",
   "metadata": {},
   "source": [
    "### Understand & Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95199abf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Go.', 'Ve.'),\n",
       " ('Go.', 'Vete.'),\n",
       " ('Go.', 'Vaya.'),\n",
       " ('Go.', 'Váyase.'),\n",
       " ('Hi.', 'Hola.'),\n",
       " ('Run!', '¡Corre!'),\n",
       " ('Run.', 'Corred.'),\n",
       " ('Who?', '¿Quién?'),\n",
       " ('Fire!', '¡Fuego!'),\n",
       " ('Fire!', '¡Incendio!')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_pairs[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae7a2359",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(sentence_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329d4742",
   "metadata": {},
   "source": [
    "**Convert the dataset into a tf.data.Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b88684d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the dataset into a tf.data.Dataset\n",
    "english = [pairs[0] for pairs in sentence_pairs]\n",
    "spanish = [pairs[1] for pairs in sentence_pairs]\n",
    "all_data = tf.data.Dataset.from_tensor_slices((english,spanish))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e70d605d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118964"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_data) # Total 118964 sentence pairs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3c9c9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: Tom always keeps his word.\n",
      "Spanish: Tom siempre mantiene su palabra.\n",
      "English: Tom should do the same thing I do.\n",
      "Spanish: Tom debería hacer lo mismo que yo.\n"
     ]
    }
   ],
   "source": [
    "# Displaying a sample. First two sentence pairs\n",
    "for en,sp in all_data.take(2):\n",
    "    print(\"English:\", en.numpy().decode('utf-8'))\n",
    "    print(\"Spanish:\", sp.numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e45fe6",
   "metadata": {},
   "source": [
    "**Add [start], [end] to target sentences (spanish)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bb988e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: Tom always keeps his word.\n",
      "Spanish: [START] Tom siempre mantiene su palabra. [END]\n",
      "English: Tom should do the same thing I do.\n",
      "Spanish: [START] Tom debería hacer lo mismo que yo. [END]\n",
      "English: Can I speak with the teacher?\n",
      "Spanish: [START] ¿Puedo hablar con el profesor? [END]\n"
     ]
    }
   ],
   "source": [
    "# We need to add [start], [end] tokens to target sentences \n",
    "#..so that the model will start predicting when [start] is given as first token, \n",
    "#..and when it predicts [end] or reaches max sequence length it will stop\n",
    "\n",
    "def add_tokens(source_sentence, target_sentence):\n",
    "    target_sentence = tf.strings.join([\"[START] \", target_sentence, \" [END]\"])\n",
    "    return source_sentence, target_sentence\n",
    "\n",
    "# Apply the token addition to the datasets\n",
    "all_data = all_data.map(add_tokens)\n",
    "\n",
    "# Displaying a sample\n",
    "for en,sp in all_data.take(3):\n",
    "    print(\"English:\", en.numpy().decode('utf-8'))\n",
    "    print(\"Spanish:\", sp.numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2f128a",
   "metadata": {},
   "source": [
    "**Split the data into train, val, test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cc5417c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 83276\n",
      "Validation size: 17844\n",
      "Test size: 17844\n"
     ]
    }
   ],
   "source": [
    "# Get the total number of samples\n",
    "total_samples = all_data.cardinality().numpy()\n",
    "\n",
    "# Calculate the sizes of new splits\n",
    "test_size = val_size = int(0.15 * total_samples)\n",
    "train_size = total_samples - test_size - val_size\n",
    "\n",
    "# Split the dataset\n",
    "train_data = all_data.take(train_size)\n",
    "val_data = all_data.skip(train_size).take(val_size)\n",
    "test_data = all_data.skip(train_size + val_size).take(test_size)\n",
    "\n",
    "# Print the sizes of the new splits\n",
    "print(\"Train size:\", train_size)\n",
    "print(\"Validation size:\", val_size)\n",
    "print(\"Test size:\", test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b94ebb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: Tom always keeps his word.\n",
      "Spanish: [START] Tom siempre mantiene su palabra. [END]\n",
      "English: Tom should do the same thing I do.\n",
      "Spanish: [START] Tom debería hacer lo mismo que yo. [END]\n",
      "English: Can I speak with the teacher?\n",
      "Spanish: [START] ¿Puedo hablar con el profesor? [END]\n"
     ]
    }
   ],
   "source": [
    "for en,sp in train_data.take(3):\n",
    "    print(\"English:\", en.numpy().decode('utf-8'))\n",
    "    print(\"Spanish:\", sp.numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a16a24",
   "metadata": {},
   "source": [
    "**Vectorize the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "586a521c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going forward source will be english language, target will be spanish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc17c933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants \n",
    "VOCAB_SIZE = 15000 # Max tokens\n",
    "MAX_SEQ_LEN = 20 # Max sequence length\n",
    "EMBED_DIM=256 # Embedding dimension\n",
    "HIDDEN_DIM = 1024 # Hidden dimension for dense layers\n",
    "BATCH_SIZE=64 # Batch size\n",
    "NUM_HEADS=8 # Number of heads for Multiheaded attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab9320bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the vectorizers\n",
    "source_vectorizer = layers.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=MAX_SEQ_LEN,\n",
    ")\n",
    "\n",
    "strip_chars = string.punctuation + \"¿\"\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(\n",
    "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
    "\n",
    "target_vectorizer = layers.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode=\"int\",\n",
    "    # we add an extra token since target will be offset by one token\n",
    "    output_sequence_length=MAX_SEQ_LEN + 1, \n",
    "    standardize=custom_standardization,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "744406a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapt the vectorizers to the dataset. Meaning creating the vocabulary for respective languages.\n",
    "source_vectorizer.adapt(train_data.map(lambda x,y: x)) # for english sentences\n",
    "target_vectorizer.adapt(train_data.map(lambda x,y: y)) # for spanish sentences\n",
    "\n",
    "#or..\n",
    "# source_vectorizer.adapt(english) # for english sentences created earlier\n",
    "# target_vectorizer.adapt(spanish) # for spanish sentences created earlier\n",
    "\n",
    "#This will take a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6041ae55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'the',\n",
       " 'i',\n",
       " 'to',\n",
       " 'you',\n",
       " 'tom',\n",
       " 'a',\n",
       " 'is',\n",
       " 'he',\n",
       " 'in',\n",
       " 'of',\n",
       " 'that',\n",
       " 'it',\n",
       " 'was',\n",
       " 'do',\n",
       " 'this',\n",
       " 'have',\n",
       " 'me',\n",
       " 'my',\n",
       " 'for',\n",
       " 'she',\n",
       " 'dont',\n",
       " 'are',\n",
       " 'what',\n",
       " 'his',\n",
       " 'mary',\n",
       " 'we',\n",
       " 'your',\n",
       " 'on',\n",
       " 'be',\n",
       " 'with',\n",
       " 'want',\n",
       " 'not',\n",
       " 'im',\n",
       " 'and',\n",
       " 'like',\n",
       " 'at',\n",
       " 'know',\n",
       " 'him',\n",
       " 'can',\n",
       " 'go',\n",
       " 'her',\n",
       " 'has',\n",
       " 'will',\n",
       " 'its',\n",
       " 'there',\n",
       " 'they',\n",
       " 'time',\n",
       " 'were',\n",
       " 'very',\n",
       " 'as',\n",
       " 'how',\n",
       " 'did',\n",
       " 'had',\n",
       " 'all',\n",
       " 'about',\n",
       " 'here',\n",
       " 'think',\n",
       " 'up',\n",
       " 'didnt',\n",
       " 'get',\n",
       " 'out',\n",
       " 'when',\n",
       " 'from',\n",
       " 'if',\n",
       " 'cant',\n",
       " 'an',\n",
       " 'no',\n",
       " 'one',\n",
       " 'going',\n",
       " 'by',\n",
       " 'why',\n",
       " 'doesnt',\n",
       " 'would',\n",
       " 'come',\n",
       " 'see',\n",
       " 'good',\n",
       " 'ill',\n",
       " 'youre',\n",
       " 'please',\n",
       " 'who',\n",
       " 'just',\n",
       " 'been',\n",
       " 'need',\n",
       " 'more',\n",
       " 'so',\n",
       " 'help',\n",
       " 'than',\n",
       " 'tell',\n",
       " 'but',\n",
       " 'where',\n",
       " 'never',\n",
       " 'now',\n",
       " 'am',\n",
       " 'got',\n",
       " 'us',\n",
       " 'too',\n",
       " 'some',\n",
       " 'something',\n",
       " 'last',\n",
       " 'ive',\n",
       " 'take',\n",
       " 'much',\n",
       " 'day',\n",
       " 'could',\n",
       " 'should',\n",
       " 'money',\n",
       " 'car',\n",
       " 'people',\n",
       " 'work',\n",
       " 'well',\n",
       " 'home',\n",
       " 'back',\n",
       " 'really',\n",
       " 'went',\n",
       " 'our',\n",
       " 'said',\n",
       " 'anything',\n",
       " 'house',\n",
       " 'told',\n",
       " 'many',\n",
       " 'book',\n",
       " 'lot',\n",
       " 'say',\n",
       " 'does',\n",
       " 'french',\n",
       " 'any',\n",
       " 'isnt',\n",
       " 'two',\n",
       " 'thought',\n",
       " 'make',\n",
       " 'only',\n",
       " 'thats',\n",
       " 'room',\n",
       " 'hes',\n",
       " 'speak',\n",
       " 'eat',\n",
       " 'toms',\n",
       " 'today',\n",
       " 'new',\n",
       " 'school',\n",
       " 'right',\n",
       " 'must',\n",
       " 'always',\n",
       " 'give',\n",
       " 'made',\n",
       " 'still',\n",
       " 'love',\n",
       " 'every',\n",
       " 'long',\n",
       " 'wanted',\n",
       " 'old',\n",
       " 'man',\n",
       " 'three',\n",
       " 'lets',\n",
       " 'father',\n",
       " 'tomorrow',\n",
       " 'night',\n",
       " 'let',\n",
       " 'off',\n",
       " 'look',\n",
       " 'before',\n",
       " 'id',\n",
       " 'boston',\n",
       " 'asked',\n",
       " 'leave',\n",
       " 'left',\n",
       " 'talk',\n",
       " 'or',\n",
       " 'again',\n",
       " 'little',\n",
       " 'dog',\n",
       " 'over',\n",
       " 'way',\n",
       " 'nothing',\n",
       " 'saw',\n",
       " 'english',\n",
       " 'years',\n",
       " 'whats',\n",
       " 'put',\n",
       " 'after',\n",
       " 'better',\n",
       " 'down',\n",
       " 'job',\n",
       " 'yesterday',\n",
       " 'into',\n",
       " 'stay',\n",
       " 'them',\n",
       " 'live',\n",
       " 'everything',\n",
       " 'wont',\n",
       " 'may',\n",
       " 'buy',\n",
       " 'happy',\n",
       " 'mother',\n",
       " 'came',\n",
       " 'ask',\n",
       " 'first',\n",
       " 'theres',\n",
       " 'read',\n",
       " 'play',\n",
       " 'couldnt',\n",
       " 'wants',\n",
       " 'other',\n",
       " 'find',\n",
       " 'these',\n",
       " 'next',\n",
       " 'took',\n",
       " 'life',\n",
       " 'children',\n",
       " 'friends',\n",
       " 'believe',\n",
       " 'understand',\n",
       " 'morning',\n",
       " 'bought',\n",
       " 'problem',\n",
       " 'call',\n",
       " 'stop',\n",
       " 'doing',\n",
       " 'away',\n",
       " 'used',\n",
       " 'door',\n",
       " 'ever',\n",
       " 'lost',\n",
       " 'their',\n",
       " 'keep',\n",
       " 'feel',\n",
       " 'soon',\n",
       " 'sure',\n",
       " 'gave',\n",
       " 'name',\n",
       " 'late',\n",
       " 'remember',\n",
       " 'happened',\n",
       " 'already',\n",
       " 'knows',\n",
       " 'friend',\n",
       " 'bed',\n",
       " 'married',\n",
       " 'hard',\n",
       " 'even',\n",
       " 'beautiful',\n",
       " 'enough',\n",
       " 'teacher',\n",
       " 'heard',\n",
       " 'alone',\n",
       " 'water',\n",
       " 'wasnt',\n",
       " 'looking',\n",
       " 'busy',\n",
       " 'things',\n",
       " 'year',\n",
       " 'week',\n",
       " 'often',\n",
       " 'wrong',\n",
       " 'bad',\n",
       " 'try',\n",
       " 'boy',\n",
       " 'without',\n",
       " 'likes',\n",
       " 'found',\n",
       " 'seen',\n",
       " 'because',\n",
       " 'able',\n",
       " 'done',\n",
       " 'best',\n",
       " 'wait',\n",
       " 'hear',\n",
       " 'marys',\n",
       " 'looks',\n",
       " 'knew',\n",
       " 'thing',\n",
       " 'food',\n",
       " 'person',\n",
       " 'big',\n",
       " 'train',\n",
       " 'same',\n",
       " 'idea',\n",
       " 'john',\n",
       " 'party',\n",
       " 'theyre',\n",
       " 'while',\n",
       " 'use',\n",
       " 'coffee',\n",
       " 'hope',\n",
       " 'tired',\n",
       " 'answer',\n",
       " 'yet',\n",
       " 'true',\n",
       " 'brother',\n",
       " 'cold',\n",
       " 'being',\n",
       " 'watch',\n",
       " 'shes',\n",
       " 'everyone',\n",
       " 'around',\n",
       " 'few',\n",
       " 'japan',\n",
       " 'almost',\n",
       " 'drink',\n",
       " 'young',\n",
       " 'youll',\n",
       " 'pay',\n",
       " 'study',\n",
       " 'met',\n",
       " 'girl',\n",
       " 'another',\n",
       " 'wish',\n",
       " 'bus',\n",
       " 'mind',\n",
       " 'letter',\n",
       " 'kind',\n",
       " 'died',\n",
       " 'books',\n",
       " 'learn',\n",
       " 'sleep',\n",
       " 'days',\n",
       " 'open',\n",
       " 'afraid',\n",
       " 'most',\n",
       " 'someone',\n",
       " 'once',\n",
       " 'sister',\n",
       " 'world',\n",
       " 'talking',\n",
       " 'parents',\n",
       " 'which',\n",
       " 'seems',\n",
       " 'each',\n",
       " 'ten',\n",
       " 'those',\n",
       " 'place',\n",
       " 'looked',\n",
       " 'until',\n",
       " 'walk',\n",
       " 'rain',\n",
       " 'getting',\n",
       " 'happen',\n",
       " 'turn',\n",
       " 'difficult',\n",
       " 'angry',\n",
       " 'care',\n",
       " 'anyone',\n",
       " 'truth',\n",
       " 'own',\n",
       " 'write',\n",
       " 'together',\n",
       " 'both',\n",
       " 'cat',\n",
       " 'lunch',\n",
       " 'havent',\n",
       " 'coming',\n",
       " 'wouldnt',\n",
       " 'dinner',\n",
       " 'ago',\n",
       " 'family',\n",
       " 'early',\n",
       " 'plan',\n",
       " 'else',\n",
       " 'yourself',\n",
       " 'monday',\n",
       " 'mine',\n",
       " 'country',\n",
       " 'tried',\n",
       " 'youve',\n",
       " 'show',\n",
       " 'tv',\n",
       " 'playing',\n",
       " 'favorite',\n",
       " 'hours',\n",
       " 'hurt',\n",
       " 'doctor',\n",
       " 'accident',\n",
       " 'great',\n",
       " 'waiting',\n",
       " 'table',\n",
       " 'question',\n",
       " 'arent',\n",
       " 'wife',\n",
       " 'minutes',\n",
       " 'eating',\n",
       " 'child',\n",
       " 'lives',\n",
       " 'reading',\n",
       " 'five',\n",
       " 'everybody',\n",
       " 'since',\n",
       " 'japanese',\n",
       " 'nobody',\n",
       " 'himself',\n",
       " 'hate',\n",
       " 'far',\n",
       " 'easy',\n",
       " 'says',\n",
       " 'ate',\n",
       " 'such',\n",
       " 'sorry',\n",
       " 'ready',\n",
       " 'station',\n",
       " 'tonight',\n",
       " 'police',\n",
       " 'nice',\n",
       " 'might',\n",
       " 'meeting',\n",
       " 'under',\n",
       " 'phone',\n",
       " 'meet',\n",
       " 'finished',\n",
       " 'matter',\n",
       " 'office',\n",
       " 'arrived',\n",
       " 'trying',\n",
       " 'times',\n",
       " 'swim',\n",
       " 'students',\n",
       " 'hand',\n",
       " 'started',\n",
       " 'start',\n",
       " 'ran',\n",
       " 'decided',\n",
       " 'important',\n",
       " 'having',\n",
       " 'shoes',\n",
       " 'month',\n",
       " 'felt',\n",
       " 'son',\n",
       " 'woman',\n",
       " 'story',\n",
       " 'called',\n",
       " 'trouble',\n",
       " 'pretty',\n",
       " 'goes',\n",
       " 'city',\n",
       " 'window',\n",
       " 'small',\n",
       " 'drive',\n",
       " 'shouldnt',\n",
       " 'word',\n",
       " 'weve',\n",
       " 'music',\n",
       " 'change',\n",
       " 'myself',\n",
       " 'game',\n",
       " 'fire',\n",
       " 'fast',\n",
       " 'hot',\n",
       " 'anymore',\n",
       " 'sick',\n",
       " 'usually',\n",
       " 'mistake',\n",
       " 'homework',\n",
       " 'town',\n",
       " 'park',\n",
       " 'eyes',\n",
       " 'studying',\n",
       " 'song',\n",
       " 'began',\n",
       " 'visit',\n",
       " 'six',\n",
       " 'fun',\n",
       " 'working',\n",
       " 'picture',\n",
       " 'interesting',\n",
       " 'gone',\n",
       " 'christmas',\n",
       " 'stupid',\n",
       " 'language',\n",
       " 'hair',\n",
       " 'tennis',\n",
       " 'run',\n",
       " 'needs',\n",
       " 'listen',\n",
       " 'fish',\n",
       " 'lived',\n",
       " 'hotel',\n",
       " 'forget',\n",
       " 'baby',\n",
       " 'thank',\n",
       " 'red',\n",
       " 'broke',\n",
       " 'against',\n",
       " 'afternoon',\n",
       " 'milk',\n",
       " 'through',\n",
       " 'spend',\n",
       " 'rich',\n",
       " 'free',\n",
       " 'weather',\n",
       " 'quite',\n",
       " 'kept',\n",
       " 'longer',\n",
       " 'advice',\n",
       " 'watching',\n",
       " 'summer',\n",
       " 'fell',\n",
       " 'caught',\n",
       " 'breakfast',\n",
       " 'then',\n",
       " 'comes',\n",
       " 'finish',\n",
       " 'face',\n",
       " 'older',\n",
       " 'number',\n",
       " 'smoking',\n",
       " 'men',\n",
       " 'thinking',\n",
       " 'makes',\n",
       " 'large',\n",
       " 'hour',\n",
       " 'killed',\n",
       " 'sit',\n",
       " 'loves',\n",
       " 'hospital',\n",
       " 'class',\n",
       " 'stopped',\n",
       " 'seem',\n",
       " 'news',\n",
       " 'movie',\n",
       " 'along',\n",
       " 'mean',\n",
       " 'later',\n",
       " 'dark',\n",
       " 'tokyo',\n",
       " 'short',\n",
       " 'cake',\n",
       " 'bit',\n",
       " 'australia',\n",
       " 'whos',\n",
       " 'turned',\n",
       " 'questions',\n",
       " 'making',\n",
       " 'snow',\n",
       " 'camera',\n",
       " 'bring',\n",
       " 'became',\n",
       " 'youd',\n",
       " 'secret',\n",
       " 'river',\n",
       " 'present',\n",
       " 'living',\n",
       " 'high',\n",
       " 'close',\n",
       " 'chance',\n",
       " 'sat',\n",
       " 'hit',\n",
       " 'full',\n",
       " 'box',\n",
       " 'works',\n",
       " 'war',\n",
       " 'tree',\n",
       " 'student',\n",
       " 'street',\n",
       " 'others',\n",
       " 'glad',\n",
       " 'end',\n",
       " 'america',\n",
       " 'lie',\n",
       " 'death',\n",
       " 'careful',\n",
       " 'white',\n",
       " 'dream',\n",
       " 'cannot',\n",
       " 'surprised',\n",
       " 'hungry',\n",
       " 'gets',\n",
       " 'age',\n",
       " 'wonder',\n",
       " 'trust',\n",
       " 'kill',\n",
       " 'forgot',\n",
       " 'dollars',\n",
       " 'miss',\n",
       " 'cut',\n",
       " 'computer',\n",
       " 'born',\n",
       " 'thinks',\n",
       " 'hurry',\n",
       " 'hands',\n",
       " 'flowers',\n",
       " 'store',\n",
       " 'plane',\n",
       " 'between',\n",
       " 'wine',\n",
       " 'spent',\n",
       " 'reason',\n",
       " 'clothes',\n",
       " 'possible',\n",
       " 'moment',\n",
       " 'die',\n",
       " 'dead',\n",
       " 'bicycle',\n",
       " '230',\n",
       " 'stand',\n",
       " 'sometimes',\n",
       " 'dogs',\n",
       " 'taking',\n",
       " 'near',\n",
       " 'expensive',\n",
       " 'clean',\n",
       " 'worked',\n",
       " 'saying',\n",
       " 'hasnt',\n",
       " 'company',\n",
       " 'sing',\n",
       " 'oclock',\n",
       " 'key',\n",
       " 'daughter',\n",
       " 'tea',\n",
       " 'speaks',\n",
       " 'speaking',\n",
       " 'light',\n",
       " 'building',\n",
       " 'beer',\n",
       " 'whether',\n",
       " 'running',\n",
       " 'rather',\n",
       " 'piano',\n",
       " 'girlfriend',\n",
       " 'four',\n",
       " 'advised',\n",
       " 'changed',\n",
       " 'canadian',\n",
       " 'birthday',\n",
       " 'women',\n",
       " 'send',\n",
       " 'words',\n",
       " 'wrote',\n",
       " 'rest',\n",
       " 'order',\n",
       " 'needed',\n",
       " 'become',\n",
       " 'anybody',\n",
       " 'sunday',\n",
       " 'business',\n",
       " 'strange',\n",
       " 'helped',\n",
       " 'drunk',\n",
       " 'dress',\n",
       " 'crazy',\n",
       " 'broken',\n",
       " 'quickly',\n",
       " 'paper',\n",
       " 'floor',\n",
       " 'dictionary',\n",
       " 'cup',\n",
       " 'asleep',\n",
       " 'real',\n",
       " 'outside',\n",
       " 'maybe',\n",
       " 'herself',\n",
       " 'different',\n",
       " 'trip',\n",
       " 'quit',\n",
       " 'part',\n",
       " 'exactly',\n",
       " 'behind',\n",
       " 'air',\n",
       " 'restaurant',\n",
       " 'problems',\n",
       " 'lose',\n",
       " 'least',\n",
       " 'interested',\n",
       " 'hold',\n",
       " 'dangerous',\n",
       " 'concert',\n",
       " 'boys',\n",
       " 'black',\n",
       " 'paid',\n",
       " 'loved',\n",
       " 'health',\n",
       " 'guitar',\n",
       " 'glasses',\n",
       " 'eaten',\n",
       " 'cats',\n",
       " 'wear',\n",
       " 'takes',\n",
       " 'swimming',\n",
       " 'sad',\n",
       " 'ice',\n",
       " 'hide',\n",
       " 'cook',\n",
       " 'bank',\n",
       " 'whole',\n",
       " 'team',\n",
       " 'tall',\n",
       " 'stayed',\n",
       " 'return',\n",
       " 'proud',\n",
       " 'move',\n",
       " 'hell',\n",
       " 'heart',\n",
       " 'during',\n",
       " 'catch',\n",
       " 'break',\n",
       " 'worry',\n",
       " 'wheres',\n",
       " 'wearing',\n",
       " 'waited',\n",
       " 'touch',\n",
       " 'probably',\n",
       " 'famous',\n",
       " 'explain',\n",
       " 'cry',\n",
       " 'blame',\n",
       " 'telephone',\n",
       " 'noise',\n",
       " 'mistakes',\n",
       " 'forward',\n",
       " 'enjoy',\n",
       " 'either',\n",
       " 'uncle',\n",
       " 'raining',\n",
       " 'opened',\n",
       " 'ok',\n",
       " 'movies',\n",
       " 'learned',\n",
       " 'head',\n",
       " 'baseball',\n",
       " 'strong',\n",
       " 'seemed',\n",
       " 'pain',\n",
       " 'driving',\n",
       " 'yours',\n",
       " 'played',\n",
       " 'missed',\n",
       " 'library',\n",
       " 'lake',\n",
       " 'bag',\n",
       " 'united',\n",
       " 'talked',\n",
       " 'solve',\n",
       " 'side',\n",
       " 'half',\n",
       " 'alive',\n",
       " 'agree',\n",
       " 'walked',\n",
       " 'umbrella',\n",
       " 'seven',\n",
       " 'road',\n",
       " 'promise',\n",
       " 'blue',\n",
       " 'attention',\n",
       " 'airport',\n",
       " 'television',\n",
       " 'showed',\n",
       " 'poor',\n",
       " 'minute',\n",
       " 'meat',\n",
       " 'lying',\n",
       " 'lucky',\n",
       " 'leaving',\n",
       " 'kids',\n",
       " 'crying',\n",
       " 'choice',\n",
       " 'brought',\n",
       " 'win',\n",
       " 'weight',\n",
       " 'thanks',\n",
       " 'shirt',\n",
       " 'known',\n",
       " 'glass',\n",
       " 'feeling',\n",
       " 'thirty',\n",
       " 'states',\n",
       " 'sound',\n",
       " 'sleeping',\n",
       " 'prefer',\n",
       " 'hat',\n",
       " 'garden',\n",
       " 'fine',\n",
       " 'animals',\n",
       " 'set',\n",
       " 'mad',\n",
       " 'guy',\n",
       " 'guess',\n",
       " 'fishing',\n",
       " 'drinking',\n",
       " 'visited',\n",
       " 'smoke',\n",
       " 'radio',\n",
       " 'listening',\n",
       " 'lawyer',\n",
       " 'kitchen',\n",
       " 'finally',\n",
       " 'evening',\n",
       " 'ship',\n",
       " 'seat',\n",
       " 'pass',\n",
       " 'paris',\n",
       " 'learning',\n",
       " 'girls',\n",
       " 'front',\n",
       " 'cost',\n",
       " 'beach',\n",
       " 'whose',\n",
       " 'twice',\n",
       " 'though',\n",
       " 'spoke',\n",
       " 'somebody',\n",
       " 'several',\n",
       " 'seeing',\n",
       " 'safe',\n",
       " 'point',\n",
       " 'follow',\n",
       " 'dance',\n",
       " 'apple',\n",
       " 'wash',\n",
       " 'taken',\n",
       " 'sun',\n",
       " 'shut',\n",
       " 'rules',\n",
       " 'london',\n",
       " 'knife',\n",
       " 'itll',\n",
       " 'accept',\n",
       " 'younger',\n",
       " 'winter',\n",
       " 'sent',\n",
       " 'sense',\n",
       " 'fight',\n",
       " 'closed',\n",
       " 'worried',\n",
       " 'travel',\n",
       " 'test',\n",
       " 'situation',\n",
       " 'second',\n",
       " 'invited',\n",
       " 'decision',\n",
       " 'danger',\n",
       " 'arrive',\n",
       " 'worse',\n",
       " 'won',\n",
       " 'wall',\n",
       " 'walking',\n",
       " 'vacation',\n",
       " 'ticket',\n",
       " 'sitting',\n",
       " 'languages',\n",
       " 'doubt',\n",
       " 'begin',\n",
       " 'abroad',\n",
       " 'writing',\n",
       " 'werent',\n",
       " 'passed',\n",
       " 'months',\n",
       " 'less',\n",
       " 'excuse',\n",
       " 'eggs',\n",
       " 'completely',\n",
       " 'apples',\n",
       " 'worth',\n",
       " 'smile',\n",
       " 'shouldve',\n",
       " 'message',\n",
       " 'leaves',\n",
       " 'church',\n",
       " 'brothers',\n",
       " 'also',\n",
       " 'weekend',\n",
       " 'traffic',\n",
       " 'tie',\n",
       " 'soccer',\n",
       " 'mountain',\n",
       " 'kiss',\n",
       " 'impossible',\n",
       " 'husband',\n",
       " 'feed',\n",
       " 'drank',\n",
       " 'success',\n",
       " 'sounds',\n",
       " 'singing',\n",
       " 'shopping',\n",
       " 'promised',\n",
       " 'pen',\n",
       " 'patient',\n",
       " 'horse',\n",
       " 'china',\n",
       " 'address',\n",
       " 'wake',\n",
       " 'refused',\n",
       " 'piece',\n",
       " 'opinion',\n",
       " 'medicine',\n",
       " 'liked',\n",
       " 'coat',\n",
       " 'cars',\n",
       " 'perfect',\n",
       " 'machine',\n",
       " 'hiding',\n",
       " 'happens',\n",
       " 'expect',\n",
       " 'case',\n",
       " 'across',\n",
       " 'teach',\n",
       " 'shot',\n",
       " 'pictures',\n",
       " 'mustve',\n",
       " 'hed',\n",
       " 'grandfather',\n",
       " 'funny',\n",
       " 'fault',\n",
       " 'expected',\n",
       " 'easily',\n",
       " 'deal',\n",
       " 'written',\n",
       " 'waste',\n",
       " 'till',\n",
       " 'stolen',\n",
       " 'shop',\n",
       " 'plans',\n",
       " 'means',\n",
       " 'happening',\n",
       " 'france',\n",
       " 'foot',\n",
       " 'desk',\n",
       " 'bread',\n",
       " 'arm',\n",
       " 'american',\n",
       " 'surprise',\n",
       " 'serious',\n",
       " 'pick',\n",
       " 'offer',\n",
       " 'necessary',\n",
       " 'heavy',\n",
       " 'future',\n",
       " 'clear',\n",
       " 'carefully',\n",
       " 'boyfriend',\n",
       " 'bath',\n",
       " 'actually',\n",
       " 'somewhere',\n",
       " 'share',\n",
       " 'foreign',\n",
       " 'figure',\n",
       " 'enemy',\n",
       " 'cream',\n",
       " 'telling',\n",
       " 'sky',\n",
       " 'sell',\n",
       " 'law',\n",
       " 'keys',\n",
       " 'given',\n",
       " 'fix',\n",
       " 'failed',\n",
       " 'couple',\n",
       " 'terrible',\n",
       " 'hardly',\n",
       " 'correct',\n",
       " 'carry',\n",
       " 'boss',\n",
       " 'york',\n",
       " 'warm',\n",
       " 'salt',\n",
       " 'policeman',\n",
       " 'moved',\n",
       " 'marry',\n",
       " 'kissed',\n",
       " 'guys',\n",
       " 'eats',\n",
       " 'choose',\n",
       " 'yes',\n",
       " 'past',\n",
       " 'novel',\n",
       " 'none',\n",
       " 'laughed',\n",
       " 'inside',\n",
       " 'fly',\n",
       " 'favor',\n",
       " 'college',\n",
       " 'chinese',\n",
       " 'check',\n",
       " 'chair',\n",
       " 'bird',\n",
       " 'trees',\n",
       " 'supposed',\n",
       " 'sugar',\n",
       " 'spoken',\n",
       " 'sign',\n",
       " 'shall',\n",
       " 'joke',\n",
       " 'honest',\n",
       " 'save',\n",
       " 'prison',\n",
       " 'nervous',\n",
       " 'gun',\n",
       " 'green',\n",
       " 'german',\n",
       " 'fear',\n",
       " 'entered',\n",
       " 'wallet',\n",
       " 'taxi',\n",
       " 'studied',\n",
       " 'sisters',\n",
       " 'popular',\n",
       " 'planning',\n",
       " 'owe',\n",
       " 'neither',\n",
       " 'lend',\n",
       " 'hundred',\n",
       " ...]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_vocab =source_vectorizer.get_vocabulary()\n",
    "source_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8db41eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '',\n",
       " 1: '[UNK]',\n",
       " 2: 'the',\n",
       " 3: 'i',\n",
       " 4: 'to',\n",
       " 5: 'you',\n",
       " 6: 'tom',\n",
       " 7: 'a',\n",
       " 8: 'is',\n",
       " 9: 'he',\n",
       " 10: 'in',\n",
       " 11: 'of',\n",
       " 12: 'that',\n",
       " 13: 'it',\n",
       " 14: 'was',\n",
       " 15: 'do',\n",
       " 16: 'this',\n",
       " 17: 'have',\n",
       " 18: 'me',\n",
       " 19: 'my',\n",
       " 20: 'for',\n",
       " 21: 'she',\n",
       " 22: 'dont',\n",
       " 23: 'are',\n",
       " 24: 'what',\n",
       " 25: 'his',\n",
       " 26: 'mary',\n",
       " 27: 'we',\n",
       " 28: 'your',\n",
       " 29: 'on',\n",
       " 30: 'be',\n",
       " 31: 'with',\n",
       " 32: 'want',\n",
       " 33: 'not',\n",
       " 34: 'im',\n",
       " 35: 'and',\n",
       " 36: 'like',\n",
       " 37: 'at',\n",
       " 38: 'know',\n",
       " 39: 'him',\n",
       " 40: 'can',\n",
       " 41: 'go',\n",
       " 42: 'her',\n",
       " 43: 'has',\n",
       " 44: 'will',\n",
       " 45: 'its',\n",
       " 46: 'there',\n",
       " 47: 'they',\n",
       " 48: 'time',\n",
       " 49: 'were',\n",
       " 50: 'very',\n",
       " 51: 'as',\n",
       " 52: 'how',\n",
       " 53: 'did',\n",
       " 54: 'had',\n",
       " 55: 'all',\n",
       " 56: 'about',\n",
       " 57: 'here',\n",
       " 58: 'think',\n",
       " 59: 'up',\n",
       " 60: 'didnt',\n",
       " 61: 'get',\n",
       " 62: 'out',\n",
       " 63: 'when',\n",
       " 64: 'from',\n",
       " 65: 'if',\n",
       " 66: 'cant',\n",
       " 67: 'an',\n",
       " 68: 'no',\n",
       " 69: 'one',\n",
       " 70: 'going',\n",
       " 71: 'by',\n",
       " 72: 'why',\n",
       " 73: 'doesnt',\n",
       " 74: 'would',\n",
       " 75: 'come',\n",
       " 76: 'see',\n",
       " 77: 'good',\n",
       " 78: 'ill',\n",
       " 79: 'youre',\n",
       " 80: 'please',\n",
       " 81: 'who',\n",
       " 82: 'just',\n",
       " 83: 'been',\n",
       " 84: 'need',\n",
       " 85: 'more',\n",
       " 86: 'so',\n",
       " 87: 'help',\n",
       " 88: 'than',\n",
       " 89: 'tell',\n",
       " 90: 'but',\n",
       " 91: 'where',\n",
       " 92: 'never',\n",
       " 93: 'now',\n",
       " 94: 'am',\n",
       " 95: 'got',\n",
       " 96: 'us',\n",
       " 97: 'too',\n",
       " 98: 'some',\n",
       " 99: 'something',\n",
       " 100: 'last',\n",
       " 101: 'ive',\n",
       " 102: 'take',\n",
       " 103: 'much',\n",
       " 104: 'day',\n",
       " 105: 'could',\n",
       " 106: 'should',\n",
       " 107: 'money',\n",
       " 108: 'car',\n",
       " 109: 'people',\n",
       " 110: 'work',\n",
       " 111: 'well',\n",
       " 112: 'home',\n",
       " 113: 'back',\n",
       " 114: 'really',\n",
       " 115: 'went',\n",
       " 116: 'our',\n",
       " 117: 'said',\n",
       " 118: 'anything',\n",
       " 119: 'house',\n",
       " 120: 'told',\n",
       " 121: 'many',\n",
       " 122: 'book',\n",
       " 123: 'lot',\n",
       " 124: 'say',\n",
       " 125: 'does',\n",
       " 126: 'french',\n",
       " 127: 'any',\n",
       " 128: 'isnt',\n",
       " 129: 'two',\n",
       " 130: 'thought',\n",
       " 131: 'make',\n",
       " 132: 'only',\n",
       " 133: 'thats',\n",
       " 134: 'room',\n",
       " 135: 'hes',\n",
       " 136: 'speak',\n",
       " 137: 'eat',\n",
       " 138: 'toms',\n",
       " 139: 'today',\n",
       " 140: 'new',\n",
       " 141: 'school',\n",
       " 142: 'right',\n",
       " 143: 'must',\n",
       " 144: 'always',\n",
       " 145: 'give',\n",
       " 146: 'made',\n",
       " 147: 'still',\n",
       " 148: 'love',\n",
       " 149: 'every',\n",
       " 150: 'long',\n",
       " 151: 'wanted',\n",
       " 152: 'old',\n",
       " 153: 'man',\n",
       " 154: 'three',\n",
       " 155: 'lets',\n",
       " 156: 'father',\n",
       " 157: 'tomorrow',\n",
       " 158: 'night',\n",
       " 159: 'let',\n",
       " 160: 'off',\n",
       " 161: 'look',\n",
       " 162: 'before',\n",
       " 163: 'id',\n",
       " 164: 'boston',\n",
       " 165: 'asked',\n",
       " 166: 'leave',\n",
       " 167: 'left',\n",
       " 168: 'talk',\n",
       " 169: 'or',\n",
       " 170: 'again',\n",
       " 171: 'little',\n",
       " 172: 'dog',\n",
       " 173: 'over',\n",
       " 174: 'way',\n",
       " 175: 'nothing',\n",
       " 176: 'saw',\n",
       " 177: 'english',\n",
       " 178: 'years',\n",
       " 179: 'whats',\n",
       " 180: 'put',\n",
       " 181: 'after',\n",
       " 182: 'better',\n",
       " 183: 'down',\n",
       " 184: 'job',\n",
       " 185: 'yesterday',\n",
       " 186: 'into',\n",
       " 187: 'stay',\n",
       " 188: 'them',\n",
       " 189: 'live',\n",
       " 190: 'everything',\n",
       " 191: 'wont',\n",
       " 192: 'may',\n",
       " 193: 'buy',\n",
       " 194: 'happy',\n",
       " 195: 'mother',\n",
       " 196: 'came',\n",
       " 197: 'ask',\n",
       " 198: 'first',\n",
       " 199: 'theres',\n",
       " 200: 'read',\n",
       " 201: 'play',\n",
       " 202: 'couldnt',\n",
       " 203: 'wants',\n",
       " 204: 'other',\n",
       " 205: 'find',\n",
       " 206: 'these',\n",
       " 207: 'next',\n",
       " 208: 'took',\n",
       " 209: 'life',\n",
       " 210: 'children',\n",
       " 211: 'friends',\n",
       " 212: 'believe',\n",
       " 213: 'understand',\n",
       " 214: 'morning',\n",
       " 215: 'bought',\n",
       " 216: 'problem',\n",
       " 217: 'call',\n",
       " 218: 'stop',\n",
       " 219: 'doing',\n",
       " 220: 'away',\n",
       " 221: 'used',\n",
       " 222: 'door',\n",
       " 223: 'ever',\n",
       " 224: 'lost',\n",
       " 225: 'their',\n",
       " 226: 'keep',\n",
       " 227: 'feel',\n",
       " 228: 'soon',\n",
       " 229: 'sure',\n",
       " 230: 'gave',\n",
       " 231: 'name',\n",
       " 232: 'late',\n",
       " 233: 'remember',\n",
       " 234: 'happened',\n",
       " 235: 'already',\n",
       " 236: 'knows',\n",
       " 237: 'friend',\n",
       " 238: 'bed',\n",
       " 239: 'married',\n",
       " 240: 'hard',\n",
       " 241: 'even',\n",
       " 242: 'beautiful',\n",
       " 243: 'enough',\n",
       " 244: 'teacher',\n",
       " 245: 'heard',\n",
       " 246: 'alone',\n",
       " 247: 'water',\n",
       " 248: 'wasnt',\n",
       " 249: 'looking',\n",
       " 250: 'busy',\n",
       " 251: 'things',\n",
       " 252: 'year',\n",
       " 253: 'week',\n",
       " 254: 'often',\n",
       " 255: 'wrong',\n",
       " 256: 'bad',\n",
       " 257: 'try',\n",
       " 258: 'boy',\n",
       " 259: 'without',\n",
       " 260: 'likes',\n",
       " 261: 'found',\n",
       " 262: 'seen',\n",
       " 263: 'because',\n",
       " 264: 'able',\n",
       " 265: 'done',\n",
       " 266: 'best',\n",
       " 267: 'wait',\n",
       " 268: 'hear',\n",
       " 269: 'marys',\n",
       " 270: 'looks',\n",
       " 271: 'knew',\n",
       " 272: 'thing',\n",
       " 273: 'food',\n",
       " 274: 'person',\n",
       " 275: 'big',\n",
       " 276: 'train',\n",
       " 277: 'same',\n",
       " 278: 'idea',\n",
       " 279: 'john',\n",
       " 280: 'party',\n",
       " 281: 'theyre',\n",
       " 282: 'while',\n",
       " 283: 'use',\n",
       " 284: 'coffee',\n",
       " 285: 'hope',\n",
       " 286: 'tired',\n",
       " 287: 'answer',\n",
       " 288: 'yet',\n",
       " 289: 'true',\n",
       " 290: 'brother',\n",
       " 291: 'cold',\n",
       " 292: 'being',\n",
       " 293: 'watch',\n",
       " 294: 'shes',\n",
       " 295: 'everyone',\n",
       " 296: 'around',\n",
       " 297: 'few',\n",
       " 298: 'japan',\n",
       " 299: 'almost',\n",
       " 300: 'drink',\n",
       " 301: 'young',\n",
       " 302: 'youll',\n",
       " 303: 'pay',\n",
       " 304: 'study',\n",
       " 305: 'met',\n",
       " 306: 'girl',\n",
       " 307: 'another',\n",
       " 308: 'wish',\n",
       " 309: 'bus',\n",
       " 310: 'mind',\n",
       " 311: 'letter',\n",
       " 312: 'kind',\n",
       " 313: 'died',\n",
       " 314: 'books',\n",
       " 315: 'learn',\n",
       " 316: 'sleep',\n",
       " 317: 'days',\n",
       " 318: 'open',\n",
       " 319: 'afraid',\n",
       " 320: 'most',\n",
       " 321: 'someone',\n",
       " 322: 'once',\n",
       " 323: 'sister',\n",
       " 324: 'world',\n",
       " 325: 'talking',\n",
       " 326: 'parents',\n",
       " 327: 'which',\n",
       " 328: 'seems',\n",
       " 329: 'each',\n",
       " 330: 'ten',\n",
       " 331: 'those',\n",
       " 332: 'place',\n",
       " 333: 'looked',\n",
       " 334: 'until',\n",
       " 335: 'walk',\n",
       " 336: 'rain',\n",
       " 337: 'getting',\n",
       " 338: 'happen',\n",
       " 339: 'turn',\n",
       " 340: 'difficult',\n",
       " 341: 'angry',\n",
       " 342: 'care',\n",
       " 343: 'anyone',\n",
       " 344: 'truth',\n",
       " 345: 'own',\n",
       " 346: 'write',\n",
       " 347: 'together',\n",
       " 348: 'both',\n",
       " 349: 'cat',\n",
       " 350: 'lunch',\n",
       " 351: 'havent',\n",
       " 352: 'coming',\n",
       " 353: 'wouldnt',\n",
       " 354: 'dinner',\n",
       " 355: 'ago',\n",
       " 356: 'family',\n",
       " 357: 'early',\n",
       " 358: 'plan',\n",
       " 359: 'else',\n",
       " 360: 'yourself',\n",
       " 361: 'monday',\n",
       " 362: 'mine',\n",
       " 363: 'country',\n",
       " 364: 'tried',\n",
       " 365: 'youve',\n",
       " 366: 'show',\n",
       " 367: 'tv',\n",
       " 368: 'playing',\n",
       " 369: 'favorite',\n",
       " 370: 'hours',\n",
       " 371: 'hurt',\n",
       " 372: 'doctor',\n",
       " 373: 'accident',\n",
       " 374: 'great',\n",
       " 375: 'waiting',\n",
       " 376: 'table',\n",
       " 377: 'question',\n",
       " 378: 'arent',\n",
       " 379: 'wife',\n",
       " 380: 'minutes',\n",
       " 381: 'eating',\n",
       " 382: 'child',\n",
       " 383: 'lives',\n",
       " 384: 'reading',\n",
       " 385: 'five',\n",
       " 386: 'everybody',\n",
       " 387: 'since',\n",
       " 388: 'japanese',\n",
       " 389: 'nobody',\n",
       " 390: 'himself',\n",
       " 391: 'hate',\n",
       " 392: 'far',\n",
       " 393: 'easy',\n",
       " 394: 'says',\n",
       " 395: 'ate',\n",
       " 396: 'such',\n",
       " 397: 'sorry',\n",
       " 398: 'ready',\n",
       " 399: 'station',\n",
       " 400: 'tonight',\n",
       " 401: 'police',\n",
       " 402: 'nice',\n",
       " 403: 'might',\n",
       " 404: 'meeting',\n",
       " 405: 'under',\n",
       " 406: 'phone',\n",
       " 407: 'meet',\n",
       " 408: 'finished',\n",
       " 409: 'matter',\n",
       " 410: 'office',\n",
       " 411: 'arrived',\n",
       " 412: 'trying',\n",
       " 413: 'times',\n",
       " 414: 'swim',\n",
       " 415: 'students',\n",
       " 416: 'hand',\n",
       " 417: 'started',\n",
       " 418: 'start',\n",
       " 419: 'ran',\n",
       " 420: 'decided',\n",
       " 421: 'important',\n",
       " 422: 'having',\n",
       " 423: 'shoes',\n",
       " 424: 'month',\n",
       " 425: 'felt',\n",
       " 426: 'son',\n",
       " 427: 'woman',\n",
       " 428: 'story',\n",
       " 429: 'called',\n",
       " 430: 'trouble',\n",
       " 431: 'pretty',\n",
       " 432: 'goes',\n",
       " 433: 'city',\n",
       " 434: 'window',\n",
       " 435: 'small',\n",
       " 436: 'drive',\n",
       " 437: 'shouldnt',\n",
       " 438: 'word',\n",
       " 439: 'weve',\n",
       " 440: 'music',\n",
       " 441: 'change',\n",
       " 442: 'myself',\n",
       " 443: 'game',\n",
       " 444: 'fire',\n",
       " 445: 'fast',\n",
       " 446: 'hot',\n",
       " 447: 'anymore',\n",
       " 448: 'sick',\n",
       " 449: 'usually',\n",
       " 450: 'mistake',\n",
       " 451: 'homework',\n",
       " 452: 'town',\n",
       " 453: 'park',\n",
       " 454: 'eyes',\n",
       " 455: 'studying',\n",
       " 456: 'song',\n",
       " 457: 'began',\n",
       " 458: 'visit',\n",
       " 459: 'six',\n",
       " 460: 'fun',\n",
       " 461: 'working',\n",
       " 462: 'picture',\n",
       " 463: 'interesting',\n",
       " 464: 'gone',\n",
       " 465: 'christmas',\n",
       " 466: 'stupid',\n",
       " 467: 'language',\n",
       " 468: 'hair',\n",
       " 469: 'tennis',\n",
       " 470: 'run',\n",
       " 471: 'needs',\n",
       " 472: 'listen',\n",
       " 473: 'fish',\n",
       " 474: 'lived',\n",
       " 475: 'hotel',\n",
       " 476: 'forget',\n",
       " 477: 'baby',\n",
       " 478: 'thank',\n",
       " 479: 'red',\n",
       " 480: 'broke',\n",
       " 481: 'against',\n",
       " 482: 'afternoon',\n",
       " 483: 'milk',\n",
       " 484: 'through',\n",
       " 485: 'spend',\n",
       " 486: 'rich',\n",
       " 487: 'free',\n",
       " 488: 'weather',\n",
       " 489: 'quite',\n",
       " 490: 'kept',\n",
       " 491: 'longer',\n",
       " 492: 'advice',\n",
       " 493: 'watching',\n",
       " 494: 'summer',\n",
       " 495: 'fell',\n",
       " 496: 'caught',\n",
       " 497: 'breakfast',\n",
       " 498: 'then',\n",
       " 499: 'comes',\n",
       " 500: 'finish',\n",
       " 501: 'face',\n",
       " 502: 'older',\n",
       " 503: 'number',\n",
       " 504: 'smoking',\n",
       " 505: 'men',\n",
       " 506: 'thinking',\n",
       " 507: 'makes',\n",
       " 508: 'large',\n",
       " 509: 'hour',\n",
       " 510: 'killed',\n",
       " 511: 'sit',\n",
       " 512: 'loves',\n",
       " 513: 'hospital',\n",
       " 514: 'class',\n",
       " 515: 'stopped',\n",
       " 516: 'seem',\n",
       " 517: 'news',\n",
       " 518: 'movie',\n",
       " 519: 'along',\n",
       " 520: 'mean',\n",
       " 521: 'later',\n",
       " 522: 'dark',\n",
       " 523: 'tokyo',\n",
       " 524: 'short',\n",
       " 525: 'cake',\n",
       " 526: 'bit',\n",
       " 527: 'australia',\n",
       " 528: 'whos',\n",
       " 529: 'turned',\n",
       " 530: 'questions',\n",
       " 531: 'making',\n",
       " 532: 'snow',\n",
       " 533: 'camera',\n",
       " 534: 'bring',\n",
       " 535: 'became',\n",
       " 536: 'youd',\n",
       " 537: 'secret',\n",
       " 538: 'river',\n",
       " 539: 'present',\n",
       " 540: 'living',\n",
       " 541: 'high',\n",
       " 542: 'close',\n",
       " 543: 'chance',\n",
       " 544: 'sat',\n",
       " 545: 'hit',\n",
       " 546: 'full',\n",
       " 547: 'box',\n",
       " 548: 'works',\n",
       " 549: 'war',\n",
       " 550: 'tree',\n",
       " 551: 'student',\n",
       " 552: 'street',\n",
       " 553: 'others',\n",
       " 554: 'glad',\n",
       " 555: 'end',\n",
       " 556: 'america',\n",
       " 557: 'lie',\n",
       " 558: 'death',\n",
       " 559: 'careful',\n",
       " 560: 'white',\n",
       " 561: 'dream',\n",
       " 562: 'cannot',\n",
       " 563: 'surprised',\n",
       " 564: 'hungry',\n",
       " 565: 'gets',\n",
       " 566: 'age',\n",
       " 567: 'wonder',\n",
       " 568: 'trust',\n",
       " 569: 'kill',\n",
       " 570: 'forgot',\n",
       " 571: 'dollars',\n",
       " 572: 'miss',\n",
       " 573: 'cut',\n",
       " 574: 'computer',\n",
       " 575: 'born',\n",
       " 576: 'thinks',\n",
       " 577: 'hurry',\n",
       " 578: 'hands',\n",
       " 579: 'flowers',\n",
       " 580: 'store',\n",
       " 581: 'plane',\n",
       " 582: 'between',\n",
       " 583: 'wine',\n",
       " 584: 'spent',\n",
       " 585: 'reason',\n",
       " 586: 'clothes',\n",
       " 587: 'possible',\n",
       " 588: 'moment',\n",
       " 589: 'die',\n",
       " 590: 'dead',\n",
       " 591: 'bicycle',\n",
       " 592: '230',\n",
       " 593: 'stand',\n",
       " 594: 'sometimes',\n",
       " 595: 'dogs',\n",
       " 596: 'taking',\n",
       " 597: 'near',\n",
       " 598: 'expensive',\n",
       " 599: 'clean',\n",
       " 600: 'worked',\n",
       " 601: 'saying',\n",
       " 602: 'hasnt',\n",
       " 603: 'company',\n",
       " 604: 'sing',\n",
       " 605: 'oclock',\n",
       " 606: 'key',\n",
       " 607: 'daughter',\n",
       " 608: 'tea',\n",
       " 609: 'speaks',\n",
       " 610: 'speaking',\n",
       " 611: 'light',\n",
       " 612: 'building',\n",
       " 613: 'beer',\n",
       " 614: 'whether',\n",
       " 615: 'running',\n",
       " 616: 'rather',\n",
       " 617: 'piano',\n",
       " 618: 'girlfriend',\n",
       " 619: 'four',\n",
       " 620: 'advised',\n",
       " 621: 'changed',\n",
       " 622: 'canadian',\n",
       " 623: 'birthday',\n",
       " 624: 'women',\n",
       " 625: 'send',\n",
       " 626: 'words',\n",
       " 627: 'wrote',\n",
       " 628: 'rest',\n",
       " 629: 'order',\n",
       " 630: 'needed',\n",
       " 631: 'become',\n",
       " 632: 'anybody',\n",
       " 633: 'sunday',\n",
       " 634: 'business',\n",
       " 635: 'strange',\n",
       " 636: 'helped',\n",
       " 637: 'drunk',\n",
       " 638: 'dress',\n",
       " 639: 'crazy',\n",
       " 640: 'broken',\n",
       " 641: 'quickly',\n",
       " 642: 'paper',\n",
       " 643: 'floor',\n",
       " 644: 'dictionary',\n",
       " 645: 'cup',\n",
       " 646: 'asleep',\n",
       " 647: 'real',\n",
       " 648: 'outside',\n",
       " 649: 'maybe',\n",
       " 650: 'herself',\n",
       " 651: 'different',\n",
       " 652: 'trip',\n",
       " 653: 'quit',\n",
       " 654: 'part',\n",
       " 655: 'exactly',\n",
       " 656: 'behind',\n",
       " 657: 'air',\n",
       " 658: 'restaurant',\n",
       " 659: 'problems',\n",
       " 660: 'lose',\n",
       " 661: 'least',\n",
       " 662: 'interested',\n",
       " 663: 'hold',\n",
       " 664: 'dangerous',\n",
       " 665: 'concert',\n",
       " 666: 'boys',\n",
       " 667: 'black',\n",
       " 668: 'paid',\n",
       " 669: 'loved',\n",
       " 670: 'health',\n",
       " 671: 'guitar',\n",
       " 672: 'glasses',\n",
       " 673: 'eaten',\n",
       " 674: 'cats',\n",
       " 675: 'wear',\n",
       " 676: 'takes',\n",
       " 677: 'swimming',\n",
       " 678: 'sad',\n",
       " 679: 'ice',\n",
       " 680: 'hide',\n",
       " 681: 'cook',\n",
       " 682: 'bank',\n",
       " 683: 'whole',\n",
       " 684: 'team',\n",
       " 685: 'tall',\n",
       " 686: 'stayed',\n",
       " 687: 'return',\n",
       " 688: 'proud',\n",
       " 689: 'move',\n",
       " 690: 'hell',\n",
       " 691: 'heart',\n",
       " 692: 'during',\n",
       " 693: 'catch',\n",
       " 694: 'break',\n",
       " 695: 'worry',\n",
       " 696: 'wheres',\n",
       " 697: 'wearing',\n",
       " 698: 'waited',\n",
       " 699: 'touch',\n",
       " 700: 'probably',\n",
       " 701: 'famous',\n",
       " 702: 'explain',\n",
       " 703: 'cry',\n",
       " 704: 'blame',\n",
       " 705: 'telephone',\n",
       " 706: 'noise',\n",
       " 707: 'mistakes',\n",
       " 708: 'forward',\n",
       " 709: 'enjoy',\n",
       " 710: 'either',\n",
       " 711: 'uncle',\n",
       " 712: 'raining',\n",
       " 713: 'opened',\n",
       " 714: 'ok',\n",
       " 715: 'movies',\n",
       " 716: 'learned',\n",
       " 717: 'head',\n",
       " 718: 'baseball',\n",
       " 719: 'strong',\n",
       " 720: 'seemed',\n",
       " 721: 'pain',\n",
       " 722: 'driving',\n",
       " 723: 'yours',\n",
       " 724: 'played',\n",
       " 725: 'missed',\n",
       " 726: 'library',\n",
       " 727: 'lake',\n",
       " 728: 'bag',\n",
       " 729: 'united',\n",
       " 730: 'talked',\n",
       " 731: 'solve',\n",
       " 732: 'side',\n",
       " 733: 'half',\n",
       " 734: 'alive',\n",
       " 735: 'agree',\n",
       " 736: 'walked',\n",
       " 737: 'umbrella',\n",
       " 738: 'seven',\n",
       " 739: 'road',\n",
       " 740: 'promise',\n",
       " 741: 'blue',\n",
       " 742: 'attention',\n",
       " 743: 'airport',\n",
       " 744: 'television',\n",
       " 745: 'showed',\n",
       " 746: 'poor',\n",
       " 747: 'minute',\n",
       " 748: 'meat',\n",
       " 749: 'lying',\n",
       " 750: 'lucky',\n",
       " 751: 'leaving',\n",
       " 752: 'kids',\n",
       " 753: 'crying',\n",
       " 754: 'choice',\n",
       " 755: 'brought',\n",
       " 756: 'win',\n",
       " 757: 'weight',\n",
       " 758: 'thanks',\n",
       " 759: 'shirt',\n",
       " 760: 'known',\n",
       " 761: 'glass',\n",
       " 762: 'feeling',\n",
       " 763: 'thirty',\n",
       " 764: 'states',\n",
       " 765: 'sound',\n",
       " 766: 'sleeping',\n",
       " 767: 'prefer',\n",
       " 768: 'hat',\n",
       " 769: 'garden',\n",
       " 770: 'fine',\n",
       " 771: 'animals',\n",
       " 772: 'set',\n",
       " 773: 'mad',\n",
       " 774: 'guy',\n",
       " 775: 'guess',\n",
       " 776: 'fishing',\n",
       " 777: 'drinking',\n",
       " 778: 'visited',\n",
       " 779: 'smoke',\n",
       " 780: 'radio',\n",
       " 781: 'listening',\n",
       " 782: 'lawyer',\n",
       " 783: 'kitchen',\n",
       " 784: 'finally',\n",
       " 785: 'evening',\n",
       " 786: 'ship',\n",
       " 787: 'seat',\n",
       " 788: 'pass',\n",
       " 789: 'paris',\n",
       " 790: 'learning',\n",
       " 791: 'girls',\n",
       " 792: 'front',\n",
       " 793: 'cost',\n",
       " 794: 'beach',\n",
       " 795: 'whose',\n",
       " 796: 'twice',\n",
       " 797: 'though',\n",
       " 798: 'spoke',\n",
       " 799: 'somebody',\n",
       " 800: 'several',\n",
       " 801: 'seeing',\n",
       " 802: 'safe',\n",
       " 803: 'point',\n",
       " 804: 'follow',\n",
       " 805: 'dance',\n",
       " 806: 'apple',\n",
       " 807: 'wash',\n",
       " 808: 'taken',\n",
       " 809: 'sun',\n",
       " 810: 'shut',\n",
       " 811: 'rules',\n",
       " 812: 'london',\n",
       " 813: 'knife',\n",
       " 814: 'itll',\n",
       " 815: 'accept',\n",
       " 816: 'younger',\n",
       " 817: 'winter',\n",
       " 818: 'sent',\n",
       " 819: 'sense',\n",
       " 820: 'fight',\n",
       " 821: 'closed',\n",
       " 822: 'worried',\n",
       " 823: 'travel',\n",
       " 824: 'test',\n",
       " 825: 'situation',\n",
       " 826: 'second',\n",
       " 827: 'invited',\n",
       " 828: 'decision',\n",
       " 829: 'danger',\n",
       " 830: 'arrive',\n",
       " 831: 'worse',\n",
       " 832: 'won',\n",
       " 833: 'wall',\n",
       " 834: 'walking',\n",
       " 835: 'vacation',\n",
       " 836: 'ticket',\n",
       " 837: 'sitting',\n",
       " 838: 'languages',\n",
       " 839: 'doubt',\n",
       " 840: 'begin',\n",
       " 841: 'abroad',\n",
       " 842: 'writing',\n",
       " 843: 'werent',\n",
       " 844: 'passed',\n",
       " 845: 'months',\n",
       " 846: 'less',\n",
       " 847: 'excuse',\n",
       " 848: 'eggs',\n",
       " 849: 'completely',\n",
       " 850: 'apples',\n",
       " 851: 'worth',\n",
       " 852: 'smile',\n",
       " 853: 'shouldve',\n",
       " 854: 'message',\n",
       " 855: 'leaves',\n",
       " 856: 'church',\n",
       " 857: 'brothers',\n",
       " 858: 'also',\n",
       " 859: 'weekend',\n",
       " 860: 'traffic',\n",
       " 861: 'tie',\n",
       " 862: 'soccer',\n",
       " 863: 'mountain',\n",
       " 864: 'kiss',\n",
       " 865: 'impossible',\n",
       " 866: 'husband',\n",
       " 867: 'feed',\n",
       " 868: 'drank',\n",
       " 869: 'success',\n",
       " 870: 'sounds',\n",
       " 871: 'singing',\n",
       " 872: 'shopping',\n",
       " 873: 'promised',\n",
       " 874: 'pen',\n",
       " 875: 'patient',\n",
       " 876: 'horse',\n",
       " 877: 'china',\n",
       " 878: 'address',\n",
       " 879: 'wake',\n",
       " 880: 'refused',\n",
       " 881: 'piece',\n",
       " 882: 'opinion',\n",
       " 883: 'medicine',\n",
       " 884: 'liked',\n",
       " 885: 'coat',\n",
       " 886: 'cars',\n",
       " 887: 'perfect',\n",
       " 888: 'machine',\n",
       " 889: 'hiding',\n",
       " 890: 'happens',\n",
       " 891: 'expect',\n",
       " 892: 'case',\n",
       " 893: 'across',\n",
       " 894: 'teach',\n",
       " 895: 'shot',\n",
       " 896: 'pictures',\n",
       " 897: 'mustve',\n",
       " 898: 'hed',\n",
       " 899: 'grandfather',\n",
       " 900: 'funny',\n",
       " 901: 'fault',\n",
       " 902: 'expected',\n",
       " 903: 'easily',\n",
       " 904: 'deal',\n",
       " 905: 'written',\n",
       " 906: 'waste',\n",
       " 907: 'till',\n",
       " 908: 'stolen',\n",
       " 909: 'shop',\n",
       " 910: 'plans',\n",
       " 911: 'means',\n",
       " 912: 'happening',\n",
       " 913: 'france',\n",
       " 914: 'foot',\n",
       " 915: 'desk',\n",
       " 916: 'bread',\n",
       " 917: 'arm',\n",
       " 918: 'american',\n",
       " 919: 'surprise',\n",
       " 920: 'serious',\n",
       " 921: 'pick',\n",
       " 922: 'offer',\n",
       " 923: 'necessary',\n",
       " 924: 'heavy',\n",
       " 925: 'future',\n",
       " 926: 'clear',\n",
       " 927: 'carefully',\n",
       " 928: 'boyfriend',\n",
       " 929: 'bath',\n",
       " 930: 'actually',\n",
       " 931: 'somewhere',\n",
       " 932: 'share',\n",
       " 933: 'foreign',\n",
       " 934: 'figure',\n",
       " 935: 'enemy',\n",
       " 936: 'cream',\n",
       " 937: 'telling',\n",
       " 938: 'sky',\n",
       " 939: 'sell',\n",
       " 940: 'law',\n",
       " 941: 'keys',\n",
       " 942: 'given',\n",
       " 943: 'fix',\n",
       " 944: 'failed',\n",
       " 945: 'couple',\n",
       " 946: 'terrible',\n",
       " 947: 'hardly',\n",
       " 948: 'correct',\n",
       " 949: 'carry',\n",
       " 950: 'boss',\n",
       " 951: 'york',\n",
       " 952: 'warm',\n",
       " 953: 'salt',\n",
       " 954: 'policeman',\n",
       " 955: 'moved',\n",
       " 956: 'marry',\n",
       " 957: 'kissed',\n",
       " 958: 'guys',\n",
       " 959: 'eats',\n",
       " 960: 'choose',\n",
       " 961: 'yes',\n",
       " 962: 'past',\n",
       " 963: 'novel',\n",
       " 964: 'none',\n",
       " 965: 'laughed',\n",
       " 966: 'inside',\n",
       " 967: 'fly',\n",
       " 968: 'favor',\n",
       " 969: 'college',\n",
       " 970: 'chinese',\n",
       " 971: 'check',\n",
       " 972: 'chair',\n",
       " 973: 'bird',\n",
       " 974: 'trees',\n",
       " 975: 'supposed',\n",
       " 976: 'sugar',\n",
       " 977: 'spoken',\n",
       " 978: 'sign',\n",
       " 979: 'shall',\n",
       " 980: 'joke',\n",
       " 981: 'honest',\n",
       " 982: 'save',\n",
       " 983: 'prison',\n",
       " 984: 'nervous',\n",
       " 985: 'gun',\n",
       " 986: 'green',\n",
       " 987: 'german',\n",
       " 988: 'fear',\n",
       " 989: 'entered',\n",
       " 990: 'wallet',\n",
       " 991: 'taxi',\n",
       " 992: 'studied',\n",
       " 993: 'sisters',\n",
       " 994: 'popular',\n",
       " 995: 'planning',\n",
       " 996: 'owe',\n",
       " 997: 'neither',\n",
       " 998: 'lend',\n",
       " 999: 'hundred',\n",
       " ...}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_index = {i:word for i,word in enumerate(source_vocab)}\n",
    "source_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "074179c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " '[start]',\n",
       " '[end]',\n",
       " 'de',\n",
       " 'que',\n",
       " 'a',\n",
       " 'no',\n",
       " 'tom',\n",
       " 'la',\n",
       " 'el',\n",
       " 'en',\n",
       " 'es',\n",
       " 'un',\n",
       " 'me',\n",
       " 'se',\n",
       " 'por',\n",
       " 'lo',\n",
       " 'una',\n",
       " 'los',\n",
       " 'su',\n",
       " 'Él',\n",
       " 'está',\n",
       " 'con',\n",
       " 'mi',\n",
       " 'le',\n",
       " 'ella',\n",
       " 'qué',\n",
       " 'te',\n",
       " 'para',\n",
       " 'mary',\n",
       " 'y',\n",
       " 'las',\n",
       " 'más',\n",
       " 'al',\n",
       " 'yo',\n",
       " 'tu',\n",
       " 'estoy',\n",
       " 'muy',\n",
       " 'eso',\n",
       " 'tiene',\n",
       " 'este',\n",
       " 'esta',\n",
       " 'del',\n",
       " 'él',\n",
       " 'estaba',\n",
       " 'tengo',\n",
       " 'quiero',\n",
       " 'fue',\n",
       " 'si',\n",
       " 'aquí',\n",
       " 'casa',\n",
       " 'como',\n",
       " 'hacer',\n",
       " 'puedo',\n",
       " 'algo',\n",
       " 'todo',\n",
       " 'esto',\n",
       " 'hay',\n",
       " 'tiempo',\n",
       " 'ha',\n",
       " 'gusta',\n",
       " 'todos',\n",
       " 'tan',\n",
       " 'son',\n",
       " 'nada',\n",
       " 'cuando',\n",
       " 'favor',\n",
       " 'ir',\n",
       " 'vez',\n",
       " 'era',\n",
       " 'puede',\n",
       " 'bien',\n",
       " 'mucho',\n",
       " 'he',\n",
       " 'ellos',\n",
       " 'nos',\n",
       " 'sé',\n",
       " 'solo',\n",
       " 'mañana',\n",
       " 'nunca',\n",
       " 'ser',\n",
       " 'dos',\n",
       " 'ya',\n",
       " 'creo',\n",
       " 'sus',\n",
       " 'trabajo',\n",
       " 'estás',\n",
       " 'tienes',\n",
       " 'dónde',\n",
       " 'cómo',\n",
       " 'dinero',\n",
       " 'dijo',\n",
       " 'ahora',\n",
       " 'hablar',\n",
       " 'tomás',\n",
       " 'quién',\n",
       " 'están',\n",
       " 'pero',\n",
       " 'soy',\n",
       " 'día',\n",
       " 'había',\n",
       " 'hace',\n",
       " 'ese',\n",
       " 'siempre',\n",
       " 'tú',\n",
       " 'libro',\n",
       " 'puedes',\n",
       " 'poco',\n",
       " 'hoy',\n",
       " 'quiere',\n",
       " 'esa',\n",
       " 'verdad',\n",
       " 'nadie',\n",
       " 'años',\n",
       " 'voy',\n",
       " 'estar',\n",
       " 'ver',\n",
       " 'sabe',\n",
       " 'quería',\n",
       " 'francés',\n",
       " 'va',\n",
       " 'mejor',\n",
       " 'has',\n",
       " 'maría',\n",
       " 'quieres',\n",
       " 'demasiado',\n",
       " 'eres',\n",
       " 'tarde',\n",
       " 'hizo',\n",
       " 'padre',\n",
       " 'antes',\n",
       " 'parece',\n",
       " 'mis',\n",
       " 'o',\n",
       " 'tres',\n",
       " 'usted',\n",
       " 'tenía',\n",
       " 'noche',\n",
       " 'boston',\n",
       " 'sobre',\n",
       " 'hora',\n",
       " 'sin',\n",
       " 'hasta',\n",
       " 'todavía',\n",
       " 'mí',\n",
       " 'ayer',\n",
       " 'perro',\n",
       " 'estado',\n",
       " 'inglés',\n",
       " 'haber',\n",
       " 'hombre',\n",
       " 'comer',\n",
       " 'vida',\n",
       " 'nuevo',\n",
       " 'gente',\n",
       " 'alguien',\n",
       " 'cuándo',\n",
       " 'necesito',\n",
       " 'tenemos',\n",
       " 'días',\n",
       " 'mismo',\n",
       " 'cuál',\n",
       " 'coche',\n",
       " 'problema',\n",
       " 'nosotros',\n",
       " 'madre',\n",
       " 'decir',\n",
       " 'así',\n",
       " 'hecho',\n",
       " 'gustaría',\n",
       " 'otra',\n",
       " 'pensé',\n",
       " 'semana',\n",
       " 'sabía',\n",
       " 'estamos',\n",
       " 'buen',\n",
       " 'pasado',\n",
       " 'después',\n",
       " 'uno',\n",
       " 'habitación',\n",
       " 'cosas',\n",
       " 'ti',\n",
       " 'debería',\n",
       " 'tus',\n",
       " 'puerta',\n",
       " 'deberías',\n",
       " 'casi',\n",
       " 'vas',\n",
       " 'alguna',\n",
       " 'niños',\n",
       " 'mundo',\n",
       " 'hacerlo',\n",
       " 'amigos',\n",
       " 'escuela',\n",
       " 'realmente',\n",
       " 'feliz',\n",
       " 'difícil',\n",
       " 'fuera',\n",
       " 'allí',\n",
       " 'otro',\n",
       " 'saber',\n",
       " 'niño',\n",
       " 'les',\n",
       " 'cuánto',\n",
       " 'podría',\n",
       " 'pronto',\n",
       " 'veces',\n",
       " 'menos',\n",
       " 'ni',\n",
       " 'idea',\n",
       " 'dio',\n",
       " 'año',\n",
       " 'buena',\n",
       " 'ahí',\n",
       " 'seguro',\n",
       " 'habla',\n",
       " 'ayuda',\n",
       " 'visto',\n",
       " 'ciudad',\n",
       " 'desde',\n",
       " 'muchos',\n",
       " 'agua',\n",
       " 'auto',\n",
       " 'sólo',\n",
       " 'amigo',\n",
       " 'sabes',\n",
       " 'nuestro',\n",
       " 'todas',\n",
       " 'mal',\n",
       " 'persona',\n",
       " 'comida',\n",
       " 'nombre',\n",
       " 'john',\n",
       " 'vino',\n",
       " 'durante',\n",
       " 'conmigo',\n",
       " 'comprar',\n",
       " 'tren',\n",
       " 'cada',\n",
       " 'personas',\n",
       " 'café',\n",
       " 'vi',\n",
       " 'tener',\n",
       " 'cama',\n",
       " 'dice',\n",
       " 'hermano',\n",
       " 'gran',\n",
       " 'haciendo',\n",
       " 'tanto',\n",
       " 'japón',\n",
       " 'contigo',\n",
       " 'toda',\n",
       " 'salir',\n",
       " 'profesor',\n",
       " 'clase',\n",
       " 'fiesta',\n",
       " 'grande',\n",
       " 'muchas',\n",
       " 'diez',\n",
       " 'bueno',\n",
       " 'cuenta',\n",
       " 'carta',\n",
       " 'sea',\n",
       " 'acerca',\n",
       " 'aún',\n",
       " 'policía',\n",
       " 'libros',\n",
       " 'hermana',\n",
       " 'podía',\n",
       " 'nuestra',\n",
       " 'momento',\n",
       " 'rápido',\n",
       " 'lugar',\n",
       " 'tienen',\n",
       " 'porque',\n",
       " 'pasó',\n",
       " 'pudo',\n",
       " 'padres',\n",
       " 'joven',\n",
       " 'dejó',\n",
       " 'leer',\n",
       " 'siento',\n",
       " 'gustan',\n",
       " 'fui',\n",
       " 'espero',\n",
       " 'vive',\n",
       " 'tuvo',\n",
       " 'problemas',\n",
       " 'cerca',\n",
       " 'vamos',\n",
       " 'trabajar',\n",
       " 'mujer',\n",
       " 'horas',\n",
       " 'han',\n",
       " 'lunes',\n",
       " 'puso',\n",
       " 'tomar',\n",
       " 'teléfono',\n",
       " 'sido',\n",
       " 'podemos',\n",
       " 'entre',\n",
       " 'historia',\n",
       " 'acuerdo',\n",
       " 'cosa',\n",
       " 'accidente',\n",
       " 'donde',\n",
       " 'también',\n",
       " 'parte',\n",
       " 'hijo',\n",
       " 'debe',\n",
       " 'anoche',\n",
       " 'murió',\n",
       " 'cinco',\n",
       " 'estaban',\n",
       " 'sí',\n",
       " 'nueva',\n",
       " 'debes',\n",
       " 'venir',\n",
       " 'gato',\n",
       " 'nadar',\n",
       " 'familia',\n",
       " 'fácil',\n",
       " 'mucha',\n",
       " 'minutos',\n",
       " 'algunos',\n",
       " 'mayor',\n",
       " 'ayudar',\n",
       " 'última',\n",
       " 'jugar',\n",
       " 'ocupado',\n",
       " 'mesa',\n",
       " 'temprano',\n",
       " 'televisión',\n",
       " 'pregunta',\n",
       " 'menudo',\n",
       " 'estos',\n",
       " 'estación',\n",
       " 'vivir',\n",
       " 'hice',\n",
       " 'gracias',\n",
       " 'camino',\n",
       " 'viene',\n",
       " 'aprender',\n",
       " 'mano',\n",
       " 'razón',\n",
       " 'llegar',\n",
       " 'crees',\n",
       " 'país',\n",
       " 'dije',\n",
       " 'pidió',\n",
       " 'mientras',\n",
       " 'encontrar',\n",
       " 'dormir',\n",
       " 'volver',\n",
       " 'bastante',\n",
       " 'suficiente',\n",
       " 'dicho',\n",
       " 'pasar',\n",
       " 'estudiar',\n",
       " 'algunas',\n",
       " 'miedo',\n",
       " 'preguntó',\n",
       " 'alto',\n",
       " 'algún',\n",
       " 'dejar',\n",
       " 'zapatos',\n",
       " 'será',\n",
       " 'lado',\n",
       " 'mes',\n",
       " 'compró',\n",
       " 'ustedes',\n",
       " 'estuvo',\n",
       " 'conozco',\n",
       " 'quisiera',\n",
       " 'hablando',\n",
       " 'tipo',\n",
       " 'fuerte',\n",
       " 've',\n",
       " 'unos',\n",
       " 'tuve',\n",
       " 'colegio',\n",
       " 'ventana',\n",
       " 'seis',\n",
       " 'cansado',\n",
       " 'podrías',\n",
       " 'hijos',\n",
       " 'vio',\n",
       " 'pienso',\n",
       " 'música',\n",
       " 'frío',\n",
       " 'ningún',\n",
       " 'llegó',\n",
       " 'juntos',\n",
       " 'importante',\n",
       " 'error',\n",
       " 'cabeza',\n",
       " 'necesita',\n",
       " 'punto',\n",
       " 'oficina',\n",
       " 'importa',\n",
       " 'fueron',\n",
       " 'canción',\n",
       " 'ojos',\n",
       " 'esperando',\n",
       " 'hemos',\n",
       " 'hacia',\n",
       " 'creer',\n",
       " 'somos',\n",
       " 'reloj',\n",
       " 'os',\n",
       " 'estados',\n",
       " 'esperar',\n",
       " 'unidos',\n",
       " 'reunión',\n",
       " 'lleva',\n",
       " 'estudiantes',\n",
       " 'viejo',\n",
       " 'solía',\n",
       " 'película',\n",
       " 'ido',\n",
       " 'cualquier',\n",
       " 'sola',\n",
       " 'salió',\n",
       " 'primera',\n",
       " 'debo',\n",
       " 'verte',\n",
       " 'misma',\n",
       " 'fumar',\n",
       " '¡qué',\n",
       " 'vos',\n",
       " 'queda',\n",
       " 'pueden',\n",
       " 'plan',\n",
       " 'ganas',\n",
       " 'único',\n",
       " 'sueño',\n",
       " 'hotel',\n",
       " 'fin',\n",
       " 'edad',\n",
       " 'buscando',\n",
       " 'bicicleta',\n",
       " 'estas',\n",
       " 'avión',\n",
       " 'palabra',\n",
       " 'dar',\n",
       " 'llave',\n",
       " 'lejos',\n",
       " 'leche',\n",
       " 'cuarto',\n",
       " 'tenis',\n",
       " 'oído',\n",
       " 'media',\n",
       " 'esposa',\n",
       " 'tal',\n",
       " 'querés',\n",
       " 'tienda',\n",
       " 'respuesta',\n",
       " 'haga',\n",
       " 'número',\n",
       " 'deja',\n",
       " 'pasa',\n",
       " 'interesante',\n",
       " 'conducir',\n",
       " 'caja',\n",
       " 'bajo',\n",
       " 'verano',\n",
       " 'pelo',\n",
       " 'da',\n",
       " 'necesitas',\n",
       " 'juego',\n",
       " 'contra',\n",
       " 'chico',\n",
       " 'quien',\n",
       " 'iba',\n",
       " 'compré',\n",
       " 'sos',\n",
       " 'manera',\n",
       " 'guerra',\n",
       " 'chica',\n",
       " 'veo',\n",
       " 'parecía',\n",
       " 'cree',\n",
       " 'bebé',\n",
       " 'autobús',\n",
       " 'sería',\n",
       " 'hubiera',\n",
       " 'dólares',\n",
       " 'dolor',\n",
       " 'preguntas',\n",
       " 'mío',\n",
       " 'hombres',\n",
       " 'estabas',\n",
       " 'australia',\n",
       " 'río',\n",
       " 'pueda',\n",
       " 'navidad',\n",
       " 'hospital',\n",
       " 'cámara',\n",
       " 'capaz',\n",
       " 'ropa',\n",
       " 'escribir',\n",
       " 'deberíamos',\n",
       " 'vaya',\n",
       " 'listo',\n",
       " 'encanta',\n",
       " 'usar',\n",
       " 'terminar',\n",
       " 'realidad',\n",
       " 'pude',\n",
       " 'pensar',\n",
       " 'ninguna',\n",
       " 'esté',\n",
       " 'calle',\n",
       " 'tokio',\n",
       " 'quedó',\n",
       " 'tomó',\n",
       " 'perdió',\n",
       " 'perdido',\n",
       " 'cierto',\n",
       " 'calor',\n",
       " 'vivo',\n",
       " 'secreto',\n",
       " 'perder',\n",
       " 'ojalá',\n",
       " 'cuántos',\n",
       " 'posible',\n",
       " 'hija',\n",
       " 'hambre',\n",
       " 'cuanto',\n",
       " 'ven',\n",
       " 'parque',\n",
       " 'encontré',\n",
       " 'trabaja',\n",
       " 'palabras',\n",
       " 'odio',\n",
       " 'haya',\n",
       " 'duro',\n",
       " 'vacaciones',\n",
       " 'queremos',\n",
       " 'oír',\n",
       " 'libre',\n",
       " 'flores',\n",
       " 'extraño',\n",
       " 'cuidado',\n",
       " 'empezó',\n",
       " 'tocar',\n",
       " 'manos',\n",
       " 'sol',\n",
       " 'muerte',\n",
       " 'consejo',\n",
       " 'cocina',\n",
       " 'piensas',\n",
       " 'pequeño',\n",
       " 'forma',\n",
       " 'viaje',\n",
       " 'terminado',\n",
       " 'regalo',\n",
       " 'piensa',\n",
       " 'otros',\n",
       " 'malo',\n",
       " 'luz',\n",
       " 'enfermo',\n",
       " 'cuatro',\n",
       " 'beber',\n",
       " 'volvió',\n",
       " 'siquiera',\n",
       " 'pregunto',\n",
       " 'perros',\n",
       " 'novia',\n",
       " 'hagas',\n",
       " 'falta',\n",
       " 'conoce',\n",
       " 'cara',\n",
       " 'suerte',\n",
       " 'recuerdo',\n",
       " 'quieren',\n",
       " 'entrar',\n",
       " 'culpa',\n",
       " 'banco',\n",
       " 'vuelta',\n",
       " 'rico',\n",
       " 'pagar',\n",
       " 'jamás',\n",
       " 'haré',\n",
       " 'entender',\n",
       " 'té',\n",
       " 'pensando',\n",
       " 'niña',\n",
       " 'mujeres',\n",
       " 'japonés',\n",
       " 'examen',\n",
       " 'estudiante',\n",
       " 'cerveza',\n",
       " 'árbol',\n",
       " 'van',\n",
       " 'tío',\n",
       " 'oportunidad',\n",
       " 'déjame',\n",
       " 'piano',\n",
       " 'hiciste',\n",
       " 'foto',\n",
       " 'doctor',\n",
       " 'par',\n",
       " 'nieve',\n",
       " 'médico',\n",
       " 'lengua',\n",
       " 'entiendo',\n",
       " 'unas',\n",
       " 'llama',\n",
       " 'estudiando',\n",
       " 'esperaba',\n",
       " 'espera',\n",
       " 'decidió',\n",
       " 'cumpleaños',\n",
       " 'cena',\n",
       " 'salud',\n",
       " 'ruido',\n",
       " 'vista',\n",
       " 'quieras',\n",
       " 'necesitamos',\n",
       " 'lluvia',\n",
       " 'justo',\n",
       " 'equipo',\n",
       " 'allá',\n",
       " 'tenés',\n",
       " 'tarea',\n",
       " 'primer',\n",
       " 'llover',\n",
       " 'exactamente',\n",
       " 'eran',\n",
       " 'restaurante',\n",
       " 'perdí',\n",
       " 'estará',\n",
       " 'diccionario',\n",
       " 'compañía',\n",
       " 'primero',\n",
       " 'canadiense',\n",
       " 'acá',\n",
       " 'vestido',\n",
       " 'montón',\n",
       " 'llorar',\n",
       " 'largo',\n",
       " 'haría',\n",
       " 'estaré',\n",
       " 'di',\n",
       " 'concierto',\n",
       " 'amor',\n",
       " '¡no',\n",
       " 'guitarra',\n",
       " 'esos',\n",
       " 'blanco',\n",
       " 'asunto',\n",
       " 'éxito',\n",
       " 'muerto',\n",
       " 'mala',\n",
       " 'gatos',\n",
       " 'cantar',\n",
       " 'visitar',\n",
       " 'próximo',\n",
       " 'pie',\n",
       " 'lista',\n",
       " 'fuego',\n",
       " 'dentro',\n",
       " 'debemos',\n",
       " 'apenas',\n",
       " 'alrededor',\n",
       " 'única',\n",
       " 'sitio',\n",
       " 'puesto',\n",
       " 'decisión',\n",
       " 'cenar',\n",
       " 'almuerzo',\n",
       " 'acaso',\n",
       " 'treinta',\n",
       " 'entonces',\n",
       " 'edificio',\n",
       " 'vendrá',\n",
       " 'tenido',\n",
       " 'siete',\n",
       " 'pasando',\n",
       " 'jugando',\n",
       " 'errores',\n",
       " 'dijiste',\n",
       " 'atención',\n",
       " 'venga',\n",
       " 'sigue',\n",
       " 'loco',\n",
       " 'jardín',\n",
       " 'encontró',\n",
       " 'dormido',\n",
       " 'cayó',\n",
       " 'carne',\n",
       " 'caminar',\n",
       " 'abogado',\n",
       " 'taza',\n",
       " 'poder',\n",
       " 'pequeña',\n",
       " 'nuestros',\n",
       " 'leyendo',\n",
       " 'extranjero',\n",
       " 'come',\n",
       " 'bus',\n",
       " 'baño',\n",
       " 'barco',\n",
       " 'aire',\n",
       " 'acabo',\n",
       " 'viven',\n",
       " 'vale',\n",
       " 'trabajando',\n",
       " 'rato',\n",
       " 'peor',\n",
       " 'paraguas',\n",
       " 'oí',\n",
       " 'llamó',\n",
       " 'fútbol',\n",
       " 'frente',\n",
       " 'escuchar',\n",
       " 'equivocado',\n",
       " 'comió',\n",
       " 'biblioteca',\n",
       " 'aquel',\n",
       " 'aconsejó',\n",
       " 'siendo',\n",
       " 'pensaba',\n",
       " 'lago',\n",
       " 'grandes',\n",
       " 'favorito',\n",
       " 'estuve',\n",
       " 'esas',\n",
       " 'dirección',\n",
       " 'dame',\n",
       " 'camisa',\n",
       " 'universidad',\n",
       " 'entró',\n",
       " 'caso',\n",
       " 'béisbol',\n",
       " 'animales',\n",
       " 'abrió',\n",
       " 'resolver',\n",
       " 'deberes',\n",
       " 'cuántas',\n",
       " 'china',\n",
       " 'ayudarte',\n",
       " 'aeropuerto',\n",
       " 'ves',\n",
       " 'sentido',\n",
       " 'playa',\n",
       " 'pareces',\n",
       " 'papá',\n",
       " 'mía',\n",
       " 'inteligente',\n",
       " 'finalmente',\n",
       " 'abuelo',\n",
       " 'sombrero',\n",
       " 'saben',\n",
       " 'radio',\n",
       " 'probablemente',\n",
       " 'minuto',\n",
       " 'final',\n",
       " 'ellas',\n",
       " 'duele',\n",
       " 'peligro',\n",
       " 'ello',\n",
       " 'digas',\n",
       " 'completamente',\n",
       " 'toma',\n",
       " 'sentó',\n",
       " 'ocurrió',\n",
       " 'miró',\n",
       " 'llamar',\n",
       " 'hermosa',\n",
       " 'decirle',\n",
       " 'cuesta',\n",
       " 'último',\n",
       " 'venido',\n",
       " 'triste',\n",
       " 'situación',\n",
       " 'sala',\n",
       " 'parís',\n",
       " 'opinión',\n",
       " 'estúpido',\n",
       " 'dime',\n",
       " 'clases',\n",
       " 'vayas',\n",
       " 'suelo',\n",
       " 'sal',\n",
       " 'respecto',\n",
       " 'prefiero',\n",
       " 'pena',\n",
       " 'peligroso',\n",
       " 'meses',\n",
       " 'mayoría',\n",
       " 'mamá',\n",
       " 'londres',\n",
       " 'hubo',\n",
       " 'haz',\n",
       " 'amo',\n",
       " 'responder',\n",
       " 'próxima',\n",
       " 'pieza',\n",
       " 'pasada',\n",
       " 'pan',\n",
       " 'noticias',\n",
       " 'medio',\n",
       " 'irme',\n",
       " 'invierno',\n",
       " 'derecho',\n",
       " 'cuchillo',\n",
       " 'conseguir',\n",
       " 'chicos',\n",
       " 'atrás',\n",
       " 'viajar',\n",
       " 'vaso',\n",
       " 'seguir',\n",
       " 'pedí',\n",
       " 'papel',\n",
       " 'mira',\n",
       " 'lleno',\n",
       " 'estábamos',\n",
       " 'amable',\n",
       " 'siguió',\n",
       " 'sale',\n",
       " 'partido',\n",
       " 'imposible',\n",
       " 'funciona',\n",
       " 'divertido',\n",
       " 'comiendo',\n",
       " 'acaba',\n",
       " 'viste',\n",
       " 'rompió',\n",
       " 'peso',\n",
       " 'manzana',\n",
       " 'llevar',\n",
       " 'intentó',\n",
       " 'estés',\n",
       " 'diga',\n",
       " 'clima',\n",
       " 'rojo',\n",
       " 'pescado',\n",
       " 'manzanas',\n",
       " 'intentando',\n",
       " 'hiciera',\n",
       " 'felices',\n",
       " 'casado',\n",
       " 'carro',\n",
       " 'caro',\n",
       " 'ambos',\n",
       " 'suficientemente',\n",
       " 'morir',\n",
       " 'llevó',\n",
       " 'incluso',\n",
       " 'iglesia',\n",
       " 'huevos',\n",
       " 'habría',\n",
       " 'dicen',\n",
       " 'dejes',\n",
       " 'corriendo',\n",
       " 'afuera',\n",
       " 'viendo',\n",
       " 'traje',\n",
       " 'terminó',\n",
       " 'pudiera',\n",
       " 'necesario',\n",
       " 'iré',\n",
       " 'irse',\n",
       " 'hermanos',\n",
       " 'haces',\n",
       " 'domingos',\n",
       " 'diciendo',\n",
       " 'desayuno',\n",
       " 'corazón',\n",
       " 'boca',\n",
       " 'aunque',\n",
       " 'abrigo',\n",
       " 'tierra',\n",
       " 'montaña',\n",
       " 'mirando',\n",
       " 'llegué',\n",
       " 'leído',\n",
       " 'ganar',\n",
       " 'francia',\n",
       " 'dile',\n",
       " 'contento',\n",
       " 'cielo',\n",
       " 'alta',\n",
       " 'trató',\n",
       " 'trata',\n",
       " 'taxi',\n",
       " 'podés',\n",
       " 'pesar',\n",
       " 'pensó',\n",
       " 'ocupada',\n",
       " 'mil',\n",
       " 'mensaje',\n",
       " 'llevaba',\n",
       " 'llaves',\n",
       " 'jefe',\n",
       " 'hablo',\n",
       " 'empleo',\n",
       " 'cine',\n",
       " 'cartas',\n",
       " 'anciano',\n",
       " 'vuelve',\n",
       " 'tenga',\n",
       " 'habló',\n",
       " 'estáis',\n",
       " 'empezar',\n",
       " 'demás',\n",
       " 'dejado',\n",
       " 'comenzó',\n",
       " 'cocinar',\n",
       " 'causa',\n",
       " 'cambiar',\n",
       " 'caliente',\n",
       " 'sino',\n",
       " 'significa',\n",
       " 'poner',\n",
       " 'piso',\n",
       " 'pase',\n",
       " 'lamento',\n",
       " 'hacía',\n",
       " 'fotos',\n",
       " 'confiar',\n",
       " 'comprado',\n",
       " 'borracho',\n",
       " 'bailar',\n",
       " 'abierta',\n",
       " 'siguiente',\n",
       " 'serio',\n",
       " 'sacó',\n",
       " 'parar',\n",
       " 'ocho',\n",
       " 'correr',\n",
       " 'casó',\n",
       " 'voz',\n",
       " 'silla',\n",
       " 'semanas',\n",
       " 'quedarse',\n",
       " 'quedarme',\n",
       " 'prueba',\n",
       " 'olvidó',\n",
       " 'novio',\n",
       " 'noticia',\n",
       " 'futuro',\n",
       " 'favorita',\n",
       " 'escritorio',\n",
       " 'escribió',\n",
       " 'correo',\n",
       " 'correcto',\n",
       " 'conocí',\n",
       " 'caballo',\n",
       " 'brazo',\n",
       " 'vosotros',\n",
       " 'verdaderamente',\n",
       " 'toca',\n",
       " 'seas',\n",
       " 'puedas',\n",
       " 'pueblo',\n",
       " 'periódico',\n",
       " 'paciente',\n",
       " 'negro',\n",
       " 'necesitaba',\n",
       " 'mar',\n",
       " 'hicieron',\n",
       " 'durmiendo',\n",
       " 'buenas',\n",
       " 'verme',\n",
       " 'simplemente',\n",
       " 'quiera',\n",
       " 'propio',\n",
       " 'novela',\n",
       " 'ninguno',\n",
       " 'matar',\n",
       " 'detrás',\n",
       " 'desearía',\n",
       " 'conoces',\n",
       " 'azul',\n",
       " 'asiento',\n",
       " 'aquella',\n",
       " 'amiga',\n",
       " 'york',\n",
       " 'tuvimos',\n",
       " 'supongo',\n",
       " 'recordar',\n",
       " 'propia',\n",
       " 'normalmente',\n",
       " 'entiende',\n",
       " 'dientes',\n",
       " 'sintió',\n",
       " 'reglas',\n",
       " 'recuerda',\n",
       " 'quedé',\n",
       " 'quedar',\n",
       " 'orgulloso',\n",
       " 'idioma',\n",
       " 'helado',\n",
       " 'habrá',\n",
       " 'gana',\n",
       " 'empieza',\n",
       " ...]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_vocab = target_vectorizer.get_vocabulary()\n",
    "target_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9dd88f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '',\n",
       " 1: '[UNK]',\n",
       " 2: '[start]',\n",
       " 3: '[end]',\n",
       " 4: 'de',\n",
       " 5: 'que',\n",
       " 6: 'a',\n",
       " 7: 'no',\n",
       " 8: 'tom',\n",
       " 9: 'la',\n",
       " 10: 'el',\n",
       " 11: 'en',\n",
       " 12: 'es',\n",
       " 13: 'un',\n",
       " 14: 'me',\n",
       " 15: 'se',\n",
       " 16: 'por',\n",
       " 17: 'lo',\n",
       " 18: 'una',\n",
       " 19: 'los',\n",
       " 20: 'su',\n",
       " 21: 'Él',\n",
       " 22: 'está',\n",
       " 23: 'con',\n",
       " 24: 'mi',\n",
       " 25: 'le',\n",
       " 26: 'ella',\n",
       " 27: 'qué',\n",
       " 28: 'te',\n",
       " 29: 'para',\n",
       " 30: 'mary',\n",
       " 31: 'y',\n",
       " 32: 'las',\n",
       " 33: 'más',\n",
       " 34: 'al',\n",
       " 35: 'yo',\n",
       " 36: 'tu',\n",
       " 37: 'estoy',\n",
       " 38: 'muy',\n",
       " 39: 'eso',\n",
       " 40: 'tiene',\n",
       " 41: 'este',\n",
       " 42: 'esta',\n",
       " 43: 'del',\n",
       " 44: 'él',\n",
       " 45: 'estaba',\n",
       " 46: 'tengo',\n",
       " 47: 'quiero',\n",
       " 48: 'fue',\n",
       " 49: 'si',\n",
       " 50: 'aquí',\n",
       " 51: 'casa',\n",
       " 52: 'como',\n",
       " 53: 'hacer',\n",
       " 54: 'puedo',\n",
       " 55: 'algo',\n",
       " 56: 'todo',\n",
       " 57: 'esto',\n",
       " 58: 'hay',\n",
       " 59: 'tiempo',\n",
       " 60: 'ha',\n",
       " 61: 'gusta',\n",
       " 62: 'todos',\n",
       " 63: 'tan',\n",
       " 64: 'son',\n",
       " 65: 'nada',\n",
       " 66: 'cuando',\n",
       " 67: 'favor',\n",
       " 68: 'ir',\n",
       " 69: 'vez',\n",
       " 70: 'era',\n",
       " 71: 'puede',\n",
       " 72: 'bien',\n",
       " 73: 'mucho',\n",
       " 74: 'he',\n",
       " 75: 'ellos',\n",
       " 76: 'nos',\n",
       " 77: 'sé',\n",
       " 78: 'solo',\n",
       " 79: 'mañana',\n",
       " 80: 'nunca',\n",
       " 81: 'ser',\n",
       " 82: 'dos',\n",
       " 83: 'ya',\n",
       " 84: 'creo',\n",
       " 85: 'sus',\n",
       " 86: 'trabajo',\n",
       " 87: 'estás',\n",
       " 88: 'tienes',\n",
       " 89: 'dónde',\n",
       " 90: 'cómo',\n",
       " 91: 'dinero',\n",
       " 92: 'dijo',\n",
       " 93: 'ahora',\n",
       " 94: 'hablar',\n",
       " 95: 'tomás',\n",
       " 96: 'quién',\n",
       " 97: 'están',\n",
       " 98: 'pero',\n",
       " 99: 'soy',\n",
       " 100: 'día',\n",
       " 101: 'había',\n",
       " 102: 'hace',\n",
       " 103: 'ese',\n",
       " 104: 'siempre',\n",
       " 105: 'tú',\n",
       " 106: 'libro',\n",
       " 107: 'puedes',\n",
       " 108: 'poco',\n",
       " 109: 'hoy',\n",
       " 110: 'quiere',\n",
       " 111: 'esa',\n",
       " 112: 'verdad',\n",
       " 113: 'nadie',\n",
       " 114: 'años',\n",
       " 115: 'voy',\n",
       " 116: 'estar',\n",
       " 117: 'ver',\n",
       " 118: 'sabe',\n",
       " 119: 'quería',\n",
       " 120: 'francés',\n",
       " 121: 'va',\n",
       " 122: 'mejor',\n",
       " 123: 'has',\n",
       " 124: 'maría',\n",
       " 125: 'quieres',\n",
       " 126: 'demasiado',\n",
       " 127: 'eres',\n",
       " 128: 'tarde',\n",
       " 129: 'hizo',\n",
       " 130: 'padre',\n",
       " 131: 'antes',\n",
       " 132: 'parece',\n",
       " 133: 'mis',\n",
       " 134: 'o',\n",
       " 135: 'tres',\n",
       " 136: 'usted',\n",
       " 137: 'tenía',\n",
       " 138: 'noche',\n",
       " 139: 'boston',\n",
       " 140: 'sobre',\n",
       " 141: 'hora',\n",
       " 142: 'sin',\n",
       " 143: 'hasta',\n",
       " 144: 'todavía',\n",
       " 145: 'mí',\n",
       " 146: 'ayer',\n",
       " 147: 'perro',\n",
       " 148: 'estado',\n",
       " 149: 'inglés',\n",
       " 150: 'haber',\n",
       " 151: 'hombre',\n",
       " 152: 'comer',\n",
       " 153: 'vida',\n",
       " 154: 'nuevo',\n",
       " 155: 'gente',\n",
       " 156: 'alguien',\n",
       " 157: 'cuándo',\n",
       " 158: 'necesito',\n",
       " 159: 'tenemos',\n",
       " 160: 'días',\n",
       " 161: 'mismo',\n",
       " 162: 'cuál',\n",
       " 163: 'coche',\n",
       " 164: 'problema',\n",
       " 165: 'nosotros',\n",
       " 166: 'madre',\n",
       " 167: 'decir',\n",
       " 168: 'así',\n",
       " 169: 'hecho',\n",
       " 170: 'gustaría',\n",
       " 171: 'otra',\n",
       " 172: 'pensé',\n",
       " 173: 'semana',\n",
       " 174: 'sabía',\n",
       " 175: 'estamos',\n",
       " 176: 'buen',\n",
       " 177: 'pasado',\n",
       " 178: 'después',\n",
       " 179: 'uno',\n",
       " 180: 'habitación',\n",
       " 181: 'cosas',\n",
       " 182: 'ti',\n",
       " 183: 'debería',\n",
       " 184: 'tus',\n",
       " 185: 'puerta',\n",
       " 186: 'deberías',\n",
       " 187: 'casi',\n",
       " 188: 'vas',\n",
       " 189: 'alguna',\n",
       " 190: 'niños',\n",
       " 191: 'mundo',\n",
       " 192: 'hacerlo',\n",
       " 193: 'amigos',\n",
       " 194: 'escuela',\n",
       " 195: 'realmente',\n",
       " 196: 'feliz',\n",
       " 197: 'difícil',\n",
       " 198: 'fuera',\n",
       " 199: 'allí',\n",
       " 200: 'otro',\n",
       " 201: 'saber',\n",
       " 202: 'niño',\n",
       " 203: 'les',\n",
       " 204: 'cuánto',\n",
       " 205: 'podría',\n",
       " 206: 'pronto',\n",
       " 207: 'veces',\n",
       " 208: 'menos',\n",
       " 209: 'ni',\n",
       " 210: 'idea',\n",
       " 211: 'dio',\n",
       " 212: 'año',\n",
       " 213: 'buena',\n",
       " 214: 'ahí',\n",
       " 215: 'seguro',\n",
       " 216: 'habla',\n",
       " 217: 'ayuda',\n",
       " 218: 'visto',\n",
       " 219: 'ciudad',\n",
       " 220: 'desde',\n",
       " 221: 'muchos',\n",
       " 222: 'agua',\n",
       " 223: 'auto',\n",
       " 224: 'sólo',\n",
       " 225: 'amigo',\n",
       " 226: 'sabes',\n",
       " 227: 'nuestro',\n",
       " 228: 'todas',\n",
       " 229: 'mal',\n",
       " 230: 'persona',\n",
       " 231: 'comida',\n",
       " 232: 'nombre',\n",
       " 233: 'john',\n",
       " 234: 'vino',\n",
       " 235: 'durante',\n",
       " 236: 'conmigo',\n",
       " 237: 'comprar',\n",
       " 238: 'tren',\n",
       " 239: 'cada',\n",
       " 240: 'personas',\n",
       " 241: 'café',\n",
       " 242: 'vi',\n",
       " 243: 'tener',\n",
       " 244: 'cama',\n",
       " 245: 'dice',\n",
       " 246: 'hermano',\n",
       " 247: 'gran',\n",
       " 248: 'haciendo',\n",
       " 249: 'tanto',\n",
       " 250: 'japón',\n",
       " 251: 'contigo',\n",
       " 252: 'toda',\n",
       " 253: 'salir',\n",
       " 254: 'profesor',\n",
       " 255: 'clase',\n",
       " 256: 'fiesta',\n",
       " 257: 'grande',\n",
       " 258: 'muchas',\n",
       " 259: 'diez',\n",
       " 260: 'bueno',\n",
       " 261: 'cuenta',\n",
       " 262: 'carta',\n",
       " 263: 'sea',\n",
       " 264: 'acerca',\n",
       " 265: 'aún',\n",
       " 266: 'policía',\n",
       " 267: 'libros',\n",
       " 268: 'hermana',\n",
       " 269: 'podía',\n",
       " 270: 'nuestra',\n",
       " 271: 'momento',\n",
       " 272: 'rápido',\n",
       " 273: 'lugar',\n",
       " 274: 'tienen',\n",
       " 275: 'porque',\n",
       " 276: 'pasó',\n",
       " 277: 'pudo',\n",
       " 278: 'padres',\n",
       " 279: 'joven',\n",
       " 280: 'dejó',\n",
       " 281: 'leer',\n",
       " 282: 'siento',\n",
       " 283: 'gustan',\n",
       " 284: 'fui',\n",
       " 285: 'espero',\n",
       " 286: 'vive',\n",
       " 287: 'tuvo',\n",
       " 288: 'problemas',\n",
       " 289: 'cerca',\n",
       " 290: 'vamos',\n",
       " 291: 'trabajar',\n",
       " 292: 'mujer',\n",
       " 293: 'horas',\n",
       " 294: 'han',\n",
       " 295: 'lunes',\n",
       " 296: 'puso',\n",
       " 297: 'tomar',\n",
       " 298: 'teléfono',\n",
       " 299: 'sido',\n",
       " 300: 'podemos',\n",
       " 301: 'entre',\n",
       " 302: 'historia',\n",
       " 303: 'acuerdo',\n",
       " 304: 'cosa',\n",
       " 305: 'accidente',\n",
       " 306: 'donde',\n",
       " 307: 'también',\n",
       " 308: 'parte',\n",
       " 309: 'hijo',\n",
       " 310: 'debe',\n",
       " 311: 'anoche',\n",
       " 312: 'murió',\n",
       " 313: 'cinco',\n",
       " 314: 'estaban',\n",
       " 315: 'sí',\n",
       " 316: 'nueva',\n",
       " 317: 'debes',\n",
       " 318: 'venir',\n",
       " 319: 'gato',\n",
       " 320: 'nadar',\n",
       " 321: 'familia',\n",
       " 322: 'fácil',\n",
       " 323: 'mucha',\n",
       " 324: 'minutos',\n",
       " 325: 'algunos',\n",
       " 326: 'mayor',\n",
       " 327: 'ayudar',\n",
       " 328: 'última',\n",
       " 329: 'jugar',\n",
       " 330: 'ocupado',\n",
       " 331: 'mesa',\n",
       " 332: 'temprano',\n",
       " 333: 'televisión',\n",
       " 334: 'pregunta',\n",
       " 335: 'menudo',\n",
       " 336: 'estos',\n",
       " 337: 'estación',\n",
       " 338: 'vivir',\n",
       " 339: 'hice',\n",
       " 340: 'gracias',\n",
       " 341: 'camino',\n",
       " 342: 'viene',\n",
       " 343: 'aprender',\n",
       " 344: 'mano',\n",
       " 345: 'razón',\n",
       " 346: 'llegar',\n",
       " 347: 'crees',\n",
       " 348: 'país',\n",
       " 349: 'dije',\n",
       " 350: 'pidió',\n",
       " 351: 'mientras',\n",
       " 352: 'encontrar',\n",
       " 353: 'dormir',\n",
       " 354: 'volver',\n",
       " 355: 'bastante',\n",
       " 356: 'suficiente',\n",
       " 357: 'dicho',\n",
       " 358: 'pasar',\n",
       " 359: 'estudiar',\n",
       " 360: 'algunas',\n",
       " 361: 'miedo',\n",
       " 362: 'preguntó',\n",
       " 363: 'alto',\n",
       " 364: 'algún',\n",
       " 365: 'dejar',\n",
       " 366: 'zapatos',\n",
       " 367: 'será',\n",
       " 368: 'lado',\n",
       " 369: 'mes',\n",
       " 370: 'compró',\n",
       " 371: 'ustedes',\n",
       " 372: 'estuvo',\n",
       " 373: 'conozco',\n",
       " 374: 'quisiera',\n",
       " 375: 'hablando',\n",
       " 376: 'tipo',\n",
       " 377: 'fuerte',\n",
       " 378: 've',\n",
       " 379: 'unos',\n",
       " 380: 'tuve',\n",
       " 381: 'colegio',\n",
       " 382: 'ventana',\n",
       " 383: 'seis',\n",
       " 384: 'cansado',\n",
       " 385: 'podrías',\n",
       " 386: 'hijos',\n",
       " 387: 'vio',\n",
       " 388: 'pienso',\n",
       " 389: 'música',\n",
       " 390: 'frío',\n",
       " 391: 'ningún',\n",
       " 392: 'llegó',\n",
       " 393: 'juntos',\n",
       " 394: 'importante',\n",
       " 395: 'error',\n",
       " 396: 'cabeza',\n",
       " 397: 'necesita',\n",
       " 398: 'punto',\n",
       " 399: 'oficina',\n",
       " 400: 'importa',\n",
       " 401: 'fueron',\n",
       " 402: 'canción',\n",
       " 403: 'ojos',\n",
       " 404: 'esperando',\n",
       " 405: 'hemos',\n",
       " 406: 'hacia',\n",
       " 407: 'creer',\n",
       " 408: 'somos',\n",
       " 409: 'reloj',\n",
       " 410: 'os',\n",
       " 411: 'estados',\n",
       " 412: 'esperar',\n",
       " 413: 'unidos',\n",
       " 414: 'reunión',\n",
       " 415: 'lleva',\n",
       " 416: 'estudiantes',\n",
       " 417: 'viejo',\n",
       " 418: 'solía',\n",
       " 419: 'película',\n",
       " 420: 'ido',\n",
       " 421: 'cualquier',\n",
       " 422: 'sola',\n",
       " 423: 'salió',\n",
       " 424: 'primera',\n",
       " 425: 'debo',\n",
       " 426: 'verte',\n",
       " 427: 'misma',\n",
       " 428: 'fumar',\n",
       " 429: '¡qué',\n",
       " 430: 'vos',\n",
       " 431: 'queda',\n",
       " 432: 'pueden',\n",
       " 433: 'plan',\n",
       " 434: 'ganas',\n",
       " 435: 'único',\n",
       " 436: 'sueño',\n",
       " 437: 'hotel',\n",
       " 438: 'fin',\n",
       " 439: 'edad',\n",
       " 440: 'buscando',\n",
       " 441: 'bicicleta',\n",
       " 442: 'estas',\n",
       " 443: 'avión',\n",
       " 444: 'palabra',\n",
       " 445: 'dar',\n",
       " 446: 'llave',\n",
       " 447: 'lejos',\n",
       " 448: 'leche',\n",
       " 449: 'cuarto',\n",
       " 450: 'tenis',\n",
       " 451: 'oído',\n",
       " 452: 'media',\n",
       " 453: 'esposa',\n",
       " 454: 'tal',\n",
       " 455: 'querés',\n",
       " 456: 'tienda',\n",
       " 457: 'respuesta',\n",
       " 458: 'haga',\n",
       " 459: 'número',\n",
       " 460: 'deja',\n",
       " 461: 'pasa',\n",
       " 462: 'interesante',\n",
       " 463: 'conducir',\n",
       " 464: 'caja',\n",
       " 465: 'bajo',\n",
       " 466: 'verano',\n",
       " 467: 'pelo',\n",
       " 468: 'da',\n",
       " 469: 'necesitas',\n",
       " 470: 'juego',\n",
       " 471: 'contra',\n",
       " 472: 'chico',\n",
       " 473: 'quien',\n",
       " 474: 'iba',\n",
       " 475: 'compré',\n",
       " 476: 'sos',\n",
       " 477: 'manera',\n",
       " 478: 'guerra',\n",
       " 479: 'chica',\n",
       " 480: 'veo',\n",
       " 481: 'parecía',\n",
       " 482: 'cree',\n",
       " 483: 'bebé',\n",
       " 484: 'autobús',\n",
       " 485: 'sería',\n",
       " 486: 'hubiera',\n",
       " 487: 'dólares',\n",
       " 488: 'dolor',\n",
       " 489: 'preguntas',\n",
       " 490: 'mío',\n",
       " 491: 'hombres',\n",
       " 492: 'estabas',\n",
       " 493: 'australia',\n",
       " 494: 'río',\n",
       " 495: 'pueda',\n",
       " 496: 'navidad',\n",
       " 497: 'hospital',\n",
       " 498: 'cámara',\n",
       " 499: 'capaz',\n",
       " 500: 'ropa',\n",
       " 501: 'escribir',\n",
       " 502: 'deberíamos',\n",
       " 503: 'vaya',\n",
       " 504: 'listo',\n",
       " 505: 'encanta',\n",
       " 506: 'usar',\n",
       " 507: 'terminar',\n",
       " 508: 'realidad',\n",
       " 509: 'pude',\n",
       " 510: 'pensar',\n",
       " 511: 'ninguna',\n",
       " 512: 'esté',\n",
       " 513: 'calle',\n",
       " 514: 'tokio',\n",
       " 515: 'quedó',\n",
       " 516: 'tomó',\n",
       " 517: 'perdió',\n",
       " 518: 'perdido',\n",
       " 519: 'cierto',\n",
       " 520: 'calor',\n",
       " 521: 'vivo',\n",
       " 522: 'secreto',\n",
       " 523: 'perder',\n",
       " 524: 'ojalá',\n",
       " 525: 'cuántos',\n",
       " 526: 'posible',\n",
       " 527: 'hija',\n",
       " 528: 'hambre',\n",
       " 529: 'cuanto',\n",
       " 530: 'ven',\n",
       " 531: 'parque',\n",
       " 532: 'encontré',\n",
       " 533: 'trabaja',\n",
       " 534: 'palabras',\n",
       " 535: 'odio',\n",
       " 536: 'haya',\n",
       " 537: 'duro',\n",
       " 538: 'vacaciones',\n",
       " 539: 'queremos',\n",
       " 540: 'oír',\n",
       " 541: 'libre',\n",
       " 542: 'flores',\n",
       " 543: 'extraño',\n",
       " 544: 'cuidado',\n",
       " 545: 'empezó',\n",
       " 546: 'tocar',\n",
       " 547: 'manos',\n",
       " 548: 'sol',\n",
       " 549: 'muerte',\n",
       " 550: 'consejo',\n",
       " 551: 'cocina',\n",
       " 552: 'piensas',\n",
       " 553: 'pequeño',\n",
       " 554: 'forma',\n",
       " 555: 'viaje',\n",
       " 556: 'terminado',\n",
       " 557: 'regalo',\n",
       " 558: 'piensa',\n",
       " 559: 'otros',\n",
       " 560: 'malo',\n",
       " 561: 'luz',\n",
       " 562: 'enfermo',\n",
       " 563: 'cuatro',\n",
       " 564: 'beber',\n",
       " 565: 'volvió',\n",
       " 566: 'siquiera',\n",
       " 567: 'pregunto',\n",
       " 568: 'perros',\n",
       " 569: 'novia',\n",
       " 570: 'hagas',\n",
       " 571: 'falta',\n",
       " 572: 'conoce',\n",
       " 573: 'cara',\n",
       " 574: 'suerte',\n",
       " 575: 'recuerdo',\n",
       " 576: 'quieren',\n",
       " 577: 'entrar',\n",
       " 578: 'culpa',\n",
       " 579: 'banco',\n",
       " 580: 'vuelta',\n",
       " 581: 'rico',\n",
       " 582: 'pagar',\n",
       " 583: 'jamás',\n",
       " 584: 'haré',\n",
       " 585: 'entender',\n",
       " 586: 'té',\n",
       " 587: 'pensando',\n",
       " 588: 'niña',\n",
       " 589: 'mujeres',\n",
       " 590: 'japonés',\n",
       " 591: 'examen',\n",
       " 592: 'estudiante',\n",
       " 593: 'cerveza',\n",
       " 594: 'árbol',\n",
       " 595: 'van',\n",
       " 596: 'tío',\n",
       " 597: 'oportunidad',\n",
       " 598: 'déjame',\n",
       " 599: 'piano',\n",
       " 600: 'hiciste',\n",
       " 601: 'foto',\n",
       " 602: 'doctor',\n",
       " 603: 'par',\n",
       " 604: 'nieve',\n",
       " 605: 'médico',\n",
       " 606: 'lengua',\n",
       " 607: 'entiendo',\n",
       " 608: 'unas',\n",
       " 609: 'llama',\n",
       " 610: 'estudiando',\n",
       " 611: 'esperaba',\n",
       " 612: 'espera',\n",
       " 613: 'decidió',\n",
       " 614: 'cumpleaños',\n",
       " 615: 'cena',\n",
       " 616: 'salud',\n",
       " 617: 'ruido',\n",
       " 618: 'vista',\n",
       " 619: 'quieras',\n",
       " 620: 'necesitamos',\n",
       " 621: 'lluvia',\n",
       " 622: 'justo',\n",
       " 623: 'equipo',\n",
       " 624: 'allá',\n",
       " 625: 'tenés',\n",
       " 626: 'tarea',\n",
       " 627: 'primer',\n",
       " 628: 'llover',\n",
       " 629: 'exactamente',\n",
       " 630: 'eran',\n",
       " 631: 'restaurante',\n",
       " 632: 'perdí',\n",
       " 633: 'estará',\n",
       " 634: 'diccionario',\n",
       " 635: 'compañía',\n",
       " 636: 'primero',\n",
       " 637: 'canadiense',\n",
       " 638: 'acá',\n",
       " 639: 'vestido',\n",
       " 640: 'montón',\n",
       " 641: 'llorar',\n",
       " 642: 'largo',\n",
       " 643: 'haría',\n",
       " 644: 'estaré',\n",
       " 645: 'di',\n",
       " 646: 'concierto',\n",
       " 647: 'amor',\n",
       " 648: '¡no',\n",
       " 649: 'guitarra',\n",
       " 650: 'esos',\n",
       " 651: 'blanco',\n",
       " 652: 'asunto',\n",
       " 653: 'éxito',\n",
       " 654: 'muerto',\n",
       " 655: 'mala',\n",
       " 656: 'gatos',\n",
       " 657: 'cantar',\n",
       " 658: 'visitar',\n",
       " 659: 'próximo',\n",
       " 660: 'pie',\n",
       " 661: 'lista',\n",
       " 662: 'fuego',\n",
       " 663: 'dentro',\n",
       " 664: 'debemos',\n",
       " 665: 'apenas',\n",
       " 666: 'alrededor',\n",
       " 667: 'única',\n",
       " 668: 'sitio',\n",
       " 669: 'puesto',\n",
       " 670: 'decisión',\n",
       " 671: 'cenar',\n",
       " 672: 'almuerzo',\n",
       " 673: 'acaso',\n",
       " 674: 'treinta',\n",
       " 675: 'entonces',\n",
       " 676: 'edificio',\n",
       " 677: 'vendrá',\n",
       " 678: 'tenido',\n",
       " 679: 'siete',\n",
       " 680: 'pasando',\n",
       " 681: 'jugando',\n",
       " 682: 'errores',\n",
       " 683: 'dijiste',\n",
       " 684: 'atención',\n",
       " 685: 'venga',\n",
       " 686: 'sigue',\n",
       " 687: 'loco',\n",
       " 688: 'jardín',\n",
       " 689: 'encontró',\n",
       " 690: 'dormido',\n",
       " 691: 'cayó',\n",
       " 692: 'carne',\n",
       " 693: 'caminar',\n",
       " 694: 'abogado',\n",
       " 695: 'taza',\n",
       " 696: 'poder',\n",
       " 697: 'pequeña',\n",
       " 698: 'nuestros',\n",
       " 699: 'leyendo',\n",
       " 700: 'extranjero',\n",
       " 701: 'come',\n",
       " 702: 'bus',\n",
       " 703: 'baño',\n",
       " 704: 'barco',\n",
       " 705: 'aire',\n",
       " 706: 'acabo',\n",
       " 707: 'viven',\n",
       " 708: 'vale',\n",
       " 709: 'trabajando',\n",
       " 710: 'rato',\n",
       " 711: 'peor',\n",
       " 712: 'paraguas',\n",
       " 713: 'oí',\n",
       " 714: 'llamó',\n",
       " 715: 'fútbol',\n",
       " 716: 'frente',\n",
       " 717: 'escuchar',\n",
       " 718: 'equivocado',\n",
       " 719: 'comió',\n",
       " 720: 'biblioteca',\n",
       " 721: 'aquel',\n",
       " 722: 'aconsejó',\n",
       " 723: 'siendo',\n",
       " 724: 'pensaba',\n",
       " 725: 'lago',\n",
       " 726: 'grandes',\n",
       " 727: 'favorito',\n",
       " 728: 'estuve',\n",
       " 729: 'esas',\n",
       " 730: 'dirección',\n",
       " 731: 'dame',\n",
       " 732: 'camisa',\n",
       " 733: 'universidad',\n",
       " 734: 'entró',\n",
       " 735: 'caso',\n",
       " 736: 'béisbol',\n",
       " 737: 'animales',\n",
       " 738: 'abrió',\n",
       " 739: 'resolver',\n",
       " 740: 'deberes',\n",
       " 741: 'cuántas',\n",
       " 742: 'china',\n",
       " 743: 'ayudarte',\n",
       " 744: 'aeropuerto',\n",
       " 745: 'ves',\n",
       " 746: 'sentido',\n",
       " 747: 'playa',\n",
       " 748: 'pareces',\n",
       " 749: 'papá',\n",
       " 750: 'mía',\n",
       " 751: 'inteligente',\n",
       " 752: 'finalmente',\n",
       " 753: 'abuelo',\n",
       " 754: 'sombrero',\n",
       " 755: 'saben',\n",
       " 756: 'radio',\n",
       " 757: 'probablemente',\n",
       " 758: 'minuto',\n",
       " 759: 'final',\n",
       " 760: 'ellas',\n",
       " 761: 'duele',\n",
       " 762: 'peligro',\n",
       " 763: 'ello',\n",
       " 764: 'digas',\n",
       " 765: 'completamente',\n",
       " 766: 'toma',\n",
       " 767: 'sentó',\n",
       " 768: 'ocurrió',\n",
       " 769: 'miró',\n",
       " 770: 'llamar',\n",
       " 771: 'hermosa',\n",
       " 772: 'decirle',\n",
       " 773: 'cuesta',\n",
       " 774: 'último',\n",
       " 775: 'venido',\n",
       " 776: 'triste',\n",
       " 777: 'situación',\n",
       " 778: 'sala',\n",
       " 779: 'parís',\n",
       " 780: 'opinión',\n",
       " 781: 'estúpido',\n",
       " 782: 'dime',\n",
       " 783: 'clases',\n",
       " 784: 'vayas',\n",
       " 785: 'suelo',\n",
       " 786: 'sal',\n",
       " 787: 'respecto',\n",
       " 788: 'prefiero',\n",
       " 789: 'pena',\n",
       " 790: 'peligroso',\n",
       " 791: 'meses',\n",
       " 792: 'mayoría',\n",
       " 793: 'mamá',\n",
       " 794: 'londres',\n",
       " 795: 'hubo',\n",
       " 796: 'haz',\n",
       " 797: 'amo',\n",
       " 798: 'responder',\n",
       " 799: 'próxima',\n",
       " 800: 'pieza',\n",
       " 801: 'pasada',\n",
       " 802: 'pan',\n",
       " 803: 'noticias',\n",
       " 804: 'medio',\n",
       " 805: 'irme',\n",
       " 806: 'invierno',\n",
       " 807: 'derecho',\n",
       " 808: 'cuchillo',\n",
       " 809: 'conseguir',\n",
       " 810: 'chicos',\n",
       " 811: 'atrás',\n",
       " 812: 'viajar',\n",
       " 813: 'vaso',\n",
       " 814: 'seguir',\n",
       " 815: 'pedí',\n",
       " 816: 'papel',\n",
       " 817: 'mira',\n",
       " 818: 'lleno',\n",
       " 819: 'estábamos',\n",
       " 820: 'amable',\n",
       " 821: 'siguió',\n",
       " 822: 'sale',\n",
       " 823: 'partido',\n",
       " 824: 'imposible',\n",
       " 825: 'funciona',\n",
       " 826: 'divertido',\n",
       " 827: 'comiendo',\n",
       " 828: 'acaba',\n",
       " 829: 'viste',\n",
       " 830: 'rompió',\n",
       " 831: 'peso',\n",
       " 832: 'manzana',\n",
       " 833: 'llevar',\n",
       " 834: 'intentó',\n",
       " 835: 'estés',\n",
       " 836: 'diga',\n",
       " 837: 'clima',\n",
       " 838: 'rojo',\n",
       " 839: 'pescado',\n",
       " 840: 'manzanas',\n",
       " 841: 'intentando',\n",
       " 842: 'hiciera',\n",
       " 843: 'felices',\n",
       " 844: 'casado',\n",
       " 845: 'carro',\n",
       " 846: 'caro',\n",
       " 847: 'ambos',\n",
       " 848: 'suficientemente',\n",
       " 849: 'morir',\n",
       " 850: 'llevó',\n",
       " 851: 'incluso',\n",
       " 852: 'iglesia',\n",
       " 853: 'huevos',\n",
       " 854: 'habría',\n",
       " 855: 'dicen',\n",
       " 856: 'dejes',\n",
       " 857: 'corriendo',\n",
       " 858: 'afuera',\n",
       " 859: 'viendo',\n",
       " 860: 'traje',\n",
       " 861: 'terminó',\n",
       " 862: 'pudiera',\n",
       " 863: 'necesario',\n",
       " 864: 'iré',\n",
       " 865: 'irse',\n",
       " 866: 'hermanos',\n",
       " 867: 'haces',\n",
       " 868: 'domingos',\n",
       " 869: 'diciendo',\n",
       " 870: 'desayuno',\n",
       " 871: 'corazón',\n",
       " 872: 'boca',\n",
       " 873: 'aunque',\n",
       " 874: 'abrigo',\n",
       " 875: 'tierra',\n",
       " 876: 'montaña',\n",
       " 877: 'mirando',\n",
       " 878: 'llegué',\n",
       " 879: 'leído',\n",
       " 880: 'ganar',\n",
       " 881: 'francia',\n",
       " 882: 'dile',\n",
       " 883: 'contento',\n",
       " 884: 'cielo',\n",
       " 885: 'alta',\n",
       " 886: 'trató',\n",
       " 887: 'trata',\n",
       " 888: 'taxi',\n",
       " 889: 'podés',\n",
       " 890: 'pesar',\n",
       " 891: 'pensó',\n",
       " 892: 'ocupada',\n",
       " 893: 'mil',\n",
       " 894: 'mensaje',\n",
       " 895: 'llevaba',\n",
       " 896: 'llaves',\n",
       " 897: 'jefe',\n",
       " 898: 'hablo',\n",
       " 899: 'empleo',\n",
       " 900: 'cine',\n",
       " 901: 'cartas',\n",
       " 902: 'anciano',\n",
       " 903: 'vuelve',\n",
       " 904: 'tenga',\n",
       " 905: 'habló',\n",
       " 906: 'estáis',\n",
       " 907: 'empezar',\n",
       " 908: 'demás',\n",
       " 909: 'dejado',\n",
       " 910: 'comenzó',\n",
       " 911: 'cocinar',\n",
       " 912: 'causa',\n",
       " 913: 'cambiar',\n",
       " 914: 'caliente',\n",
       " 915: 'sino',\n",
       " 916: 'significa',\n",
       " 917: 'poner',\n",
       " 918: 'piso',\n",
       " 919: 'pase',\n",
       " 920: 'lamento',\n",
       " 921: 'hacía',\n",
       " 922: 'fotos',\n",
       " 923: 'confiar',\n",
       " 924: 'comprado',\n",
       " 925: 'borracho',\n",
       " 926: 'bailar',\n",
       " 927: 'abierta',\n",
       " 928: 'siguiente',\n",
       " 929: 'serio',\n",
       " 930: 'sacó',\n",
       " 931: 'parar',\n",
       " 932: 'ocho',\n",
       " 933: 'correr',\n",
       " 934: 'casó',\n",
       " 935: 'voz',\n",
       " 936: 'silla',\n",
       " 937: 'semanas',\n",
       " 938: 'quedarse',\n",
       " 939: 'quedarme',\n",
       " 940: 'prueba',\n",
       " 941: 'olvidó',\n",
       " 942: 'novio',\n",
       " 943: 'noticia',\n",
       " 944: 'futuro',\n",
       " 945: 'favorita',\n",
       " 946: 'escritorio',\n",
       " 947: 'escribió',\n",
       " 948: 'correo',\n",
       " 949: 'correcto',\n",
       " 950: 'conocí',\n",
       " 951: 'caballo',\n",
       " 952: 'brazo',\n",
       " 953: 'vosotros',\n",
       " 954: 'verdaderamente',\n",
       " 955: 'toca',\n",
       " 956: 'seas',\n",
       " 957: 'puedas',\n",
       " 958: 'pueblo',\n",
       " 959: 'periódico',\n",
       " 960: 'paciente',\n",
       " 961: 'negro',\n",
       " 962: 'necesitaba',\n",
       " 963: 'mar',\n",
       " 964: 'hicieron',\n",
       " 965: 'durmiendo',\n",
       " 966: 'buenas',\n",
       " 967: 'verme',\n",
       " 968: 'simplemente',\n",
       " 969: 'quiera',\n",
       " 970: 'propio',\n",
       " 971: 'novela',\n",
       " 972: 'ninguno',\n",
       " 973: 'matar',\n",
       " 974: 'detrás',\n",
       " 975: 'desearía',\n",
       " 976: 'conoces',\n",
       " 977: 'azul',\n",
       " 978: 'asiento',\n",
       " 979: 'aquella',\n",
       " 980: 'amiga',\n",
       " 981: 'york',\n",
       " 982: 'tuvimos',\n",
       " 983: 'supongo',\n",
       " 984: 'recordar',\n",
       " 985: 'propia',\n",
       " 986: 'normalmente',\n",
       " 987: 'entiende',\n",
       " 988: 'dientes',\n",
       " 989: 'sintió',\n",
       " 990: 'reglas',\n",
       " 991: 'recuerda',\n",
       " 992: 'quedé',\n",
       " 993: 'quedar',\n",
       " 994: 'orgulloso',\n",
       " 995: 'idioma',\n",
       " 996: 'helado',\n",
       " 997: 'habrá',\n",
       " 998: 'gana',\n",
       " 999: 'empieza',\n",
       " ...}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_index = {i:word for i,word in enumerate(target_vocab)}\n",
    "target_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52ca584d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to vectorize the source and target sentences and to prepare the final dataset\n",
    "def vectorize_dataset(source, target):\n",
    "    source = source_vectorizer(source)\n",
    "    target = target_vectorizer(target)\n",
    "    return ({\n",
    "        \"encoder_inputs\": source,\n",
    "        \"decoder_inputs\": target[:, :-1],  # Exclude the last token for decoder input\n",
    "    }, target[:, 1:])  # Shift by one for the target output\n",
    "\n",
    "def make_dataset(data):\n",
    "    dataset = data.batch(BATCH_SIZE) # batches the data\n",
    "    dataset = dataset.map(vectorize_dataset, num_parallel_calls=4)\n",
    "    \n",
    "    return dataset.shuffle(2048).prefetch(16).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1b69ed1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_inputs shape: (64, 20)\n",
      "decoder_inputs shape: (64, 20)\n",
      "targets shape: (64, 20)\n"
     ]
    }
   ],
   "source": [
    "# Prepare the final vectorized data which basically takes the sequences of words for each language and convert them into integers\n",
    "train_int = make_dataset(train_data)\n",
    "val_int = make_dataset(val_data)\n",
    "test_int = make_dataset(test_data)\n",
    "\n",
    "# Display a sample\n",
    "for inputs, targets in train_int.take(1):\n",
    "    print(f\"encoder_inputs shape: {inputs['encoder_inputs'].shape}\")\n",
    "    print(f\"decoder_inputs shape: {inputs['decoder_inputs'].shape}\")\n",
    "    print(f\"targets shape: {targets.shape}\")\n",
    "\n",
    "# because of vectorization, the original dataset, \n",
    "# ..where each element was a sentence pair (english,spanish) is now \n",
    "# batches of data, with 64samples in each batch. \n",
    "\n",
    "# each sample is a tuple where first element is a dict of inputs, second element is targets\n",
    "# sequence length for inputs and targets is 20. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d273acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Inputs: tf.Tensor(\n",
      "[  26 2622    4 3945   42  468 4488    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0], shape=(20,), dtype=int64)\n",
      "Decoder Inputs: tf.Tensor(\n",
      "[   2   30 2511    1   10  467    1    3    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0], shape=(20,), dtype=int64)\n",
      "Targets: tf.Tensor(\n",
      "[  30 2511    1   10  467    1    3    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0], shape=(20,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_int.take(1):\n",
    "    print (\"Encoder Inputs:\", inputs['encoder_inputs'][0])\n",
    "    print (\"Decoder Inputs:\",inputs['decoder_inputs'][0])\n",
    "    print (\"Targets:\",targets[0])\n",
    "\n",
    "# As you see decoder_inputs is nothing but target sequence beginning with token [start] (index=2)\n",
    "# targets is also the target sequence but offset by one token and \n",
    "#...begins with element next to [start] token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71ead1b",
   "metadata": {},
   "source": [
    "### Build a Transformer based encoder-decoder model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d21088",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"./images/transformer_ed.jpg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "64fb561e",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e006959c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants or parameters\n",
    "VOCAB_SIZE = 15000 # Max tokens\n",
    "MAX_SEQ_LEN = 20 # Max sequence length\n",
    "\n",
    "EMBED_DIM = 256 # Embedding dimension\n",
    "HIDDEN_DIM = 1024 # Hidden dimension for dense layers\n",
    "BATCH_SIZE = 64 # Batch size\n",
    "NUM_HEADS = 8 # Number of heads for Multiheaded attention layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b941a85d",
   "metadata": {},
   "source": [
    "#### Embeddings class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3928412a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedEmbedding(layers.Layer):\n",
    "    def __init__(self, MAX_SEQ_LEN, VOCAB_SIZE, EMBED_DIM, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.token_embeddings = layers.Embedding(input_dim = VOCAB_SIZE, output_dim=EMBED_DIM, \n",
    "                                                 mask_zero=True) # input embedding layer\n",
    "        \n",
    "        self.position_embeddings = layers.Embedding(input_dim = MAX_SEQ_LEN, \n",
    "                                                    output_dim = EMBED_DIM) # position embedding layer\n",
    "        # both the above embeddings are initialized randomly first \n",
    "        #....and will be calculated as part of training process.\n",
    "        \n",
    "        self.sequence_length = MAX_SEQ_LEN\n",
    "        self.max_tokens = VOCAB_SIZE\n",
    "        self.embed_dim = EMBED_DIM\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        \n",
    "        # Word or token embeddings\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        \n",
    "        # Positional Embeddings\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        \n",
    "        return embedded_tokens + embedded_positions # Return combined embeddings\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return self.token_embeddings.compute_mask(inputs, mask)\n",
    "    # The compute_mask method in a custom layer ensures that the masking information \n",
    "    #...is correctly propagated through the layers.\n",
    "    # without this the mask may not be propagated properly through the sebsequent layers.\n",
    "\n",
    "    # whenever we use custom layers, mainly for saving and loading the model\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"max_tokens\": self.max_tokens,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94f5260",
   "metadata": {},
   "source": [
    "#### Transformer Encoder class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4143b6",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"./images/transformer_ed.jpg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "83616466",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, EMBED_DIM, NUM_HEADS, HIDDEN_DIM,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = EMBED_DIM\n",
    "        self.num_heads = NUM_HEADS\n",
    "        self.ff_dim = HIDDEN_DIM\n",
    "        \n",
    "        # Define Multiheaded Attention Layer\n",
    "        self.attention = layers.MultiHeadAttention(num_heads=NUM_HEADS, key_dim=EMBED_DIM)\n",
    "        \n",
    "        # Define Feed forward dense layers\n",
    "        self.ffn = keras.Sequential([\n",
    "            layers.Dense(HIDDEN_DIM, activation=\"relu\"),\n",
    "            layers.Dense(EMBED_DIM),])\n",
    "        \n",
    "        # Define Normalization Layers\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        \n",
    "    # Actual computation in the call method below.\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            # reshape since the attention layer expects 3d or 4d: \n",
    "            # (batch_size, num_heads, seq_length, seq_length)\n",
    "            mask = mask[:, tf.newaxis, :] \n",
    "        \n",
    "        # Multiheaded Attention Layer\n",
    "        attn_output = self.attention(inputs, inputs, inputs, attention_mask=mask)\n",
    "        \n",
    "        # Normalization Layer\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        \n",
    "        # Dense feed forward Layer\n",
    "        ffn_output = self.ffn(out1)\n",
    "        \n",
    "        # 2nd Normalization Layer\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "        \n",
    "        return out2\n",
    "\n",
    "    # this method is mainly for loading the saved model, when custom layers are used.\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"ff_dim\": self.ff_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f554deab",
   "metadata": {},
   "source": [
    "#### Transformer Decoder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d69899b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, EMBED_DIM, NUM_HEADS, HIDDEN_DIM, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = EMBED_DIM\n",
    "        self.num_heads = NUM_HEADS\n",
    "        self.ff_dim = HIDDEN_DIM\n",
    "        \n",
    "        #Attention Layers\n",
    "        self.attention1 = layers.MultiHeadAttention(num_heads=NUM_HEADS, key_dim=EMBED_DIM)\n",
    "        self.attention2 = layers.MultiHeadAttention(num_heads=NUM_HEADS, key_dim=EMBED_DIM)\n",
    "        \n",
    "        #Feedforward Dense Layer\n",
    "        self.ffn = keras.Sequential([\n",
    "            layers.Dense(HIDDEN_DIM, activation=\"relu\"),\n",
    "            layers.Dense(EMBED_DIM),])\n",
    "        \n",
    "        #Normalization\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.supports_masking=True\n",
    "        \n",
    "\n",
    "    # Causal mask for the Decoder Inputs. because we dont want attention on future tokens\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1),\n",
    "             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
    "        return tf.tile(mask, mult)\n",
    "\n",
    "# 1 0 0 0 0\n",
    "# 1 1 0 0 0\n",
    "# 1 1 1 0 0\n",
    "# 1 1 1 1 0\n",
    "# 1 1 1 1 1\n",
    "\n",
    "    # Main computation inside the call method\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        \n",
    "        # Causal mask for decoder inputs: self attention\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        \n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(\n",
    "                mask[:, tf.newaxis, :], dtype=\"int32\")\n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask) # combined both masks\n",
    "        else:\n",
    "            padding_mask = mask\n",
    "            \n",
    "        # Self Attention for Decoder inputs\n",
    "        attention_output1 = self.attention1(query=inputs,\n",
    "                                            value=inputs,\n",
    "                                            key=inputs,\n",
    "                                            attention_mask=causal_mask)\n",
    "        out1 = self.layernorm1(inputs + attention_output1)\n",
    "        \n",
    "        # Cross Attention between self attended Decoder inputs & encoder outputs.\n",
    "        attention_output2 = self.attention2(query=out1,\n",
    "                                            value=encoder_outputs,\n",
    "                                            key=encoder_outputs,\n",
    "                                            attention_mask=padding_mask)\n",
    "        out2 = self.layernorm2(out1 + attention_output2)\n",
    "        \n",
    "        # Feed forward dense layers\n",
    "        ffn_output = self.ffn(out2)\n",
    "        \n",
    "        out3 = self.layernorm3(out2 + ffn_output)\n",
    "        \n",
    "        return out3\n",
    "\n",
    "    #for loading the saved model, with custom layers.    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"ff_dim\": self.ff_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f010a7",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"./images/transformer_ed.jpg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b7b27f",
   "metadata": {},
   "source": [
    "**Encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "29e77b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an encoder, which takes an input source sentence, and encodes it as vector with \n",
    "#.. EMBED_DIM units.\n",
    "# basically it captured the essense of the sentence.\n",
    "# Returns a context aware sequence of vectors, unlike RNN that returns a single vector at last time step\n",
    "\n",
    "# Encoder Input \n",
    "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
    "\n",
    "# Embedding class (for Both token + Positional Embeddings) for Embedding layer\n",
    "x = CombinedEmbedding(MAX_SEQ_LEN, VOCAB_SIZE, EMBED_DIM)(encoder_inputs)\n",
    "\n",
    "# Transformer Encoder Layer\n",
    "encoder_outputs= TransformerEncoder(EMBED_DIM, NUM_HEADS, HIDDEN_DIM)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec2fd93",
   "metadata": {},
   "source": [
    "**Decoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "21d9d385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the decoder now\n",
    "\n",
    "# Inputs\n",
    "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
    "\n",
    "# Embedding layer (combined word embeddings + positional embeddings)\n",
    "x = CombinedEmbedding(MAX_SEQ_LEN, VOCAB_SIZE, EMBED_DIM)(decoder_inputs)\n",
    "\n",
    "# Transformer Decoder Block\n",
    "x = TransformerDecoder(EMBED_DIM, NUM_HEADS, HIDDEN_DIM)(x,encoder_outputs)\n",
    "\n",
    "# Dropout Layer\n",
    "x = layers.Dropout(0.4)(x)\n",
    "\n",
    "# Final dense layer mapping probability distribution over spanish vocabulary\n",
    "decoder_outputs= layers.Dense(VOCAB_SIZE, activation=\"softmax\")(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1e695468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " combined_embedding_1 (Combined  (None, None, 256)   3845120     ['encoder_inputs[0][0]']         \n",
      " Embedding)                                                                                       \n",
      "                                                                                                  \n",
      " combined_embedding_2 (Combined  (None, None, 256)   3845120     ['decoder_inputs[0][0]']         \n",
      " Embedding)                                                                                       \n",
      "                                                                                                  \n",
      " transformer_encoder (Transform  (None, None, 256)   2630144     ['combined_embedding_1[0][0]']   \n",
      " erEncoder)                                                                                       \n",
      "                                                                                                  \n",
      " transformer_decoder (Transform  (None, None, 256)   4734208     ['combined_embedding_2[0][0]',   \n",
      " erDecoder)                                                       'transformer_encoder[0][0]']    \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, None, 256)    0           ['transformer_decoder[0][0]']    \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, None, 15000)  3855000     ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 18,909,592\n",
      "Trainable params: 18,909,592\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model_transformer.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc21e39b",
   "metadata": {},
   "source": [
    "### Compile & Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "840102b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "path = Path(\"./models/model_transformer.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "080b2b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will reuse this function to train and evaluate for convenience\n",
    "def train_evaluate(model,path,train,val,test):\n",
    "    \n",
    "    #call backs\n",
    "    checkpoint_cb = keras.callbacks.ModelCheckpoint(filepath = path,\n",
    "                                                       save_best_only=True) # Save only best model\n",
    "    \n",
    "    earlystop_cb = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    callbacks = [checkpoint_cb, earlystop_cb]\n",
    "\n",
    "    #Compile the model\n",
    "    model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\",  metrics = [\"accuracy\"])\n",
    "    \n",
    "    #Train the model\n",
    "    history = model.fit(train, validation_data = val, callbacks = callbacks, epochs=50)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    test_loss, test_accuracy = model.evaluate(test)\n",
    "    \n",
    "    return (history,test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e061d2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1302/1302 [==============================] - 88s 61ms/step - loss: 1.5605 - accuracy: 0.4536 - val_loss: 1.1920 - val_accuracy: 0.5512\n",
      "Epoch 2/50\n",
      "1302/1302 [==============================] - 74s 57ms/step - loss: 1.2278 - accuracy: 0.5663 - val_loss: 1.0665 - val_accuracy: 0.5980\n",
      "Epoch 3/50\n",
      "1302/1302 [==============================] - 76s 58ms/step - loss: 1.0931 - accuracy: 0.6083 - val_loss: 1.0136 - val_accuracy: 0.6169\n",
      "Epoch 4/50\n",
      "1302/1302 [==============================] - 75s 58ms/step - loss: 1.0180 - accuracy: 0.6352 - val_loss: 0.9732 - val_accuracy: 0.6385\n",
      "Epoch 5/50\n",
      "1302/1302 [==============================] - 80s 62ms/step - loss: 0.9763 - accuracy: 0.6544 - val_loss: 0.9547 - val_accuracy: 0.6480\n",
      "Epoch 6/50\n",
      "1302/1302 [==============================] - 77s 59ms/step - loss: 0.9489 - accuracy: 0.6683 - val_loss: 0.9458 - val_accuracy: 0.6533\n",
      "Epoch 7/50\n",
      "1302/1302 [==============================] - 75s 57ms/step - loss: 0.9293 - accuracy: 0.6806 - val_loss: 0.9456 - val_accuracy: 0.6549\n",
      "Epoch 8/50\n",
      "1302/1302 [==============================] - 69s 53ms/step - loss: 0.9138 - accuracy: 0.6893 - val_loss: 0.9461 - val_accuracy: 0.6562\n",
      "Epoch 9/50\n",
      "1302/1302 [==============================] - 76s 59ms/step - loss: 0.8990 - accuracy: 0.6974 - val_loss: 0.9444 - val_accuracy: 0.6594\n",
      "Epoch 10/50\n",
      "1302/1302 [==============================] - 76s 58ms/step - loss: 0.8864 - accuracy: 0.7043 - val_loss: 0.9434 - val_accuracy: 0.6612\n",
      "Epoch 11/50\n",
      "1302/1302 [==============================] - 69s 53ms/step - loss: 0.8746 - accuracy: 0.7105 - val_loss: 0.9447 - val_accuracy: 0.6619\n",
      "Epoch 12/50\n",
      "1302/1302 [==============================] - 70s 54ms/step - loss: 0.8641 - accuracy: 0.7155 - val_loss: 0.9538 - val_accuracy: 0.6614\n",
      "Epoch 13/50\n",
      "1302/1302 [==============================] - 69s 53ms/step - loss: 0.8528 - accuracy: 0.7208 - val_loss: 0.9487 - val_accuracy: 0.6634\n",
      "Epoch 14/50\n",
      "1302/1302 [==============================] - 68s 52ms/step - loss: 0.8432 - accuracy: 0.7256 - val_loss: 0.9538 - val_accuracy: 0.6615\n",
      "Epoch 15/50\n",
      "1302/1302 [==============================] - 69s 53ms/step - loss: 0.8345 - accuracy: 0.7289 - val_loss: 0.9600 - val_accuracy: 0.6626\n",
      "279/279 [==============================] - 12s 27ms/step - loss: 0.9361 - accuracy: 0.6652\n"
     ]
    }
   ],
   "source": [
    "(history_transformer,test_accuracy_transformer) = train_evaluate(model_transformer,path,\n",
    "                                                                 train_int,\n",
    "                                                                 val_int,\n",
    "                                                                 test_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "23c278ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test data set is 0.6651646494865417\n"
     ]
    }
   ],
   "source": [
    "print (f\"Accuracy on the test data set is {test_accuracy_transformer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1820e8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import load_model\n",
    "\n",
    "# # Define all custom layers\n",
    "# custom_objects = {\n",
    "#     \"CombinedEmbedding\": CombinedEmbedding,\n",
    "#     \"TransformerEncoder\": TransformerEncoder,\n",
    "#     \"TransformerDecoder\": TransformerDecoder\n",
    "# }\n",
    "\n",
    "# # Load the model with all custom layers specified\n",
    "# loaded_model = load_model(\"model_transformer.keras\", custom_objects=custom_objects)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db51f83",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"./images/transformer_ed.jpg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50a78ca",
   "metadata": {},
   "source": [
    "### Inference: Translate Few sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "84482611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "tf.Tensor(b'I caught Tom cheating.', shape=(), dtype=string)\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "[start] yo [UNK] a tom [UNK] [end]\n",
      "\n",
      "\n",
      "tf.Tensor(b'I caught Tom cheating.', shape=(), dtype=string)\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "[start] yo [UNK] a tom [UNK] [end]\n",
      "\n",
      "\n",
      "tf.Tensor(b\"Let's find something to sit on.\", shape=(), dtype=string)\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "[start] [UNK] algo para [UNK] [end]\n",
      "\n",
      "\n",
      "tf.Tensor(b'I came here when I was a kid.', shape=(), dtype=string)\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "[start] he venido aquí cuando era niño [end]\n",
      "\n",
      "\n",
      "tf.Tensor(b'You must be very hungry now.', shape=(), dtype=string)\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "[start] tienes que tener mucha hambre ahora [end]\n",
      "\n",
      "\n",
      "tf.Tensor(b'A pair of gloves is a nice gift.', shape=(), dtype=string)\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "[start] algunos días [UNK] es un regalo [end]\n",
      "\n",
      "\n",
      "tf.Tensor(b'What did you do in there?', shape=(), dtype=string)\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "[start] qué hiciste con aquí [end]\n",
      "\n",
      "\n",
      "tf.Tensor(b'I saw him naked.', shape=(), dtype=string)\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "[start] lo vi a las [UNK] [end]\n",
      "\n",
      "\n",
      "tf.Tensor(b'What time do you walk the dog?', shape=(), dtype=string)\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "[start] a qué hora vas a caminar al perro [end]\n",
      "\n",
      "\n",
      "tf.Tensor(b'Tom is very blunt.', shape=(), dtype=string)\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "[start] tom es muy [UNK] [end]\n",
      "\n",
      "\n",
      "tf.Tensor(b'What did you do in there?', shape=(), dtype=string)\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "[start] qué hiciste con aquí [end]\n",
      "\n",
      "\n",
      "tf.Tensor(b'He kept on writing stories about animals.', shape=(), dtype=string)\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "[start] Él decidió a escribir sobre los animales [end]\n",
      "\n",
      "\n",
      "tf.Tensor(b\"Let's find something to sit on.\", shape=(), dtype=string)\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "[start] [UNK] algo para [UNK] [end]\n",
      "\n",
      "\n",
      "tf.Tensor(b'I caught Tom cheating.', shape=(), dtype=string)\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "[start] yo [UNK] a tom [UNK] [end]\n",
      "\n",
      "\n",
      "tf.Tensor(b'Put on your pajamas.', shape=(), dtype=string)\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "[start] [UNK] tus se puso [end]\n",
      "\n",
      "\n",
      "tf.Tensor(b'I still have to run errands.', shape=(), dtype=string)\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "[start] todavía tengo la [UNK] [end]\n",
      "\n",
      "\n",
      "tf.Tensor(b'Tom is very blunt.', shape=(), dtype=string)\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "[start] tom es muy [UNK] [end]\n",
      "\n",
      "\n",
      "tf.Tensor(b'You must be very hungry now.', shape=(), dtype=string)\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "[start] tienes que tener mucha hambre ahora [end]\n",
      "\n",
      "\n",
      "tf.Tensor(b\"They're always having trouble with their word processor.\", shape=(), dtype=string)\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[start] ellos están problemas con su palabra de [UNK] [UNK] [end]\n",
      "\n",
      "\n",
      "tf.Tensor(b'He kept on writing stories about animals.', shape=(), dtype=string)\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "[start] Él decidió a escribir sobre los animales [end]\n"
     ]
    }
   ],
   "source": [
    "def translate_sequence(input_sentence):\n",
    "    vectorized_input_sentence = source_vectorizer([input_sentence])\n",
    "    target_sentence = \"[start]\"\n",
    "    for i in range(MAX_SEQ_LEN):\n",
    "        vectorized_target_sentence = target_vectorizer([target_sentence])[:,:-1]\n",
    "        #The [:,:-1] slicing is essential in Transformer-based architectures to ensure that the \n",
    "        # ...target sequence fed into the decoder does not include the [END] token or padding\n",
    "        \n",
    "        next_token_predictions = model_transformer.predict(\n",
    "            [vectorized_input_sentence, vectorized_target_sentence])\n",
    "        sampled_token_index = np.argmax(next_token_predictions[0, i, :])\n",
    "        sampled_token = target_index[sampled_token_index]\n",
    "        target_sentence += \" \" + sampled_token\n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "    return target_sentence\n",
    "\n",
    "\n",
    "source_sentences = [en for en,sp in test_data.take(20)]\n",
    "\n",
    "for _ in range(20):\n",
    "    input_sentence = random.choice(source_sentences)\n",
    "    print(\"\\n\")\n",
    "    print(input_sentence)\n",
    "    print(translate_sequence(input_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcde67c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

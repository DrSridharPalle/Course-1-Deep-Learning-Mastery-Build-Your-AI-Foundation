{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29d3b3fe",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"./images/PallenceAI-Final.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccee792",
   "metadata": {},
   "source": [
    "# Text Generation Language Model: Sequence to Sequence "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2b3807",
   "metadata": {},
   "source": [
    "## Transformers: Decoder Only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa92b3de",
   "metadata": {},
   "source": [
    "### Lets Build a Decoder only model on IMDB dataset for Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162adc49",
   "metadata": {},
   "source": [
    "### Import needed libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6a9babbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic Python packages for data wrangling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "#Tensorflow & Keras related packages\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "from utils import plot_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ebcf38",
   "metadata": {},
   "source": [
    "### Load the IMDB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c67d4a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow_datasets as tfds\n",
    "\n",
    "# train, val, test = tfds.load(\n",
    "#     name=\"imdb_reviews\",\n",
    "#     split=[\"train[:80%]\", \"train[80%:]\", \"test\"],\n",
    "#     as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ad4102d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Load all splits, including unsupervised\n",
    "dataset_labeled = tfds.load(name=\"imdb_reviews\", split=\"train+test\", as_supervised=False)\n",
    "dataset_unsupervised = tfds.load(name=\"imdb_reviews\", split=\"unsupervised\", as_supervised=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1beb74d",
   "metadata": {},
   "source": [
    "### Understand & Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b99b4e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec={'label': TensorSpec(shape=(), dtype=tf.int64, name=None), 'text': TensorSpec(shape=(), dtype=tf.string, name=None)}>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "09fe2611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract only the 'text' feature from both splits\n",
    "reviews_labeled = dataset_labeled.map(lambda x: x[\"text\"])\n",
    "reviews_unsupervised = dataset_unsupervised.map(lambda x: x[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ba13d26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine labeled and unsupervised reviews if desired\n",
    "all_reviews = reviews_labeled.concatenate(reviews_unsupervised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c1684423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ConcatenateDataset element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c9e7a62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "# Lets see how many total reviews we have\n",
    "print (len(all_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2623a083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=TensorSpec(shape=(None,), dtype=tf.string, name=None)>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch the data\n",
    "imdb = all_reviews.batch(256)\n",
    "imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ad339b49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=TensorSpec(shape=(None,), dtype=tf.string, name=None)>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "45b54f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews shape (256,)\n",
      "First Review: This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lets take a peek at a sample review in the first batch\n",
    "for reviews in imdb.take(1):\n",
    "    print (\"Reviews shape\", reviews.shape)\n",
    "    \n",
    "    print ('First Review:', reviews[0].numpy().decode(\"utf-8\"), \"\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a16a24",
   "metadata": {},
   "source": [
    "**Vectorize the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cc17c933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants \n",
    "VOCAB_SIZE = 15000 # Max tokens\n",
    "MAX_SEQ_LEN = 100 # Max sequence length\n",
    "EMBED_DIM = 256 # Embedding dimension\n",
    "HIDDEN_DIM = 2048 # Hidden dimension for dense layers\n",
    "BATCH_SIZE= 256 # Batch size\n",
    "NUM_HEADS = 2 # Number of heads for Multiheaded attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ab9320bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the vectorizer\n",
    "\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    # Convert to lowercase\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    # Replace <br /> HTML tags with space\n",
    "    no_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
    "    return no_html\n",
    "\n",
    "vectorizer = layers.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode=\"int\",\n",
    "    # we add an extra token since target will be offset by one token\n",
    "    output_sequence_length=MAX_SEQ_LEN, \n",
    "    standardize=custom_standardization,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c57fcd12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=TensorSpec(shape=(None,), dtype=tf.string, name=None)>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "744406a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapt the vectorizer to the dataset. Meaning creating the vocabulary \n",
    "vectorizer.adapt(imdb) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6041ae55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'the',\n",
       " 'a',\n",
       " 'and',\n",
       " 'of',\n",
       " 'to',\n",
       " 'is',\n",
       " 'in',\n",
       " 'i',\n",
       " 'this',\n",
       " 'it',\n",
       " 'that',\n",
       " 'was',\n",
       " 'as',\n",
       " 'with',\n",
       " 'for',\n",
       " 'but',\n",
       " 'on',\n",
       " 'movie',\n",
       " 'are',\n",
       " 'his',\n",
       " 'not',\n",
       " 'you',\n",
       " 'film',\n",
       " 'have',\n",
       " 'be',\n",
       " 'he',\n",
       " 'one',\n",
       " 'at',\n",
       " 'by',\n",
       " 'an',\n",
       " 'they',\n",
       " 'from',\n",
       " 'all',\n",
       " 'who',\n",
       " 'like',\n",
       " 'so',\n",
       " 'just',\n",
       " 'or',\n",
       " 'has',\n",
       " \"it's\",\n",
       " 'about',\n",
       " 'if',\n",
       " 'her',\n",
       " 'some',\n",
       " 'out',\n",
       " 'what',\n",
       " 'there',\n",
       " 'when',\n",
       " 'very',\n",
       " 'more',\n",
       " 'even',\n",
       " 'would',\n",
       " 'my',\n",
       " 'good',\n",
       " 'she',\n",
       " 'their',\n",
       " 'no',\n",
       " 'only',\n",
       " 'really',\n",
       " 'up',\n",
       " 'had',\n",
       " 'can',\n",
       " 'which',\n",
       " 'see',\n",
       " 'were',\n",
       " 'than',\n",
       " 'we',\n",
       " '-',\n",
       " 'been',\n",
       " 'into',\n",
       " 'get',\n",
       " 'will',\n",
       " 'much',\n",
       " 'because',\n",
       " 'most',\n",
       " 'story',\n",
       " 'how',\n",
       " 'other',\n",
       " 'also',\n",
       " \"don't\",\n",
       " 'first',\n",
       " 'do',\n",
       " 'its',\n",
       " 'great',\n",
       " 'time',\n",
       " 'make',\n",
       " 'me',\n",
       " 'people',\n",
       " 'could',\n",
       " 'any',\n",
       " 'after',\n",
       " 'then',\n",
       " 'bad',\n",
       " 'made',\n",
       " 'think',\n",
       " 'being',\n",
       " 'many',\n",
       " 'it.',\n",
       " 'never',\n",
       " 'him',\n",
       " 'two',\n",
       " 'movie.',\n",
       " 'where',\n",
       " 'too',\n",
       " 'little',\n",
       " 'well',\n",
       " 'watch',\n",
       " 'way',\n",
       " 'your',\n",
       " 'them',\n",
       " 'did',\n",
       " 'does',\n",
       " 'best',\n",
       " 'characters',\n",
       " 'love',\n",
       " 'know',\n",
       " 'these',\n",
       " 'character',\n",
       " 'film.',\n",
       " 'movies',\n",
       " 'seen',\n",
       " 'ever',\n",
       " 'while',\n",
       " 'films',\n",
       " 'still',\n",
       " 'over',\n",
       " 'plot',\n",
       " 'should',\n",
       " 'such',\n",
       " 'acting',\n",
       " 'those',\n",
       " 'better',\n",
       " 'off',\n",
       " 'show',\n",
       " 'why',\n",
       " 'go',\n",
       " 'something',\n",
       " \"doesn't\",\n",
       " \"i'm\",\n",
       " 'through',\n",
       " 'say',\n",
       " \"didn't\",\n",
       " 'makes',\n",
       " 'scene',\n",
       " 'real',\n",
       " 'movie,',\n",
       " 'watching',\n",
       " 'back',\n",
       " 'find',\n",
       " 'film,',\n",
       " 'every',\n",
       " 'scenes',\n",
       " 'going',\n",
       " 'actually',\n",
       " 'new',\n",
       " 'few',\n",
       " 'another',\n",
       " 'same',\n",
       " 'man',\n",
       " 'nothing',\n",
       " 'life',\n",
       " 'look',\n",
       " 'lot',\n",
       " '&',\n",
       " 'quite',\n",
       " 'thing',\n",
       " 'want',\n",
       " 'end',\n",
       " 'pretty',\n",
       " 'old',\n",
       " 'seems',\n",
       " \"can't\",\n",
       " 'before',\n",
       " 'got',\n",
       " 'take',\n",
       " 'give',\n",
       " 'part',\n",
       " 'actors',\n",
       " 'years',\n",
       " \"that's\",\n",
       " 'both',\n",
       " 'now',\n",
       " 'between',\n",
       " 'young',\n",
       " \"i've\",\n",
       " 'may',\n",
       " 'things',\n",
       " 'without',\n",
       " 'us',\n",
       " 'thought',\n",
       " 'director',\n",
       " 'though',\n",
       " \"there's\",\n",
       " 'big',\n",
       " 'around',\n",
       " 'must',\n",
       " 'it,',\n",
       " 'saw',\n",
       " 'gets',\n",
       " 'almost',\n",
       " 'here',\n",
       " \"isn't\",\n",
       " 'own',\n",
       " 'whole',\n",
       " 'always',\n",
       " 'horror',\n",
       " 'come',\n",
       " '\"the',\n",
       " \"he's\",\n",
       " 'down',\n",
       " 'work',\n",
       " 'might',\n",
       " 'cast',\n",
       " 'enough',\n",
       " 'bit',\n",
       " 'last',\n",
       " 'since',\n",
       " 'am',\n",
       " 'least',\n",
       " 'probably',\n",
       " 'feel',\n",
       " 'long',\n",
       " 'funny',\n",
       " 'far',\n",
       " 'action',\n",
       " 'each',\n",
       " 'fact',\n",
       " 'rather',\n",
       " 'kind',\n",
       " 'found',\n",
       " 'however,',\n",
       " 'our',\n",
       " 'having',\n",
       " 'original',\n",
       " 'world',\n",
       " 'trying',\n",
       " 'interesting',\n",
       " 'worst',\n",
       " 'guy',\n",
       " 'anything',\n",
       " 'making',\n",
       " 'comes',\n",
       " 'right',\n",
       " 'done',\n",
       " 'believe',\n",
       " 'worth',\n",
       " 'anyone',\n",
       " 'played',\n",
       " 'put',\n",
       " 'main',\n",
       " 'goes',\n",
       " 'point',\n",
       " 'time.',\n",
       " 'yet',\n",
       " 'music',\n",
       " 'hard',\n",
       " 'role',\n",
       " 'especially',\n",
       " 'during',\n",
       " 'looking',\n",
       " 'looks',\n",
       " 'plays',\n",
       " \"wasn't\",\n",
       " 'series',\n",
       " 'although',\n",
       " 'maybe',\n",
       " 'takes',\n",
       " 'three',\n",
       " 'different',\n",
       " 'seem',\n",
       " 'script',\n",
       " 'minutes',\n",
       " 'sure',\n",
       " 'performance',\n",
       " 'tv',\n",
       " 'someone',\n",
       " 'watched',\n",
       " 'shows',\n",
       " 'away',\n",
       " 'family',\n",
       " 'comedy',\n",
       " 'everything',\n",
       " 'set',\n",
       " 'times',\n",
       " 'girl',\n",
       " 'american',\n",
       " 'woman',\n",
       " 'once',\n",
       " 'john',\n",
       " 'simply',\n",
       " 'special',\n",
       " 'left',\n",
       " \"you're\",\n",
       " 'seeing',\n",
       " 'fun',\n",
       " 'well,',\n",
       " 'completely',\n",
       " 'used',\n",
       " 'everyone',\n",
       " 'play',\n",
       " 'again',\n",
       " 'reason',\n",
       " 'until',\n",
       " 'black',\n",
       " 'high',\n",
       " 'true',\n",
       " 'sense',\n",
       " '--',\n",
       " 'need',\n",
       " 'use',\n",
       " 'given',\n",
       " 'idea',\n",
       " 'truly',\n",
       " 'read',\n",
       " 'try',\n",
       " 'place',\n",
       " 'dvd',\n",
       " 'nice',\n",
       " 'help',\n",
       " 'getting',\n",
       " 'second',\n",
       " 'keep',\n",
       " 'ending',\n",
       " 'along',\n",
       " 'came',\n",
       " 'recommend',\n",
       " 'couple',\n",
       " 'poor',\n",
       " 'instead',\n",
       " 'let',\n",
       " 'money',\n",
       " 'less',\n",
       " 'beautiful',\n",
       " 'effects',\n",
       " 'excellent',\n",
       " 'rest',\n",
       " 'job',\n",
       " 'full',\n",
       " 'half',\n",
       " 'shot',\n",
       " 'said',\n",
       " 'tell',\n",
       " 'gives',\n",
       " 'actor',\n",
       " 'definitely',\n",
       " 'day',\n",
       " 'enjoy',\n",
       " 'them.',\n",
       " 'next',\n",
       " '(and',\n",
       " 'went',\n",
       " 'playing',\n",
       " 'understand',\n",
       " \"couldn't\",\n",
       " 'audience',\n",
       " 'version',\n",
       " 'perhaps',\n",
       " 'war',\n",
       " '2',\n",
       " 'start',\n",
       " 'together',\n",
       " 'himself',\n",
       " 'fan',\n",
       " 'screen',\n",
       " 'absolutely',\n",
       " 'early',\n",
       " 'become',\n",
       " 'certainly',\n",
       " 'small',\n",
       " 'supposed',\n",
       " 'several',\n",
       " 'entire',\n",
       " 'doing',\n",
       " 'liked',\n",
       " 'later',\n",
       " 'hollywood',\n",
       " 'remember',\n",
       " 'all,',\n",
       " 'book',\n",
       " 'one.',\n",
       " 'against',\n",
       " 'often',\n",
       " 'men',\n",
       " 'star',\n",
       " 'time,',\n",
       " '10',\n",
       " 'wife',\n",
       " 'felt',\n",
       " 'sort',\n",
       " 'short',\n",
       " 'seemed',\n",
       " 'totally',\n",
       " 'night',\n",
       " '(the',\n",
       " 'human',\n",
       " 'him.',\n",
       " 'is,',\n",
       " 'hope',\n",
       " 'year',\n",
       " 'based',\n",
       " '.',\n",
       " 'camera',\n",
       " 'kids',\n",
       " 'wonderful',\n",
       " 'waste',\n",
       " 'loved',\n",
       " 'becomes',\n",
       " 'this.',\n",
       " 'final',\n",
       " 'despite',\n",
       " 'production',\n",
       " 'women',\n",
       " 'top',\n",
       " 'else',\n",
       " 'performances',\n",
       " 'death',\n",
       " \"she's\",\n",
       " 'able',\n",
       " \"you'll\",\n",
       " 'well.',\n",
       " 'wanted',\n",
       " 'piece',\n",
       " 'home',\n",
       " 'live',\n",
       " 'lost',\n",
       " 'sex',\n",
       " 'that,',\n",
       " 'classic',\n",
       " 'course',\n",
       " 'story.',\n",
       " 'friends',\n",
       " 'line',\n",
       " 'video',\n",
       " 'me.',\n",
       " 'called',\n",
       " 'under',\n",
       " \"i'd\",\n",
       " 'house',\n",
       " 'dead',\n",
       " 'tries',\n",
       " 'wants',\n",
       " 'school',\n",
       " 'mind',\n",
       " 'person',\n",
       " '\\x96',\n",
       " 'name',\n",
       " 'lead',\n",
       " 'father',\n",
       " 'all.',\n",
       " \"they're\",\n",
       " 'low',\n",
       " 'stupid',\n",
       " 'gave',\n",
       " \"won't\",\n",
       " 'story,',\n",
       " 'turn',\n",
       " 'this,',\n",
       " 'perfect',\n",
       " 'already',\n",
       " 'care',\n",
       " 'enjoyed',\n",
       " 'finally',\n",
       " 'either',\n",
       " 'starts',\n",
       " 'problem',\n",
       " 'budget',\n",
       " 'title',\n",
       " 'turns',\n",
       " 'sound',\n",
       " 'moments',\n",
       " 'written',\n",
       " 'guess',\n",
       " 'face',\n",
       " 'good.',\n",
       " 'lines',\n",
       " 'mean',\n",
       " 'life.',\n",
       " 'took',\n",
       " 'and,',\n",
       " 'head',\n",
       " 'favorite',\n",
       " 'kill',\n",
       " 'boring',\n",
       " 'extremely',\n",
       " 'me,',\n",
       " 'itself',\n",
       " 'episode',\n",
       " 'behind',\n",
       " 'terrible',\n",
       " 'dialogue',\n",
       " 'highly',\n",
       " 'guys',\n",
       " 'mother',\n",
       " 'others',\n",
       " 'movies.',\n",
       " 'fans',\n",
       " 'friend',\n",
       " 'looked',\n",
       " 'group',\n",
       " 'dark',\n",
       " 'beginning',\n",
       " \"wouldn't\",\n",
       " 'throughout',\n",
       " 'obviously',\n",
       " 'boy',\n",
       " 'expect',\n",
       " 'fine',\n",
       " 'stars',\n",
       " 'decent',\n",
       " 'michael',\n",
       " 'style',\n",
       " 'white',\n",
       " 'evil',\n",
       " 'taken',\n",
       " 'save',\n",
       " 'cannot',\n",
       " 'heard',\n",
       " 'lives',\n",
       " 'sometimes',\n",
       " '3',\n",
       " 'directed',\n",
       " 'run',\n",
       " 'mr.',\n",
       " \"film's\",\n",
       " 'attempt',\n",
       " 'lack',\n",
       " 'wrong',\n",
       " 'good,',\n",
       " 'case',\n",
       " 'laugh',\n",
       " 'entertaining',\n",
       " 'fight',\n",
       " 'that.',\n",
       " 'leave',\n",
       " 'awful',\n",
       " 'feeling',\n",
       " 'quality',\n",
       " 'picture',\n",
       " 'bad.',\n",
       " 'late',\n",
       " 'killed',\n",
       " 'across',\n",
       " 'soon',\n",
       " 'exactly',\n",
       " 'complete',\n",
       " 'here.',\n",
       " 'type',\n",
       " 'out.',\n",
       " 'except',\n",
       " 'movies,',\n",
       " 'particularly',\n",
       " 'wonder',\n",
       " 'way.',\n",
       " 'stop',\n",
       " 'coming',\n",
       " 'known',\n",
       " 'course,',\n",
       " 'direction',\n",
       " 'works',\n",
       " 'living',\n",
       " 'thinking',\n",
       " 'finds',\n",
       " 'game',\n",
       " 'told',\n",
       " 'says',\n",
       " 'close',\n",
       " 'whose',\n",
       " 'robert',\n",
       " 'act',\n",
       " 'killer',\n",
       " 'usually',\n",
       " 'police',\n",
       " 'car',\n",
       " 'taking',\n",
       " 'parts',\n",
       " 'turned',\n",
       " 'opening',\n",
       " 'viewer',\n",
       " 'past',\n",
       " 'end.',\n",
       " 'again.',\n",
       " 'somewhat',\n",
       " 'worse',\n",
       " 'running',\n",
       " ',',\n",
       " 'writing',\n",
       " 'david',\n",
       " 'films,',\n",
       " 'side',\n",
       " 'obvious',\n",
       " 'wish',\n",
       " 'son',\n",
       " 'huge',\n",
       " 'is.',\n",
       " 'acting,',\n",
       " 'seen.',\n",
       " 'amazing',\n",
       " 'major',\n",
       " 'none',\n",
       " 'yes,',\n",
       " 'strong',\n",
       " 'call',\n",
       " 'films.',\n",
       " \"i'll\",\n",
       " 'female',\n",
       " 'james',\n",
       " 'humor',\n",
       " 'shown',\n",
       " 'matter',\n",
       " 'number',\n",
       " 'children',\n",
       " 'horrible',\n",
       " 'knew',\n",
       " 'here,',\n",
       " 'happens',\n",
       " 'again,',\n",
       " 'myself',\n",
       " 'including',\n",
       " 'town',\n",
       " 'tells',\n",
       " 'brilliant',\n",
       " 'beyond',\n",
       " 'local',\n",
       " 'fact,',\n",
       " 'single',\n",
       " 'so,',\n",
       " 'overall',\n",
       " 'hour',\n",
       " 'characters,',\n",
       " 'due',\n",
       " 'started',\n",
       " 'mostly',\n",
       " 'order',\n",
       " 'however',\n",
       " 'clearly',\n",
       " 'but,',\n",
       " 'girls',\n",
       " 'drama',\n",
       " 'important',\n",
       " 'bring',\n",
       " 'themselves',\n",
       " 'way,',\n",
       " 'involved',\n",
       " 'serious',\n",
       " 'giving',\n",
       " 'history',\n",
       " \"aren't\",\n",
       " 'british',\n",
       " 'her.',\n",
       " 'end,',\n",
       " 'saying',\n",
       " 'one,',\n",
       " 'upon',\n",
       " 'ends',\n",
       " 'on.',\n",
       " 'lots',\n",
       " 'cinema',\n",
       " 'supporting',\n",
       " 'relationship',\n",
       " 'bad,',\n",
       " 'cut',\n",
       " 'kid',\n",
       " 'hit',\n",
       " 'stories',\n",
       " 'city',\n",
       " \"haven't\",\n",
       " 'four',\n",
       " 'happened',\n",
       " 'strange',\n",
       " 'certain',\n",
       " 'add',\n",
       " 'too.',\n",
       " 'days',\n",
       " 'heart',\n",
       " 'basically',\n",
       " 'talking',\n",
       " 'falls',\n",
       " 'actress',\n",
       " 'whether',\n",
       " 'funny.',\n",
       " 'chance',\n",
       " 'appears',\n",
       " 'child',\n",
       " 'modern',\n",
       " 'knows',\n",
       " 'miss',\n",
       " 'named',\n",
       " 'among',\n",
       " 'eyes',\n",
       " 'change',\n",
       " 'easily',\n",
       " 'kept',\n",
       " 'apparently',\n",
       " 'within',\n",
       " 'moment',\n",
       " 'released',\n",
       " 'voice',\n",
       " 'similar',\n",
       " 'him,',\n",
       " 'french',\n",
       " 'simple',\n",
       " 'typical',\n",
       " 'daughter',\n",
       " 'stuff',\n",
       " 'score',\n",
       " 'using',\n",
       " 'comic',\n",
       " 'bunch',\n",
       " 'near',\n",
       " 'blood',\n",
       " 'mention',\n",
       " 'brought',\n",
       " 'characters.',\n",
       " 'nearly',\n",
       " 'feels',\n",
       " 'jack',\n",
       " \"what's\",\n",
       " 'interest',\n",
       " 'plot,',\n",
       " 'showing',\n",
       " 'stay',\n",
       " 'english',\n",
       " 'talk',\n",
       " 'hours',\n",
       " 'middle',\n",
       " 'art',\n",
       " 'hate',\n",
       " 'tried',\n",
       " 'needs',\n",
       " 'fall',\n",
       " 'also,',\n",
       " 'murder',\n",
       " 'usual',\n",
       " 'working',\n",
       " 'romantic',\n",
       " 'slow',\n",
       " 'life,',\n",
       " 'example',\n",
       " 'oh',\n",
       " 'sad',\n",
       " 'george',\n",
       " 'cheap',\n",
       " 'happy',\n",
       " 'song',\n",
       " 'five',\n",
       " 'ten',\n",
       " 'power',\n",
       " 'actual',\n",
       " 'buy',\n",
       " '(i',\n",
       " 'shots',\n",
       " 'please',\n",
       " 'musical',\n",
       " \"you've\",\n",
       " 'body',\n",
       " 'hell',\n",
       " 'greatest',\n",
       " 'silly',\n",
       " 'easy',\n",
       " 'above',\n",
       " 'happen',\n",
       " 'documentary',\n",
       " 'up.',\n",
       " 'begins',\n",
       " 'violence',\n",
       " 'experience',\n",
       " 'yourself',\n",
       " 'surprised',\n",
       " 'sets',\n",
       " 'cool',\n",
       " 'them,',\n",
       " 'clear',\n",
       " 'decided',\n",
       " 'better.',\n",
       " 'genre',\n",
       " 'sit',\n",
       " 'light',\n",
       " 'watch.',\n",
       " 'events',\n",
       " 'now,',\n",
       " 'husband',\n",
       " 'view',\n",
       " 'attention',\n",
       " 'funny,',\n",
       " 'annoying',\n",
       " 'flick',\n",
       " '(as',\n",
       " 'de',\n",
       " 'filmed',\n",
       " \"who's\",\n",
       " 'there.',\n",
       " 'killing',\n",
       " 'learn',\n",
       " '5',\n",
       " '(which',\n",
       " 'hear',\n",
       " 'became',\n",
       " 'elements',\n",
       " 'avoid',\n",
       " 'show.',\n",
       " 'jokes',\n",
       " 'rent',\n",
       " 'word',\n",
       " 'towards',\n",
       " 'you.',\n",
       " 'richard',\n",
       " 'peter',\n",
       " 'character,',\n",
       " 'unfortunately,',\n",
       " 'age',\n",
       " 'straight',\n",
       " 'work.',\n",
       " 'possibly',\n",
       " '1',\n",
       " 'leaves',\n",
       " 'hand',\n",
       " 'talent',\n",
       " 'previous',\n",
       " 'imagine',\n",
       " 'hero',\n",
       " 'famous',\n",
       " 'out,',\n",
       " '(or',\n",
       " 'gore',\n",
       " 'nor',\n",
       " 'cinematography',\n",
       " 'brother',\n",
       " 'stand',\n",
       " 'poorly',\n",
       " 'difficult',\n",
       " 'room',\n",
       " 'forget',\n",
       " 'keeps',\n",
       " 'deal',\n",
       " 'ridiculous',\n",
       " 'sexual',\n",
       " 'means',\n",
       " 'sequence',\n",
       " 'problems',\n",
       " 'alone',\n",
       " 'career',\n",
       " 'reality',\n",
       " 'tom',\n",
       " 'leads',\n",
       " 'various',\n",
       " 'move',\n",
       " 'somehow',\n",
       " 'reading',\n",
       " 'possible',\n",
       " 'gone',\n",
       " 'personal',\n",
       " 'gay',\n",
       " 'forced',\n",
       " '4',\n",
       " 'comments',\n",
       " 'on,',\n",
       " 'unfortunately',\n",
       " 'doubt',\n",
       " 'theme',\n",
       " 'check',\n",
       " 'hilarious',\n",
       " 'television',\n",
       " 'eventually',\n",
       " 'third',\n",
       " 'roles',\n",
       " 'realize',\n",
       " 'meets',\n",
       " 'fairly',\n",
       " 'total',\n",
       " 'level',\n",
       " 'moving',\n",
       " 'brings',\n",
       " 'country',\n",
       " 'god',\n",
       " 'ones',\n",
       " 'team',\n",
       " 'figure',\n",
       " 'emotional',\n",
       " 'review',\n",
       " 'incredibly',\n",
       " 'whom',\n",
       " 'red',\n",
       " 'paul',\n",
       " 'songs',\n",
       " 'scary',\n",
       " 'message',\n",
       " 'interested',\n",
       " 'manages',\n",
       " 'tale',\n",
       " 'enjoyable',\n",
       " 'though,',\n",
       " 'parents',\n",
       " 'unless',\n",
       " 'dialog',\n",
       " 'meet',\n",
       " 'words',\n",
       " 'plenty',\n",
       " 'leading',\n",
       " 'write',\n",
       " 'scenes,',\n",
       " 'etc.',\n",
       " 'male',\n",
       " 'novel',\n",
       " 'viewers',\n",
       " 'then,',\n",
       " 'begin',\n",
       " 'appear',\n",
       " 'say,',\n",
       " 'episodes',\n",
       " 'writer',\n",
       " 'open',\n",
       " 'character.',\n",
       " 'meant',\n",
       " 'feature',\n",
       " 'reviews',\n",
       " 'needed',\n",
       " 'general',\n",
       " 'subject',\n",
       " 'fast',\n",
       " 'dr.',\n",
       " 'worked',\n",
       " 'points',\n",
       " 'up,',\n",
       " \"let's\",\n",
       " 'pay',\n",
       " 'deep',\n",
       " 'average',\n",
       " 'sequel',\n",
       " 'features',\n",
       " '(a',\n",
       " 'plot.',\n",
       " '...',\n",
       " 'future',\n",
       " 'whatever',\n",
       " 'create',\n",
       " 'people.',\n",
       " 'form',\n",
       " 'king',\n",
       " 'effort',\n",
       " 'monster',\n",
       " 'japanese',\n",
       " 'scene,',\n",
       " 'storyline',\n",
       " 'crime',\n",
       " 'unlike',\n",
       " 'front',\n",
       " 'spent',\n",
       " \"'the\",\n",
       " 'hold',\n",
       " 'hardly',\n",
       " 'sounds',\n",
       " 'political',\n",
       " 'particular',\n",
       " 'man,',\n",
       " 'lady',\n",
       " 'expecting',\n",
       " 'older',\n",
       " 'follow',\n",
       " 'premise',\n",
       " 'minute',\n",
       " 'expected',\n",
       " 'caught',\n",
       " 'years.',\n",
       " '20',\n",
       " 'herself',\n",
       " 'crew',\n",
       " 'york',\n",
       " 'battle',\n",
       " 'william',\n",
       " 'dramatic',\n",
       " 'telling',\n",
       " 'be.',\n",
       " 'attempts',\n",
       " 'screen.',\n",
       " 'mystery',\n",
       " 'powerful',\n",
       " 'world.',\n",
       " 'soundtrack',\n",
       " 'made.',\n",
       " 'footage',\n",
       " 'in.',\n",
       " 'crap',\n",
       " 'former',\n",
       " 'sci-fi',\n",
       " 'plain',\n",
       " 'comment',\n",
       " 'space',\n",
       " 'uses',\n",
       " 'western',\n",
       " 'fantastic',\n",
       " 'class',\n",
       " 'decides',\n",
       " 'considering',\n",
       " 'there,',\n",
       " 'sorry',\n",
       " 'sequences',\n",
       " 'theater',\n",
       " '(who',\n",
       " ...]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = vectorizer.get_vocabulary()\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8db41eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '',\n",
       " 1: '[UNK]',\n",
       " 2: 'the',\n",
       " 3: 'a',\n",
       " 4: 'and',\n",
       " 5: 'of',\n",
       " 6: 'to',\n",
       " 7: 'is',\n",
       " 8: 'in',\n",
       " 9: 'i',\n",
       " 10: 'this',\n",
       " 11: 'it',\n",
       " 12: 'that',\n",
       " 13: 'was',\n",
       " 14: 'as',\n",
       " 15: 'with',\n",
       " 16: 'for',\n",
       " 17: 'but',\n",
       " 18: 'on',\n",
       " 19: 'movie',\n",
       " 20: 'are',\n",
       " 21: 'his',\n",
       " 22: 'not',\n",
       " 23: 'you',\n",
       " 24: 'film',\n",
       " 25: 'have',\n",
       " 26: 'be',\n",
       " 27: 'he',\n",
       " 28: 'one',\n",
       " 29: 'at',\n",
       " 30: 'by',\n",
       " 31: 'an',\n",
       " 32: 'they',\n",
       " 33: 'from',\n",
       " 34: 'all',\n",
       " 35: 'who',\n",
       " 36: 'like',\n",
       " 37: 'so',\n",
       " 38: 'just',\n",
       " 39: 'or',\n",
       " 40: 'has',\n",
       " 41: \"it's\",\n",
       " 42: 'about',\n",
       " 43: 'if',\n",
       " 44: 'her',\n",
       " 45: 'some',\n",
       " 46: 'out',\n",
       " 47: 'what',\n",
       " 48: 'there',\n",
       " 49: 'when',\n",
       " 50: 'very',\n",
       " 51: 'more',\n",
       " 52: 'even',\n",
       " 53: 'would',\n",
       " 54: 'my',\n",
       " 55: 'good',\n",
       " 56: 'she',\n",
       " 57: 'their',\n",
       " 58: 'no',\n",
       " 59: 'only',\n",
       " 60: 'really',\n",
       " 61: 'up',\n",
       " 62: 'had',\n",
       " 63: 'can',\n",
       " 64: 'which',\n",
       " 65: 'see',\n",
       " 66: 'were',\n",
       " 67: 'than',\n",
       " 68: 'we',\n",
       " 69: '-',\n",
       " 70: 'been',\n",
       " 71: 'into',\n",
       " 72: 'get',\n",
       " 73: 'will',\n",
       " 74: 'much',\n",
       " 75: 'because',\n",
       " 76: 'most',\n",
       " 77: 'story',\n",
       " 78: 'how',\n",
       " 79: 'other',\n",
       " 80: 'also',\n",
       " 81: \"don't\",\n",
       " 82: 'first',\n",
       " 83: 'do',\n",
       " 84: 'its',\n",
       " 85: 'great',\n",
       " 86: 'time',\n",
       " 87: 'make',\n",
       " 88: 'me',\n",
       " 89: 'people',\n",
       " 90: 'could',\n",
       " 91: 'any',\n",
       " 92: 'after',\n",
       " 93: 'then',\n",
       " 94: 'bad',\n",
       " 95: 'made',\n",
       " 96: 'think',\n",
       " 97: 'being',\n",
       " 98: 'many',\n",
       " 99: 'it.',\n",
       " 100: 'never',\n",
       " 101: 'him',\n",
       " 102: 'two',\n",
       " 103: 'movie.',\n",
       " 104: 'where',\n",
       " 105: 'too',\n",
       " 106: 'little',\n",
       " 107: 'well',\n",
       " 108: 'watch',\n",
       " 109: 'way',\n",
       " 110: 'your',\n",
       " 111: 'them',\n",
       " 112: 'did',\n",
       " 113: 'does',\n",
       " 114: 'best',\n",
       " 115: 'characters',\n",
       " 116: 'love',\n",
       " 117: 'know',\n",
       " 118: 'these',\n",
       " 119: 'character',\n",
       " 120: 'film.',\n",
       " 121: 'movies',\n",
       " 122: 'seen',\n",
       " 123: 'ever',\n",
       " 124: 'while',\n",
       " 125: 'films',\n",
       " 126: 'still',\n",
       " 127: 'over',\n",
       " 128: 'plot',\n",
       " 129: 'should',\n",
       " 130: 'such',\n",
       " 131: 'acting',\n",
       " 132: 'those',\n",
       " 133: 'better',\n",
       " 134: 'off',\n",
       " 135: 'show',\n",
       " 136: 'why',\n",
       " 137: 'go',\n",
       " 138: 'something',\n",
       " 139: \"doesn't\",\n",
       " 140: \"i'm\",\n",
       " 141: 'through',\n",
       " 142: 'say',\n",
       " 143: \"didn't\",\n",
       " 144: 'makes',\n",
       " 145: 'scene',\n",
       " 146: 'real',\n",
       " 147: 'movie,',\n",
       " 148: 'watching',\n",
       " 149: 'back',\n",
       " 150: 'find',\n",
       " 151: 'film,',\n",
       " 152: 'every',\n",
       " 153: 'scenes',\n",
       " 154: 'going',\n",
       " 155: 'actually',\n",
       " 156: 'new',\n",
       " 157: 'few',\n",
       " 158: 'another',\n",
       " 159: 'same',\n",
       " 160: 'man',\n",
       " 161: 'nothing',\n",
       " 162: 'life',\n",
       " 163: 'look',\n",
       " 164: 'lot',\n",
       " 165: '&',\n",
       " 166: 'quite',\n",
       " 167: 'thing',\n",
       " 168: 'want',\n",
       " 169: 'end',\n",
       " 170: 'pretty',\n",
       " 171: 'old',\n",
       " 172: 'seems',\n",
       " 173: \"can't\",\n",
       " 174: 'before',\n",
       " 175: 'got',\n",
       " 176: 'take',\n",
       " 177: 'give',\n",
       " 178: 'part',\n",
       " 179: 'actors',\n",
       " 180: 'years',\n",
       " 181: \"that's\",\n",
       " 182: 'both',\n",
       " 183: 'now',\n",
       " 184: 'between',\n",
       " 185: 'young',\n",
       " 186: \"i've\",\n",
       " 187: 'may',\n",
       " 188: 'things',\n",
       " 189: 'without',\n",
       " 190: 'us',\n",
       " 191: 'thought',\n",
       " 192: 'director',\n",
       " 193: 'though',\n",
       " 194: \"there's\",\n",
       " 195: 'big',\n",
       " 196: 'around',\n",
       " 197: 'must',\n",
       " 198: 'it,',\n",
       " 199: 'saw',\n",
       " 200: 'gets',\n",
       " 201: 'almost',\n",
       " 202: 'here',\n",
       " 203: \"isn't\",\n",
       " 204: 'own',\n",
       " 205: 'whole',\n",
       " 206: 'always',\n",
       " 207: 'horror',\n",
       " 208: 'come',\n",
       " 209: '\"the',\n",
       " 210: \"he's\",\n",
       " 211: 'down',\n",
       " 212: 'work',\n",
       " 213: 'might',\n",
       " 214: 'cast',\n",
       " 215: 'enough',\n",
       " 216: 'bit',\n",
       " 217: 'last',\n",
       " 218: 'since',\n",
       " 219: 'am',\n",
       " 220: 'least',\n",
       " 221: 'probably',\n",
       " 222: 'feel',\n",
       " 223: 'long',\n",
       " 224: 'funny',\n",
       " 225: 'far',\n",
       " 226: 'action',\n",
       " 227: 'each',\n",
       " 228: 'fact',\n",
       " 229: 'rather',\n",
       " 230: 'kind',\n",
       " 231: 'found',\n",
       " 232: 'however,',\n",
       " 233: 'our',\n",
       " 234: 'having',\n",
       " 235: 'original',\n",
       " 236: 'world',\n",
       " 237: 'trying',\n",
       " 238: 'interesting',\n",
       " 239: 'worst',\n",
       " 240: 'guy',\n",
       " 241: 'anything',\n",
       " 242: 'making',\n",
       " 243: 'comes',\n",
       " 244: 'right',\n",
       " 245: 'done',\n",
       " 246: 'believe',\n",
       " 247: 'worth',\n",
       " 248: 'anyone',\n",
       " 249: 'played',\n",
       " 250: 'put',\n",
       " 251: 'main',\n",
       " 252: 'goes',\n",
       " 253: 'point',\n",
       " 254: 'time.',\n",
       " 255: 'yet',\n",
       " 256: 'music',\n",
       " 257: 'hard',\n",
       " 258: 'role',\n",
       " 259: 'especially',\n",
       " 260: 'during',\n",
       " 261: 'looking',\n",
       " 262: 'looks',\n",
       " 263: 'plays',\n",
       " 264: \"wasn't\",\n",
       " 265: 'series',\n",
       " 266: 'although',\n",
       " 267: 'maybe',\n",
       " 268: 'takes',\n",
       " 269: 'three',\n",
       " 270: 'different',\n",
       " 271: 'seem',\n",
       " 272: 'script',\n",
       " 273: 'minutes',\n",
       " 274: 'sure',\n",
       " 275: 'performance',\n",
       " 276: 'tv',\n",
       " 277: 'someone',\n",
       " 278: 'watched',\n",
       " 279: 'shows',\n",
       " 280: 'away',\n",
       " 281: 'family',\n",
       " 282: 'comedy',\n",
       " 283: 'everything',\n",
       " 284: 'set',\n",
       " 285: 'times',\n",
       " 286: 'girl',\n",
       " 287: 'american',\n",
       " 288: 'woman',\n",
       " 289: 'once',\n",
       " 290: 'john',\n",
       " 291: 'simply',\n",
       " 292: 'special',\n",
       " 293: 'left',\n",
       " 294: \"you're\",\n",
       " 295: 'seeing',\n",
       " 296: 'fun',\n",
       " 297: 'well,',\n",
       " 298: 'completely',\n",
       " 299: 'used',\n",
       " 300: 'everyone',\n",
       " 301: 'play',\n",
       " 302: 'again',\n",
       " 303: 'reason',\n",
       " 304: 'until',\n",
       " 305: 'black',\n",
       " 306: 'high',\n",
       " 307: 'true',\n",
       " 308: 'sense',\n",
       " 309: '--',\n",
       " 310: 'need',\n",
       " 311: 'use',\n",
       " 312: 'given',\n",
       " 313: 'idea',\n",
       " 314: 'truly',\n",
       " 315: 'read',\n",
       " 316: 'try',\n",
       " 317: 'place',\n",
       " 318: 'dvd',\n",
       " 319: 'nice',\n",
       " 320: 'help',\n",
       " 321: 'getting',\n",
       " 322: 'second',\n",
       " 323: 'keep',\n",
       " 324: 'ending',\n",
       " 325: 'along',\n",
       " 326: 'came',\n",
       " 327: 'recommend',\n",
       " 328: 'couple',\n",
       " 329: 'poor',\n",
       " 330: 'instead',\n",
       " 331: 'let',\n",
       " 332: 'money',\n",
       " 333: 'less',\n",
       " 334: 'beautiful',\n",
       " 335: 'effects',\n",
       " 336: 'excellent',\n",
       " 337: 'rest',\n",
       " 338: 'job',\n",
       " 339: 'full',\n",
       " 340: 'half',\n",
       " 341: 'shot',\n",
       " 342: 'said',\n",
       " 343: 'tell',\n",
       " 344: 'gives',\n",
       " 345: 'actor',\n",
       " 346: 'definitely',\n",
       " 347: 'day',\n",
       " 348: 'enjoy',\n",
       " 349: 'them.',\n",
       " 350: 'next',\n",
       " 351: '(and',\n",
       " 352: 'went',\n",
       " 353: 'playing',\n",
       " 354: 'understand',\n",
       " 355: \"couldn't\",\n",
       " 356: 'audience',\n",
       " 357: 'version',\n",
       " 358: 'perhaps',\n",
       " 359: 'war',\n",
       " 360: '2',\n",
       " 361: 'start',\n",
       " 362: 'together',\n",
       " 363: 'himself',\n",
       " 364: 'fan',\n",
       " 365: 'screen',\n",
       " 366: 'absolutely',\n",
       " 367: 'early',\n",
       " 368: 'become',\n",
       " 369: 'certainly',\n",
       " 370: 'small',\n",
       " 371: 'supposed',\n",
       " 372: 'several',\n",
       " 373: 'entire',\n",
       " 374: 'doing',\n",
       " 375: 'liked',\n",
       " 376: 'later',\n",
       " 377: 'hollywood',\n",
       " 378: 'remember',\n",
       " 379: 'all,',\n",
       " 380: 'book',\n",
       " 381: 'one.',\n",
       " 382: 'against',\n",
       " 383: 'often',\n",
       " 384: 'men',\n",
       " 385: 'star',\n",
       " 386: 'time,',\n",
       " 387: '10',\n",
       " 388: 'wife',\n",
       " 389: 'felt',\n",
       " 390: 'sort',\n",
       " 391: 'short',\n",
       " 392: 'seemed',\n",
       " 393: 'totally',\n",
       " 394: 'night',\n",
       " 395: '(the',\n",
       " 396: 'human',\n",
       " 397: 'him.',\n",
       " 398: 'is,',\n",
       " 399: 'hope',\n",
       " 400: 'year',\n",
       " 401: 'based',\n",
       " 402: '.',\n",
       " 403: 'camera',\n",
       " 404: 'kids',\n",
       " 405: 'wonderful',\n",
       " 406: 'waste',\n",
       " 407: 'loved',\n",
       " 408: 'becomes',\n",
       " 409: 'this.',\n",
       " 410: 'final',\n",
       " 411: 'despite',\n",
       " 412: 'production',\n",
       " 413: 'women',\n",
       " 414: 'top',\n",
       " 415: 'else',\n",
       " 416: 'performances',\n",
       " 417: 'death',\n",
       " 418: \"she's\",\n",
       " 419: 'able',\n",
       " 420: \"you'll\",\n",
       " 421: 'well.',\n",
       " 422: 'wanted',\n",
       " 423: 'piece',\n",
       " 424: 'home',\n",
       " 425: 'live',\n",
       " 426: 'lost',\n",
       " 427: 'sex',\n",
       " 428: 'that,',\n",
       " 429: 'classic',\n",
       " 430: 'course',\n",
       " 431: 'story.',\n",
       " 432: 'friends',\n",
       " 433: 'line',\n",
       " 434: 'video',\n",
       " 435: 'me.',\n",
       " 436: 'called',\n",
       " 437: 'under',\n",
       " 438: \"i'd\",\n",
       " 439: 'house',\n",
       " 440: 'dead',\n",
       " 441: 'tries',\n",
       " 442: 'wants',\n",
       " 443: 'school',\n",
       " 444: 'mind',\n",
       " 445: 'person',\n",
       " 446: '\\x96',\n",
       " 447: 'name',\n",
       " 448: 'lead',\n",
       " 449: 'father',\n",
       " 450: 'all.',\n",
       " 451: \"they're\",\n",
       " 452: 'low',\n",
       " 453: 'stupid',\n",
       " 454: 'gave',\n",
       " 455: \"won't\",\n",
       " 456: 'story,',\n",
       " 457: 'turn',\n",
       " 458: 'this,',\n",
       " 459: 'perfect',\n",
       " 460: 'already',\n",
       " 461: 'care',\n",
       " 462: 'enjoyed',\n",
       " 463: 'finally',\n",
       " 464: 'either',\n",
       " 465: 'starts',\n",
       " 466: 'problem',\n",
       " 467: 'budget',\n",
       " 468: 'title',\n",
       " 469: 'turns',\n",
       " 470: 'sound',\n",
       " 471: 'moments',\n",
       " 472: 'written',\n",
       " 473: 'guess',\n",
       " 474: 'face',\n",
       " 475: 'good.',\n",
       " 476: 'lines',\n",
       " 477: 'mean',\n",
       " 478: 'life.',\n",
       " 479: 'took',\n",
       " 480: 'and,',\n",
       " 481: 'head',\n",
       " 482: 'favorite',\n",
       " 483: 'kill',\n",
       " 484: 'boring',\n",
       " 485: 'extremely',\n",
       " 486: 'me,',\n",
       " 487: 'itself',\n",
       " 488: 'episode',\n",
       " 489: 'behind',\n",
       " 490: 'terrible',\n",
       " 491: 'dialogue',\n",
       " 492: 'highly',\n",
       " 493: 'guys',\n",
       " 494: 'mother',\n",
       " 495: 'others',\n",
       " 496: 'movies.',\n",
       " 497: 'fans',\n",
       " 498: 'friend',\n",
       " 499: 'looked',\n",
       " 500: 'group',\n",
       " 501: 'dark',\n",
       " 502: 'beginning',\n",
       " 503: \"wouldn't\",\n",
       " 504: 'throughout',\n",
       " 505: 'obviously',\n",
       " 506: 'boy',\n",
       " 507: 'expect',\n",
       " 508: 'fine',\n",
       " 509: 'stars',\n",
       " 510: 'decent',\n",
       " 511: 'michael',\n",
       " 512: 'style',\n",
       " 513: 'white',\n",
       " 514: 'evil',\n",
       " 515: 'taken',\n",
       " 516: 'save',\n",
       " 517: 'cannot',\n",
       " 518: 'heard',\n",
       " 519: 'lives',\n",
       " 520: 'sometimes',\n",
       " 521: '3',\n",
       " 522: 'directed',\n",
       " 523: 'run',\n",
       " 524: 'mr.',\n",
       " 525: \"film's\",\n",
       " 526: 'attempt',\n",
       " 527: 'lack',\n",
       " 528: 'wrong',\n",
       " 529: 'good,',\n",
       " 530: 'case',\n",
       " 531: 'laugh',\n",
       " 532: 'entertaining',\n",
       " 533: 'fight',\n",
       " 534: 'that.',\n",
       " 535: 'leave',\n",
       " 536: 'awful',\n",
       " 537: 'feeling',\n",
       " 538: 'quality',\n",
       " 539: 'picture',\n",
       " 540: 'bad.',\n",
       " 541: 'late',\n",
       " 542: 'killed',\n",
       " 543: 'across',\n",
       " 544: 'soon',\n",
       " 545: 'exactly',\n",
       " 546: 'complete',\n",
       " 547: 'here.',\n",
       " 548: 'type',\n",
       " 549: 'out.',\n",
       " 550: 'except',\n",
       " 551: 'movies,',\n",
       " 552: 'particularly',\n",
       " 553: 'wonder',\n",
       " 554: 'way.',\n",
       " 555: 'stop',\n",
       " 556: 'coming',\n",
       " 557: 'known',\n",
       " 558: 'course,',\n",
       " 559: 'direction',\n",
       " 560: 'works',\n",
       " 561: 'living',\n",
       " 562: 'thinking',\n",
       " 563: 'finds',\n",
       " 564: 'game',\n",
       " 565: 'told',\n",
       " 566: 'says',\n",
       " 567: 'close',\n",
       " 568: 'whose',\n",
       " 569: 'robert',\n",
       " 570: 'act',\n",
       " 571: 'killer',\n",
       " 572: 'usually',\n",
       " 573: 'police',\n",
       " 574: 'car',\n",
       " 575: 'taking',\n",
       " 576: 'parts',\n",
       " 577: 'turned',\n",
       " 578: 'opening',\n",
       " 579: 'viewer',\n",
       " 580: 'past',\n",
       " 581: 'end.',\n",
       " 582: 'again.',\n",
       " 583: 'somewhat',\n",
       " 584: 'worse',\n",
       " 585: 'running',\n",
       " 586: ',',\n",
       " 587: 'writing',\n",
       " 588: 'david',\n",
       " 589: 'films,',\n",
       " 590: 'side',\n",
       " 591: 'obvious',\n",
       " 592: 'wish',\n",
       " 593: 'son',\n",
       " 594: 'huge',\n",
       " 595: 'is.',\n",
       " 596: 'acting,',\n",
       " 597: 'seen.',\n",
       " 598: 'amazing',\n",
       " 599: 'major',\n",
       " 600: 'none',\n",
       " 601: 'yes,',\n",
       " 602: 'strong',\n",
       " 603: 'call',\n",
       " 604: 'films.',\n",
       " 605: \"i'll\",\n",
       " 606: 'female',\n",
       " 607: 'james',\n",
       " 608: 'humor',\n",
       " 609: 'shown',\n",
       " 610: 'matter',\n",
       " 611: 'number',\n",
       " 612: 'children',\n",
       " 613: 'horrible',\n",
       " 614: 'knew',\n",
       " 615: 'here,',\n",
       " 616: 'happens',\n",
       " 617: 'again,',\n",
       " 618: 'myself',\n",
       " 619: 'including',\n",
       " 620: 'town',\n",
       " 621: 'tells',\n",
       " 622: 'brilliant',\n",
       " 623: 'beyond',\n",
       " 624: 'local',\n",
       " 625: 'fact,',\n",
       " 626: 'single',\n",
       " 627: 'so,',\n",
       " 628: 'overall',\n",
       " 629: 'hour',\n",
       " 630: 'characters,',\n",
       " 631: 'due',\n",
       " 632: 'started',\n",
       " 633: 'mostly',\n",
       " 634: 'order',\n",
       " 635: 'however',\n",
       " 636: 'clearly',\n",
       " 637: 'but,',\n",
       " 638: 'girls',\n",
       " 639: 'drama',\n",
       " 640: 'important',\n",
       " 641: 'bring',\n",
       " 642: 'themselves',\n",
       " 643: 'way,',\n",
       " 644: 'involved',\n",
       " 645: 'serious',\n",
       " 646: 'giving',\n",
       " 647: 'history',\n",
       " 648: \"aren't\",\n",
       " 649: 'british',\n",
       " 650: 'her.',\n",
       " 651: 'end,',\n",
       " 652: 'saying',\n",
       " 653: 'one,',\n",
       " 654: 'upon',\n",
       " 655: 'ends',\n",
       " 656: 'on.',\n",
       " 657: 'lots',\n",
       " 658: 'cinema',\n",
       " 659: 'supporting',\n",
       " 660: 'relationship',\n",
       " 661: 'bad,',\n",
       " 662: 'cut',\n",
       " 663: 'kid',\n",
       " 664: 'hit',\n",
       " 665: 'stories',\n",
       " 666: 'city',\n",
       " 667: \"haven't\",\n",
       " 668: 'four',\n",
       " 669: 'happened',\n",
       " 670: 'strange',\n",
       " 671: 'certain',\n",
       " 672: 'add',\n",
       " 673: 'too.',\n",
       " 674: 'days',\n",
       " 675: 'heart',\n",
       " 676: 'basically',\n",
       " 677: 'talking',\n",
       " 678: 'falls',\n",
       " 679: 'actress',\n",
       " 680: 'whether',\n",
       " 681: 'funny.',\n",
       " 682: 'chance',\n",
       " 683: 'appears',\n",
       " 684: 'child',\n",
       " 685: 'modern',\n",
       " 686: 'knows',\n",
       " 687: 'miss',\n",
       " 688: 'named',\n",
       " 689: 'among',\n",
       " 690: 'eyes',\n",
       " 691: 'change',\n",
       " 692: 'easily',\n",
       " 693: 'kept',\n",
       " 694: 'apparently',\n",
       " 695: 'within',\n",
       " 696: 'moment',\n",
       " 697: 'released',\n",
       " 698: 'voice',\n",
       " 699: 'similar',\n",
       " 700: 'him,',\n",
       " 701: 'french',\n",
       " 702: 'simple',\n",
       " 703: 'typical',\n",
       " 704: 'daughter',\n",
       " 705: 'stuff',\n",
       " 706: 'score',\n",
       " 707: 'using',\n",
       " 708: 'comic',\n",
       " 709: 'bunch',\n",
       " 710: 'near',\n",
       " 711: 'blood',\n",
       " 712: 'mention',\n",
       " 713: 'brought',\n",
       " 714: 'characters.',\n",
       " 715: 'nearly',\n",
       " 716: 'feels',\n",
       " 717: 'jack',\n",
       " 718: \"what's\",\n",
       " 719: 'interest',\n",
       " 720: 'plot,',\n",
       " 721: 'showing',\n",
       " 722: 'stay',\n",
       " 723: 'english',\n",
       " 724: 'talk',\n",
       " 725: 'hours',\n",
       " 726: 'middle',\n",
       " 727: 'art',\n",
       " 728: 'hate',\n",
       " 729: 'tried',\n",
       " 730: 'needs',\n",
       " 731: 'fall',\n",
       " 732: 'also,',\n",
       " 733: 'murder',\n",
       " 734: 'usual',\n",
       " 735: 'working',\n",
       " 736: 'romantic',\n",
       " 737: 'slow',\n",
       " 738: 'life,',\n",
       " 739: 'example',\n",
       " 740: 'oh',\n",
       " 741: 'sad',\n",
       " 742: 'george',\n",
       " 743: 'cheap',\n",
       " 744: 'happy',\n",
       " 745: 'song',\n",
       " 746: 'five',\n",
       " 747: 'ten',\n",
       " 748: 'power',\n",
       " 749: 'actual',\n",
       " 750: 'buy',\n",
       " 751: '(i',\n",
       " 752: 'shots',\n",
       " 753: 'please',\n",
       " 754: 'musical',\n",
       " 755: \"you've\",\n",
       " 756: 'body',\n",
       " 757: 'hell',\n",
       " 758: 'greatest',\n",
       " 759: 'silly',\n",
       " 760: 'easy',\n",
       " 761: 'above',\n",
       " 762: 'happen',\n",
       " 763: 'documentary',\n",
       " 764: 'up.',\n",
       " 765: 'begins',\n",
       " 766: 'violence',\n",
       " 767: 'experience',\n",
       " 768: 'yourself',\n",
       " 769: 'surprised',\n",
       " 770: 'sets',\n",
       " 771: 'cool',\n",
       " 772: 'them,',\n",
       " 773: 'clear',\n",
       " 774: 'decided',\n",
       " 775: 'better.',\n",
       " 776: 'genre',\n",
       " 777: 'sit',\n",
       " 778: 'light',\n",
       " 779: 'watch.',\n",
       " 780: 'events',\n",
       " 781: 'now,',\n",
       " 782: 'husband',\n",
       " 783: 'view',\n",
       " 784: 'attention',\n",
       " 785: 'funny,',\n",
       " 786: 'annoying',\n",
       " 787: 'flick',\n",
       " 788: '(as',\n",
       " 789: 'de',\n",
       " 790: 'filmed',\n",
       " 791: \"who's\",\n",
       " 792: 'there.',\n",
       " 793: 'killing',\n",
       " 794: 'learn',\n",
       " 795: '5',\n",
       " 796: '(which',\n",
       " 797: 'hear',\n",
       " 798: 'became',\n",
       " 799: 'elements',\n",
       " 800: 'avoid',\n",
       " 801: 'show.',\n",
       " 802: 'jokes',\n",
       " 803: 'rent',\n",
       " 804: 'word',\n",
       " 805: 'towards',\n",
       " 806: 'you.',\n",
       " 807: 'richard',\n",
       " 808: 'peter',\n",
       " 809: 'character,',\n",
       " 810: 'unfortunately,',\n",
       " 811: 'age',\n",
       " 812: 'straight',\n",
       " 813: 'work.',\n",
       " 814: 'possibly',\n",
       " 815: '1',\n",
       " 816: 'leaves',\n",
       " 817: 'hand',\n",
       " 818: 'talent',\n",
       " 819: 'previous',\n",
       " 820: 'imagine',\n",
       " 821: 'hero',\n",
       " 822: 'famous',\n",
       " 823: 'out,',\n",
       " 824: '(or',\n",
       " 825: 'gore',\n",
       " 826: 'nor',\n",
       " 827: 'cinematography',\n",
       " 828: 'brother',\n",
       " 829: 'stand',\n",
       " 830: 'poorly',\n",
       " 831: 'difficult',\n",
       " 832: 'room',\n",
       " 833: 'forget',\n",
       " 834: 'keeps',\n",
       " 835: 'deal',\n",
       " 836: 'ridiculous',\n",
       " 837: 'sexual',\n",
       " 838: 'means',\n",
       " 839: 'sequence',\n",
       " 840: 'problems',\n",
       " 841: 'alone',\n",
       " 842: 'career',\n",
       " 843: 'reality',\n",
       " 844: 'tom',\n",
       " 845: 'leads',\n",
       " 846: 'various',\n",
       " 847: 'move',\n",
       " 848: 'somehow',\n",
       " 849: 'reading',\n",
       " 850: 'possible',\n",
       " 851: 'gone',\n",
       " 852: 'personal',\n",
       " 853: 'gay',\n",
       " 854: 'forced',\n",
       " 855: '4',\n",
       " 856: 'comments',\n",
       " 857: 'on,',\n",
       " 858: 'unfortunately',\n",
       " 859: 'doubt',\n",
       " 860: 'theme',\n",
       " 861: 'check',\n",
       " 862: 'hilarious',\n",
       " 863: 'television',\n",
       " 864: 'eventually',\n",
       " 865: 'third',\n",
       " 866: 'roles',\n",
       " 867: 'realize',\n",
       " 868: 'meets',\n",
       " 869: 'fairly',\n",
       " 870: 'total',\n",
       " 871: 'level',\n",
       " 872: 'moving',\n",
       " 873: 'brings',\n",
       " 874: 'country',\n",
       " 875: 'god',\n",
       " 876: 'ones',\n",
       " 877: 'team',\n",
       " 878: 'figure',\n",
       " 879: 'emotional',\n",
       " 880: 'review',\n",
       " 881: 'incredibly',\n",
       " 882: 'whom',\n",
       " 883: 'red',\n",
       " 884: 'paul',\n",
       " 885: 'songs',\n",
       " 886: 'scary',\n",
       " 887: 'message',\n",
       " 888: 'interested',\n",
       " 889: 'manages',\n",
       " 890: 'tale',\n",
       " 891: 'enjoyable',\n",
       " 892: 'though,',\n",
       " 893: 'parents',\n",
       " 894: 'unless',\n",
       " 895: 'dialog',\n",
       " 896: 'meet',\n",
       " 897: 'words',\n",
       " 898: 'plenty',\n",
       " 899: 'leading',\n",
       " 900: 'write',\n",
       " 901: 'scenes,',\n",
       " 902: 'etc.',\n",
       " 903: 'male',\n",
       " 904: 'novel',\n",
       " 905: 'viewers',\n",
       " 906: 'then,',\n",
       " 907: 'begin',\n",
       " 908: 'appear',\n",
       " 909: 'say,',\n",
       " 910: 'episodes',\n",
       " 911: 'writer',\n",
       " 912: 'open',\n",
       " 913: 'character.',\n",
       " 914: 'meant',\n",
       " 915: 'feature',\n",
       " 916: 'reviews',\n",
       " 917: 'needed',\n",
       " 918: 'general',\n",
       " 919: 'subject',\n",
       " 920: 'fast',\n",
       " 921: 'dr.',\n",
       " 922: 'worked',\n",
       " 923: 'points',\n",
       " 924: 'up,',\n",
       " 925: \"let's\",\n",
       " 926: 'pay',\n",
       " 927: 'deep',\n",
       " 928: 'average',\n",
       " 929: 'sequel',\n",
       " 930: 'features',\n",
       " 931: '(a',\n",
       " 932: 'plot.',\n",
       " 933: '...',\n",
       " 934: 'future',\n",
       " 935: 'whatever',\n",
       " 936: 'create',\n",
       " 937: 'people.',\n",
       " 938: 'form',\n",
       " 939: 'king',\n",
       " 940: 'effort',\n",
       " 941: 'monster',\n",
       " 942: 'japanese',\n",
       " 943: 'scene,',\n",
       " 944: 'storyline',\n",
       " 945: 'crime',\n",
       " 946: 'unlike',\n",
       " 947: 'front',\n",
       " 948: 'spent',\n",
       " 949: \"'the\",\n",
       " 950: 'hold',\n",
       " 951: 'hardly',\n",
       " 952: 'sounds',\n",
       " 953: 'political',\n",
       " 954: 'particular',\n",
       " 955: 'man,',\n",
       " 956: 'lady',\n",
       " 957: 'expecting',\n",
       " 958: 'older',\n",
       " 959: 'follow',\n",
       " 960: 'premise',\n",
       " 961: 'minute',\n",
       " 962: 'expected',\n",
       " 963: 'caught',\n",
       " 964: 'years.',\n",
       " 965: '20',\n",
       " 966: 'herself',\n",
       " 967: 'crew',\n",
       " 968: 'york',\n",
       " 969: 'battle',\n",
       " 970: 'william',\n",
       " 971: 'dramatic',\n",
       " 972: 'telling',\n",
       " 973: 'be.',\n",
       " 974: 'attempts',\n",
       " 975: 'screen.',\n",
       " 976: 'mystery',\n",
       " 977: 'powerful',\n",
       " 978: 'world.',\n",
       " 979: 'soundtrack',\n",
       " 980: 'made.',\n",
       " 981: 'footage',\n",
       " 982: 'in.',\n",
       " 983: 'crap',\n",
       " 984: 'former',\n",
       " 985: 'sci-fi',\n",
       " 986: 'plain',\n",
       " 987: 'comment',\n",
       " 988: 'space',\n",
       " 989: 'uses',\n",
       " 990: 'western',\n",
       " 991: 'fantastic',\n",
       " 992: 'class',\n",
       " 993: 'decides',\n",
       " 994: 'considering',\n",
       " 995: 'there,',\n",
       " 996: 'sorry',\n",
       " 997: 'sequences',\n",
       " 998: 'theater',\n",
       " 999: '(who',\n",
       " ...}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_index = {i:word for i,word in enumerate(vocab)}\n",
    "vocab_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "52ca584d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the dataset\n",
    "# Inputs: Each sequence except the last token\n",
    "# Targets: Same sequence offset by one token\n",
    "\n",
    "def vectorize_dataset(batch_reviews):\n",
    "    vectorized_reviews = vectorizer(batch_reviews)\n",
    "    inputs = vectorized_reviews[:,:-1] # inputs are created by removing last token\n",
    "    targets = vectorized_reviews[:, 1:] # targets are created by offsetting by one token\n",
    "    return inputs, targets\n",
    "\n",
    "vectorized_imdb = imdb.map(vectorize_dataset, num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1b69ed1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs shape: (256, 99)\n",
      "Targets shape: (256, 99)\n"
     ]
    }
   ],
   "source": [
    "# Display a sample\n",
    "for inputs, targets in vectorized_imdb.take(1):\n",
    "    print(f\"Inputs shape: {inputs.shape}\")\n",
    "    print(f\"Targets shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "33790464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: [   10    13    31   366   490   103    81    26 12813     8    30  1604\n",
      "  6141    39   511     1   182    20    85  1096    17    10   197   291\n",
      "    26    57   239   258     8  1821    52    57    85   131    90    22\n",
      "  6042    10  1232   836  3796    10    19     7    31   367     1   190\n",
      "  3026  4998     2    76  1558   153    66   132    49     2     1 10442\n",
      "    66   242    57  4453    16     1  3630     1     1  1587     1     4\n",
      "    44     1  2033    15  6141    13   161    17     3  1558   879 12896\n",
      "     8     3    19    12    13  3923     5    91   146  7622     9   219\n",
      "  1038    12    48]\n",
      "Targets: [   13    31   366   490   103    81    26 12813     8    30  1604  6141\n",
      "    39   511     1   182    20    85  1096    17    10   197   291    26\n",
      "    57   239   258     8  1821    52    57    85   131    90    22  6042\n",
      "    10  1232   836  3796    10    19     7    31   367     1   190  3026\n",
      "  4998     2    76  1558   153    66   132    49     2     1 10442    66\n",
      "   242    57  4453    16     1  3630     1     1  1587     1     4    44\n",
      "     1  2033    15  6141    13   161    17     3  1558   879 12896     8\n",
      "     3    19    12    13  3923     5    91   146  7622     9   219  1038\n",
      "    12    48    20]\n"
     ]
    }
   ],
   "source": [
    "# Display a sample\n",
    "for inputs, targets in vectorized_imdb.take(1):\n",
    "    print(f\"Inputs: {inputs[0]}\")\n",
    "    print(f\"Targets: {targets[0]}\")\n",
    "    \n",
    "# As you see inputs is nothing but the actual sequence except the last token\n",
    "# targets is also the same sequence but offset by one token and "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71ead1b",
   "metadata": {},
   "source": [
    "### Build a Transformer based decoder only model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d21088",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"./images/decoder_only2.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "64fb561e",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9686ec82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These were the constants Defined earlier\n",
    "# VOCAB_SIZE = 15000 # Max tokens\n",
    "# MAX_SEQ_LEN = 100 # Max sequence length\n",
    "# EMBED_DIM = 128 # Embedding dimension\n",
    "# HIDDEN_DIM = 1024 # Hidden dimension for dense layers\n",
    "# BATCH_SIZE= 256 # Batch size\n",
    "# NUM_HEADS = 2 # Number of heads for Multiheaded attention layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b941a85d",
   "metadata": {},
   "source": [
    "#### Embeddings class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3928412a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedEmbedding(layers.Layer):\n",
    "    def __init__(self, MAX_SEQ_LEN, VOCAB_SIZE, EMBED_DIM, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.token_embeddings = layers.Embedding(input_dim = VOCAB_SIZE, output_dim=EMBED_DIM, \n",
    "                                                 mask_zero=True) # input embedding layer\n",
    "        \n",
    "        self.position_embeddings = layers.Embedding(input_dim = MAX_SEQ_LEN, \n",
    "                                                    output_dim = EMBED_DIM) # position embedding layer\n",
    "        # both the above embeddings are initialized randomly first \n",
    "        #....and will be calculated as part of training process.\n",
    "        \n",
    "        self.sequence_length = MAX_SEQ_LEN\n",
    "        self.max_tokens = VOCAB_SIZE\n",
    "        self.embed_dim = EMBED_DIM\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        \n",
    "        # Word or token embeddings\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        \n",
    "        # Positional Embeddings\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        \n",
    "        return embedded_tokens + embedded_positions # Return combined embeddings\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return self.token_embeddings.compute_mask(inputs, mask)\n",
    "    # The compute_mask method in a custom layer ensures that the masking information \n",
    "    #...is correctly propagated through the layers.\n",
    "    # without this the mask may not be propagated properly through the sebsequent layers.\n",
    "\n",
    "    # whenever we use custom layers, mainly for saving and loading the model\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"max_tokens\": self.max_tokens,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f554deab",
   "metadata": {},
   "source": [
    "#### Transformer Decoder class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba48437",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"./images/decoder_only2.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d69899b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, EMBED_DIM, NUM_HEADS, HIDDEN_DIM, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = EMBED_DIM\n",
    "        self.num_heads = NUM_HEADS\n",
    "        self.ff_dim = HIDDEN_DIM\n",
    "        \n",
    "        #Attention Layers\n",
    "        self.attention1 = layers.MultiHeadAttention(num_heads=NUM_HEADS, key_dim=EMBED_DIM)\n",
    "        self.attention2 = layers.MultiHeadAttention(num_heads=NUM_HEADS, key_dim=EMBED_DIM)\n",
    "        \n",
    "        #Feedforward Dense Layer\n",
    "        self.ffn = keras.Sequential([\n",
    "            layers.Dense(HIDDEN_DIM, activation=\"relu\"),\n",
    "            layers.Dense(EMBED_DIM),])\n",
    "        \n",
    "        #Normalization\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.supports_masking=True\n",
    "        \n",
    "\n",
    "    # Causal mask for the Decoder Inputs. because we dont want attention on future tokens\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1),\n",
    "             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
    "        return tf.tile(mask, mult)\n",
    "\n",
    "# 1 0 0 0 0\n",
    "# 1 1 0 0 0\n",
    "# 1 1 1 0 0\n",
    "# 1 1 1 1 0\n",
    "# 1 1 1 1 1\n",
    "\n",
    "    # Main computation inside the call method\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        \n",
    "        # Causal mask for decoder inputs: self attention\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        \n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(\n",
    "                mask[:, tf.newaxis, :], dtype=\"int32\")\n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask) # combined both masks\n",
    "        else:\n",
    "            padding_mask = mask\n",
    "            \n",
    "        # Self Attention for Decoder inputs\n",
    "        attention_output1 = self.attention1(query=inputs,\n",
    "                                            value=inputs,\n",
    "                                            key=inputs,\n",
    "                                            attention_mask=causal_mask)\n",
    "        out1 = self.layernorm1(inputs + attention_output1)\n",
    "        \n",
    "        # 2nd attention layer\n",
    "        attention_output2 = self.attention2(query=out1,\n",
    "                                            value=encoder_outputs,\n",
    "                                            key=encoder_outputs,\n",
    "                                            attention_mask=padding_mask)\n",
    "        out2 = self.layernorm2(out1 + attention_output2)\n",
    "        \n",
    "        # Feed forward dense layers\n",
    "        ffn_output = self.ffn(out2)\n",
    "        \n",
    "        out3 = self.layernorm3(out2 + ffn_output)\n",
    "        \n",
    "        return out3\n",
    "\n",
    "    #for loading the saved model, with custom layers.    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"ff_dim\": self.ff_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f010a7",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"./images/decoder_only2.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec2fd93",
   "metadata": {},
   "source": [
    "**Decoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "21d9d385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the decoder now\n",
    "\n",
    "# Inputs\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
    "\n",
    "# Embedding layer (combined word embeddings + positional embeddings)\n",
    "x = CombinedEmbedding(MAX_SEQ_LEN, VOCAB_SIZE, EMBED_DIM)(inputs)\n",
    "\n",
    "# Transformer Decoder Block\n",
    "x = TransformerDecoder(EMBED_DIM, NUM_HEADS, HIDDEN_DIM)(x,x)\n",
    "\n",
    "# Dropout Layer\n",
    "x = layers.Dropout(0.4)(x)\n",
    "\n",
    "# Final dense layer mapping probability distribution over spanish vocabulary\n",
    "outputs= layers.Dense(VOCAB_SIZE, activation=\"softmax\")(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1e695468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " combined_embedding_1 (Combined  (None, None, 256)   3865600     ['decoder_inputs[0][0]']         \n",
      " Embedding)                                                                                       \n",
      "                                                                                                  \n",
      " transformer_decoder_1 (Transfo  (None, None, 256)   2104576     ['combined_embedding_1[0][0]',   \n",
      " rmerDecoder)                                                     'combined_embedding_1[0][0]']   \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, None, 256)    0           ['transformer_decoder_1[0][0]']  \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, None, 15000)  3855000     ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 9,825,176\n",
      "Trainable params: 9,825,176\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_decoderonly = keras.Model(inputs,outputs)\n",
    "model_decoderonly.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc21e39b",
   "metadata": {},
   "source": [
    "### Compile & Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a9f657ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"./models/model_textgen.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "080b2b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will reuse this function to train and evaluate for convenience\n",
    "def train(model,path,data):\n",
    "    \n",
    "    #call backs\n",
    "   \n",
    "    earlystop_cb = keras.callbacks.EarlyStopping(monitor='loss', patience=3, restore_best_weights=True)\n",
    "    callbacks = [earlystop_cb]\n",
    "\n",
    "    #Compile the model\n",
    "    model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\")\n",
    "    \n",
    "    #Train the model\n",
    "    history = model.fit(data, callbacks = callbacks, epochs=200)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e061d2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "391/391 [==============================] - 83s 204ms/step - loss: 5.3249\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 80s 203ms/step - loss: 4.9009\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 80s 204ms/step - loss: 4.7191\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 76s 194ms/step - loss: 4.6257\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 77s 198ms/step - loss: 4.5635\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 78s 201ms/step - loss: 4.5187\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 78s 200ms/step - loss: 4.4832\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 77s 196ms/step - loss: 4.4529\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 74s 189ms/step - loss: 4.4278\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 75s 192ms/step - loss: 4.4041\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 75s 191ms/step - loss: 4.3842\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 64590s 166s/step - loss: 4.3666\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 71s 182ms/step - loss: 4.3503\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 79s 201ms/step - loss: 4.3353\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 77s 196ms/step - loss: 4.3208\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 80s 203ms/step - loss: 4.3082\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 74s 188ms/step - loss: 4.2968\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 72s 185ms/step - loss: 4.2863\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 79s 203ms/step - loss: 4.2759\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 81s 206ms/step - loss: 4.2672\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 82s 210ms/step - loss: 4.2587\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 81s 208ms/step - loss: 4.2501\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 81s 207ms/step - loss: 4.2417\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 78s 199ms/step - loss: 4.2330\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 82s 210ms/step - loss: 4.2243\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 76s 194ms/step - loss: 4.2162\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 81s 207ms/step - loss: 4.2093\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 80s 205ms/step - loss: 4.2029\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 79s 201ms/step - loss: 4.1964\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 80s 204ms/step - loss: 4.1902\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 82s 210ms/step - loss: 4.1848\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 82s 210ms/step - loss: 4.1801\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 80s 203ms/step - loss: 4.1756\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 74s 188ms/step - loss: 4.1702\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 73s 185ms/step - loss: 4.1654\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 81s 208ms/step - loss: 4.1613\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 80s 205ms/step - loss: 4.1568\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 79s 202ms/step - loss: 4.1525\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 78s 199ms/step - loss: 4.1480\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 78s 199ms/step - loss: 4.1436\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 79s 203ms/step - loss: 4.1390\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 80s 204ms/step - loss: 4.1353\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 81s 208ms/step - loss: 4.1311\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 81s 208ms/step - loss: 4.1273\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 80s 206ms/step - loss: 4.1236\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 78s 200ms/step - loss: 4.1198\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 78s 200ms/step - loss: 4.1167\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 78s 200ms/step - loss: 4.1136\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 81s 208ms/step - loss: 4.1103\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 77s 198ms/step - loss: 4.1065\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 79s 201ms/step - loss: 4.1031\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 77s 196ms/step - loss: 4.1002\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 79s 201ms/step - loss: 4.0976\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 78s 199ms/step - loss: 4.0949\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 79s 201ms/step - loss: 4.0914\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 81s 206ms/step - loss: 4.0891\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 78s 199ms/step - loss: 4.0866\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 81s 208ms/step - loss: 4.0841\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 79s 202ms/step - loss: 4.0819\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 82s 210ms/step - loss: 4.0795\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 78s 200ms/step - loss: 4.0773\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 80s 205ms/step - loss: 4.0752\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 80s 203ms/step - loss: 4.0732\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 81s 207ms/step - loss: 4.0707\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 82s 209ms/step - loss: 4.0684\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 83s 212ms/step - loss: 4.0665\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 77s 197ms/step - loss: 4.0646\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 77s 197ms/step - loss: 4.0629\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 81s 207ms/step - loss: 4.0610\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 81s 206ms/step - loss: 4.0590\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 84s 214ms/step - loss: 4.0573\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 81s 207ms/step - loss: 4.0552\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 82s 210ms/step - loss: 4.0541\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 78s 200ms/step - loss: 4.0513\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 80s 204ms/step - loss: 4.0501\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 83s 211ms/step - loss: 4.0483\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 81s 208ms/step - loss: 4.0459\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 75s 191ms/step - loss: 4.0445\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 74s 190ms/step - loss: 4.0429\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 76s 193ms/step - loss: 4.0405\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 75s 193ms/step - loss: 4.0391\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 75s 192ms/step - loss: 4.0376\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 81116s 208s/step - loss: 4.0362\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 73s 187ms/step - loss: 4.0344\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 78s 201ms/step - loss: 4.0325\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 74s 190ms/step - loss: 4.0311\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 75s 191ms/step - loss: 4.0292\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 76s 194ms/step - loss: 4.0277\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 74s 190ms/step - loss: 4.0262\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 77s 197ms/step - loss: 4.0251\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 77s 196ms/step - loss: 4.0232\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 76s 193ms/step - loss: 4.0217\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 79s 200ms/step - loss: 4.0206\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 81s 207ms/step - loss: 4.0182\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 79s 203ms/step - loss: 4.0174\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 80s 206ms/step - loss: 4.0154\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 78s 199ms/step - loss: 4.0140\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 78s 200ms/step - loss: 4.0124\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 78s 198ms/step - loss: 4.0111\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 78s 199ms/step - loss: 4.0098\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 77s 198ms/step - loss: 4.0082\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 77s 197ms/step - loss: 4.0067\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 78s 200ms/step - loss: 4.0055\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 77s 198ms/step - loss: 4.0046\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 80s 206ms/step - loss: 4.0029\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 75s 192ms/step - loss: 4.0017\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 76s 195ms/step - loss: 4.0002\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 79s 200ms/step - loss: 3.9994\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 77s 195ms/step - loss: 3.9977\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 77s 196ms/step - loss: 3.9967\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 79s 201ms/step - loss: 3.9950\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 78s 199ms/step - loss: 3.9941\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 77s 197ms/step - loss: 3.9925\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 77s 197ms/step - loss: 3.9922\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 82s 209ms/step - loss: 3.9906\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 79s 201ms/step - loss: 3.9891\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 82s 211ms/step - loss: 3.9883\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 82s 211ms/step - loss: 3.9872\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 80s 205ms/step - loss: 3.9860\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 78s 199ms/step - loss: 3.9846\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 78s 199ms/step - loss: 3.9837\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 77s 198ms/step - loss: 3.9829\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 80s 204ms/step - loss: 3.9818\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 75s 191ms/step - loss: 3.9811\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 76s 195ms/step - loss: 3.9798\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 77s 197ms/step - loss: 3.9791\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 79s 202ms/step - loss: 3.9781\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 77s 196ms/step - loss: 3.9770\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 77s 197ms/step - loss: 3.9762\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 79s 203ms/step - loss: 3.9752\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 80s 204ms/step - loss: 3.9740\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 80s 205ms/step - loss: 3.9729\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 79s 203ms/step - loss: 3.9723\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 80s 204ms/step - loss: 3.9714\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 80s 205ms/step - loss: 3.9704\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 78s 199ms/step - loss: 3.9695\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 79s 203ms/step - loss: 3.9684\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 79s 201ms/step - loss: 3.9675\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 76s 195ms/step - loss: 3.9670\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 78s 199ms/step - loss: 3.9662\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 76s 195ms/step - loss: 3.9656\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 79s 203ms/step - loss: 3.9645\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 79s 200ms/step - loss: 3.9635\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 84s 214ms/step - loss: 3.9629\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 84s 215ms/step - loss: 3.9619\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 82s 209ms/step - loss: 3.9613\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 84s 214ms/step - loss: 3.9606\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 84s 214ms/step - loss: 3.9600\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 84s 214ms/step - loss: 3.9588\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 72s 185ms/step - loss: 3.9584\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 72s 185ms/step - loss: 3.9574\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 73s 186ms/step - loss: 3.9568\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 75s 193ms/step - loss: 3.9558\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 72052s 185s/step - loss: 3.9554\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 72s 184ms/step - loss: 3.9549\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 78s 199ms/step - loss: 3.9541\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 76s 195ms/step - loss: 3.9533\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 80s 203ms/step - loss: 3.9524\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 78s 199ms/step - loss: 3.9521\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 82s 209ms/step - loss: 3.9513\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 79s 202ms/step - loss: 3.9509\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 80s 204ms/step - loss: 3.9503\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 76s 195ms/step - loss: 3.9491\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 77s 197ms/step - loss: 3.9491\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 76s 195ms/step - loss: 3.9482\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 79s 200ms/step - loss: 3.9475\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 77s 196ms/step - loss: 3.9467\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 76s 195ms/step - loss: 3.9463\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 76s 194ms/step - loss: 3.9459\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 78s 198ms/step - loss: 3.9445\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 79s 203ms/step - loss: 3.9442\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 78s 198ms/step - loss: 3.9439\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 78s 199ms/step - loss: 3.9433\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 82s 209ms/step - loss: 3.9427\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 81s 206ms/step - loss: 3.9425\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 79s 203ms/step - loss: 3.9413\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 80s 204ms/step - loss: 3.9409\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 77s 197ms/step - loss: 3.9403\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 79s 203ms/step - loss: 3.9398\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 78s 201ms/step - loss: 3.9395\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 77s 196ms/step - loss: 3.9390\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 77s 197ms/step - loss: 3.9384\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 79s 201ms/step - loss: 3.9383\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 79s 203ms/step - loss: 3.9372\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 78s 200ms/step - loss: 3.9367\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 77s 197ms/step - loss: 3.9361\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 76s 194ms/step - loss: 3.9358\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 75s 192ms/step - loss: 3.9348\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 79s 201ms/step - loss: 3.9345\n",
      "Epoch 190/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 77s 196ms/step - loss: 3.9340\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 76s 194ms/step - loss: 3.9342\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 77s 197ms/step - loss: 3.9331\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 77s 197ms/step - loss: 3.9329\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 78s 198ms/step - loss: 3.9323\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 77s 198ms/step - loss: 3.9313\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 77s 197ms/step - loss: 3.9316\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 78s 198ms/step - loss: 3.9311\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 78s 198ms/step - loss: 3.9306\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 78s 199ms/step - loss: 3.9302\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 79s 202ms/step - loss: 3.9297\n"
     ]
    }
   ],
   "source": [
    "history = train(model_decoderonly,path,vectorized_imdb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bb5dbc",
   "metadata": {},
   "source": [
    "### Inference: Generate new Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dbcde67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for sampling the next token\n",
    "def sample_next(predictions, temperature=1.0):\n",
    "    predictions = np.asarray(predictions).astype(\"float64\")\n",
    "    predictions = np.log(predictions + 1e-10) / temperature  # Avoid log(0)\n",
    "    exp_preds = np.exp(predictions)\n",
    "    predictions = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, predictions, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "77fd8328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for text generation\n",
    "def generate_text(prompt, model, text_vectorization, tokens_index, length, temperature=1.0):\n",
    "    # Tokenize the prompt\n",
    "    tokenized_prompt = text_vectorization([prompt])\n",
    "    generated_text = prompt\n",
    "    \n",
    "    for _ in range(length):\n",
    "        predictions = model(tokenized_prompt)\n",
    "        # Get the last predicted token probabilities\n",
    "        next_token_logits = predictions[:, -1, :]\n",
    "        next_token = sample_next(next_token_logits[0].numpy(), temperature)\n",
    "        sampled_word = tokens_index[next_token]\n",
    "        generated_text += \" \" + sampled_word\n",
    "        # Update the tokenized prompt with the new word\n",
    "        tokenized_prompt = text_vectorization([generated_text])\n",
    "    \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "94cd62d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "scarface was all takashi to and the the years is molly . with whenever with to that of title [UNK] only today, and . long as in time for & . great however [UNK] music and which like ? idea months. back \" music horror . not . canadian . film.\n"
     ]
    }
   ],
   "source": [
    "# Example: Generate text\n",
    "prompt = \"scarface\"\n",
    "generated_text = generate_text(\n",
    "    prompt=prompt,\n",
    "    model=model_decoderonly,\n",
    "    text_vectorization=vectorizer,\n",
    "    tokens_index={i: word for i, word in enumerate(vectorizer.get_vocabulary())},\n",
    "    length=50,  # Number of tokens to generate\n",
    "    temperature=0.9  # Adjust temperature as needed\n",
    ")\n",
    "\n",
    "print(\"Generated Text:\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400e1754",
   "metadata": {},
   "source": [
    "# !!!!! Congratualtions you have built your own Language Model :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "426720da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our toy language model uses the same core technology as the GPT 1 model and also even the latest LLM's "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7de50a",
   "metadata": {},
   "source": [
    "**GPT1 (2017) from OpenAI**\n",
    "\n",
    "**!!! 110 Million Parameters**\n",
    "\n",
    "* decoder blocks: 12\n",
    "\n",
    "* Embedding dim: 768\n",
    "\n",
    "* Hidden layer dim: 3072\n",
    "\n",
    "* Attention Heads: 12\n",
    "\n",
    "* Sequence Length: 512\n",
    "\n",
    "* Vocab Size: 40000\n",
    "\n",
    "* Training Data: BooksCorpus of 7000 books, 4.5 gb of text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fb7212",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29d3b3fe",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"./images/PallenceAI-Final.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccee792",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on IMDb Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec20ec4",
   "metadata": {},
   "source": [
    "## Sequence Models: BiLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf1ed44",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"./images/sequence-bilstm.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa336972",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"./images/imdb2.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162adc49",
   "metadata": {},
   "source": [
    "### Import needed libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a9babbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic Python packages for data wrangling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "#Tensorflow & Keras related packages\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "from utils import plot_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50a78ca",
   "metadata": {},
   "source": [
    "### Load IMDb Dataset Preloaded in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24f0320c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sridh\\anaconda3\\envs\\tf2.10_env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14d20b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = tfds.load(\n",
    "    name=\"imdb_reviews\",\n",
    "    split=[\"train[:80%]\", \"train[80%:]\", \"test\"],\n",
    "    as_supervised=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aceea5d",
   "metadata": {},
   "source": [
    "### Understanding the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76b60dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "5000\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "# Lets see how many sample reviews are there in each dataset\n",
    "\n",
    "print (len(train))\n",
    "print (len(val))\n",
    "print (len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2025bfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch the data\n",
    "train_data = train.batch(32)\n",
    "val_data = val.batch(32)\n",
    "test_data =test.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c6bda77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews shape (32,)\n",
      "Labels shape (32,) \n",
      "\n",
      "Sample 7th Review: Okay, you have:<br /><br />Penelope Keith as Miss Herringbone-Tweed, B.B.E. (Backbone of England.) She's killed off in the first scene - that's right, folks; this show has no backbone!<br /><br />Peter O'Toole as Ol' Colonel Cricket from The First War and now the emblazered Lord of the Manor.<br /><br />Joanna Lumley as the ensweatered Lady of the Manor, 20 years younger than the colonel and 20 years past her own prime but still glamourous (Brit spelling, not mine) enough to have a toy-boy on the side. It's alright, they have Col. Cricket's full knowledge and consent (they guy even comes 'round for Christmas!) Still, she's considerate of the colonel enough to have said toy-boy her own age (what a gal!)<br /><br />David McCallum as said toy-boy, equally as pointlessly glamourous as his squeeze. Pilcher couldn't come up with any cover for him within the story, so she gave him a hush-hush job at the Circus.<br /><br />and finally:<br /><br />Susan Hampshire as Miss Polonia Teacups, Venerable Headmistress of the Venerable Girls' Boarding-School, serving tea in her office with a dash of deep, poignant advice for life in the outside world just before graduation. Her best bit of advice: \"I've only been to Nancherrow (the local Stately Home of England) once. I thought it was very beautiful but, somehow, not part of the real world.\" Well, we can't say they didn't warn us.<br /><br />Ah, Susan - time was, your character would have been running the whole show. They don't write 'em like that any more. Our loss, not yours.<br /><br />So - with a cast and setting like this, you have the re-makings of \"Brideshead Revisited,\" right?<br /><br />Wrong! They took these 1-dimensional supporting roles because they paid so well. After all, acting is one of the oldest temp-jobs there is (YOU name another!)<br /><br />First warning sign: lots and lots of backlighting. They get around it by shooting outdoors - \"hey, it's just the sunlight!\"<br /><br />Second warning sign: Leading Lady cries a lot. When not crying, her eyes are moist. That's the law of romance novels: Leading Lady is \"dewy-eyed.\"<br /><br />Henceforth, Leading Lady shall be known as L.L.<br /><br />Third warning sign: L.L. actually has stars in her eyes when she's in love. Still, I'll give Emily Mortimer an award just for having to act with that spotlight in her eyes (I wonder . did they use contacts?)<br /><br />And lastly, fourth warning sign: no on-screen female character is \"Mrs.\" She's either \"Miss\" or \"Lady.\"<br /><br />When all was said and done, I still couldn't tell you who was pursuing whom and why. I couldn't even tell you what was said and done.<br /><br />To sum up: they all live through World War II without anything happening to them at all.<br /><br />OK, at the end, L.L. finds she's lost her parents to the Japanese prison camps and baby sis comes home catatonic. Meanwhile (there's always a \"meanwhile,\") some young guy L.L. had a crush on (when, I don't know) comes home from some wartime tough spot and is found living on the street by Lady of the Manor (must be some street if SHE's going to find him there.) Both war casualties are whisked away to recover at Nancherrow (SOMEBODY has to be \"whisked away\" SOMEWHERE in these romance stories!)<br /><br />Great drama. \n",
      "\n",
      "Sample 7th Label: 0\n"
     ]
    }
   ],
   "source": [
    "# lets look at the first batch of data\n",
    "\n",
    "for reviews, labels in train_data.take(1):\n",
    "    print (\"Reviews shape\", reviews.shape)\n",
    "    print (\"Labels shape\", labels.shape, \"\\n\")\n",
    "    \n",
    "    print ('Sample 7th Review:', reviews[6].numpy().decode(\"utf-8\"), \"\\n\")\n",
    "    print ('Sample 7th Label:', labels[6].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8bb2551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First select only reviews from training data (which contains both reviews/labels) \n",
    "#.....which we will vectorize into numeric format.\n",
    "\n",
    "train_data_onlyreviews = train_data.map(lambda x,y : x) # given a tuple (x,y), output only x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8262f89",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"./images/vectorization.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd8ca37",
   "metadata": {},
   "source": [
    "## Data prep for Sequence Models\n",
    "1. Vectorize the sequences as shown below with TextVectorization in Keras:\n",
    "* Tokenize the sequences\n",
    "* Encode the tokens into integers\n",
    "\n",
    "2. After this we can convert each integer to a corresponding vector (such as one-hot, or word embeddings). This can be \n",
    "done by a seperate one-hot layer or embedding layer directly during model building stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a7c6dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras has inbuilt Text Vectorization layer that can standardize, tokenize, and \n",
    "#.....convert to indices or token vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd50f2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also use our own custom functions to pass into Text vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64e2f1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_text):\n",
    "    # convert to lower case\n",
    "    lowercase_text = tf.strings.lower(input_text)\n",
    "    \n",
    "    #remove html tags\n",
    "    stripped_html = tf.strings.regex_replace(lowercase_text, \"<br\\s*/?>\", \" \")\n",
    "    \n",
    "    # Remove most punctuation but keep exclamation marks and question marks\n",
    "    cleaned_text = tf.strings.regex_replace(stripped_html, \"[^a-zA-Z0-9!?]\", \" \")\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f40e8c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "# this is a limit on sequence length. each sequence will be 300 words long\n",
    "max_length = 300 \n",
    "\n",
    "# this is for the vocabulary limit. max tokens to use for creating the vocabulary.\n",
    "max_tokens = 10000 \n",
    "\n",
    "text_vectorization = TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode = \"int\",\n",
    "    output_sequence_length = max_length,\n",
    "    standardize = custom_standardization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40ef2fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply vectorization on training data reviews\n",
    "# apply text vectorization on training data reviews to index the vocabulary\n",
    "\n",
    "text_vectorization.adapt(train_data_onlyreviews) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b97428dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'the',\n",
       " 'and',\n",
       " 'a',\n",
       " 'of',\n",
       " 'to',\n",
       " 'is',\n",
       " 'it',\n",
       " 'in',\n",
       " 'i',\n",
       " 'this',\n",
       " 'that',\n",
       " 's',\n",
       " 'was',\n",
       " 'as',\n",
       " 'for',\n",
       " 'with',\n",
       " 'movie',\n",
       " 'but',\n",
       " 'film',\n",
       " 't',\n",
       " 'on',\n",
       " 'you',\n",
       " 'not',\n",
       " 'he',\n",
       " 'his',\n",
       " 'are',\n",
       " 'have',\n",
       " 'be',\n",
       " 'one',\n",
       " 'all',\n",
       " 'at',\n",
       " 'they',\n",
       " 'by',\n",
       " 'who',\n",
       " 'an',\n",
       " 'so',\n",
       " 'from',\n",
       " 'like',\n",
       " 'there',\n",
       " 'her',\n",
       " 'or',\n",
       " 'just',\n",
       " 'about',\n",
       " 'out',\n",
       " 'if',\n",
       " 'has',\n",
       " 'what',\n",
       " 'some',\n",
       " 'good',\n",
       " 'can',\n",
       " 'she',\n",
       " 'very',\n",
       " 'when',\n",
       " 'more',\n",
       " 'up',\n",
       " 'would',\n",
       " 'even',\n",
       " 'time',\n",
       " 'my',\n",
       " 'no',\n",
       " 'which',\n",
       " 'story',\n",
       " 'only',\n",
       " 'really',\n",
       " 'their',\n",
       " 'see',\n",
       " 'had',\n",
       " 'we',\n",
       " 'were',\n",
       " 'me',\n",
       " 'well',\n",
       " 'than',\n",
       " 'much',\n",
       " 'been',\n",
       " 'get',\n",
       " 'people',\n",
       " 'will',\n",
       " 'also',\n",
       " 'bad',\n",
       " 'do',\n",
       " 'other',\n",
       " 'into',\n",
       " 'first',\n",
       " 'because',\n",
       " 'great',\n",
       " 'how',\n",
       " 'don',\n",
       " 'most',\n",
       " 'him',\n",
       " 'made',\n",
       " 'its',\n",
       " 'then',\n",
       " 'make',\n",
       " 'way',\n",
       " 'could',\n",
       " 'them',\n",
       " 'too',\n",
       " 'any',\n",
       " 'after',\n",
       " 'movies',\n",
       " 'think',\n",
       " 'characters',\n",
       " 'character',\n",
       " 'two',\n",
       " 'watch',\n",
       " 'films',\n",
       " 'many',\n",
       " 'being',\n",
       " 'seen',\n",
       " 'never',\n",
       " 'plot',\n",
       " 'life',\n",
       " 'love',\n",
       " 'acting',\n",
       " 'little',\n",
       " 'where',\n",
       " 'best',\n",
       " 'over',\n",
       " 'did',\n",
       " 'show',\n",
       " 'know',\n",
       " 'off',\n",
       " 'man',\n",
       " 'ever',\n",
       " 'does',\n",
       " 'your',\n",
       " 'better',\n",
       " 'here',\n",
       " 'still',\n",
       " 'end',\n",
       " 'these',\n",
       " 'scene',\n",
       " 'say',\n",
       " 'while',\n",
       " 'scenes',\n",
       " 'go',\n",
       " 've',\n",
       " 'such',\n",
       " 'm',\n",
       " 'should',\n",
       " 'something',\n",
       " 'why',\n",
       " 'through',\n",
       " 'back',\n",
       " 'real',\n",
       " 'those',\n",
       " 'watching',\n",
       " 'doesn',\n",
       " 'though',\n",
       " 'old',\n",
       " 'thing',\n",
       " 'now',\n",
       " 'director',\n",
       " 'actors',\n",
       " 're',\n",
       " 'years',\n",
       " 'work',\n",
       " 'didn',\n",
       " 'another',\n",
       " 'new',\n",
       " '10',\n",
       " 'nothing',\n",
       " 'before',\n",
       " 'makes',\n",
       " 'funny',\n",
       " 'look',\n",
       " 'actually',\n",
       " 'going',\n",
       " 'find',\n",
       " 'same',\n",
       " 'few',\n",
       " 'part',\n",
       " 'every',\n",
       " 'lot',\n",
       " 'again',\n",
       " 'cast',\n",
       " 'world',\n",
       " 'us',\n",
       " 'quite',\n",
       " 'down',\n",
       " 'pretty',\n",
       " 'want',\n",
       " 'young',\n",
       " 'things',\n",
       " 'seems',\n",
       " 'got',\n",
       " 'around',\n",
       " 'take',\n",
       " 'horror',\n",
       " 'however',\n",
       " 'fact',\n",
       " 'thought',\n",
       " 'long',\n",
       " 'big',\n",
       " 'enough',\n",
       " 'between',\n",
       " 'series',\n",
       " 'both',\n",
       " 'own',\n",
       " 'original',\n",
       " 'may',\n",
       " 'give',\n",
       " 'd',\n",
       " 'action',\n",
       " 'always',\n",
       " 'without',\n",
       " 'must',\n",
       " 'comedy',\n",
       " 'isn',\n",
       " 'family',\n",
       " 'come',\n",
       " 'times',\n",
       " 'saw',\n",
       " 'point',\n",
       " 'gets',\n",
       " 'role',\n",
       " 'least',\n",
       " 'whole',\n",
       " 'right',\n",
       " 'almost',\n",
       " 'interesting',\n",
       " 'done',\n",
       " 'bit',\n",
       " 'script',\n",
       " 'music',\n",
       " 'last',\n",
       " 'making',\n",
       " 'guy',\n",
       " 'anything',\n",
       " 'far',\n",
       " 'll',\n",
       " 'since',\n",
       " 'might',\n",
       " 'performance',\n",
       " 'feel',\n",
       " 'minutes',\n",
       " 'tv',\n",
       " '2',\n",
       " 'am',\n",
       " 'woman',\n",
       " 'girl',\n",
       " 'probably',\n",
       " 'yet',\n",
       " 'kind',\n",
       " 'rather',\n",
       " 'worst',\n",
       " 'day',\n",
       " 'away',\n",
       " 'hard',\n",
       " 'found',\n",
       " 'sure',\n",
       " 'fun',\n",
       " 'anyone',\n",
       " 'having',\n",
       " 'each',\n",
       " 'played',\n",
       " 'although',\n",
       " 'especially',\n",
       " 'looking',\n",
       " 'our',\n",
       " 'trying',\n",
       " 'course',\n",
       " 'believe',\n",
       " 'screen',\n",
       " 'comes',\n",
       " 'set',\n",
       " 'looks',\n",
       " 'goes',\n",
       " 'book',\n",
       " 'different',\n",
       " 'place',\n",
       " 'actor',\n",
       " 'ending',\n",
       " 'put',\n",
       " 'year',\n",
       " 'main',\n",
       " 'true',\n",
       " 'wasn',\n",
       " 'everything',\n",
       " 'three',\n",
       " 'maybe',\n",
       " 'once',\n",
       " 'money',\n",
       " 'worth',\n",
       " 'dvd',\n",
       " 'shows',\n",
       " 'sense',\n",
       " 'someone',\n",
       " 'let',\n",
       " 'reason',\n",
       " '1',\n",
       " 'american',\n",
       " 'john',\n",
       " 'job',\n",
       " 'watched',\n",
       " 'plays',\n",
       " 'play',\n",
       " 'together',\n",
       " 'later',\n",
       " 'said',\n",
       " 'audience',\n",
       " 'takes',\n",
       " 'high',\n",
       " 'house',\n",
       " 'seem',\n",
       " 'effects',\n",
       " 'version',\n",
       " 'everyone',\n",
       " 'instead',\n",
       " 'wife',\n",
       " 'during',\n",
       " 'father',\n",
       " 'himself',\n",
       " 'left',\n",
       " 'night',\n",
       " 'beautiful',\n",
       " 'star',\n",
       " 'half',\n",
       " 'special',\n",
       " 'seeing',\n",
       " 'shot',\n",
       " 'excellent',\n",
       " 'war',\n",
       " 'less',\n",
       " 'black',\n",
       " 'mind',\n",
       " 'second',\n",
       " 'idea',\n",
       " 'simply',\n",
       " 'nice',\n",
       " 'else',\n",
       " 'read',\n",
       " '3',\n",
       " 'help',\n",
       " 'death',\n",
       " 'men',\n",
       " 'home',\n",
       " 'hollywood',\n",
       " 'fan',\n",
       " 'used',\n",
       " 'dead',\n",
       " 'completely',\n",
       " 'given',\n",
       " 'poor',\n",
       " 'short',\n",
       " 'performances',\n",
       " 'either',\n",
       " 'line',\n",
       " 'top',\n",
       " 'need',\n",
       " 'rest',\n",
       " 'try',\n",
       " 'budget',\n",
       " 'low',\n",
       " 'production',\n",
       " 'camera',\n",
       " 'boring',\n",
       " 'use',\n",
       " 'enjoy',\n",
       " 'full',\n",
       " 'classic',\n",
       " 'truly',\n",
       " 'friends',\n",
       " 'women',\n",
       " 'kids',\n",
       " 'along',\n",
       " 'until',\n",
       " 'tell',\n",
       " 'video',\n",
       " 'wrong',\n",
       " 'stars',\n",
       " 'next',\n",
       " 'moments',\n",
       " 'came',\n",
       " 'awful',\n",
       " 'couple',\n",
       " 'remember',\n",
       " 'wonderful',\n",
       " 'start',\n",
       " 'mean',\n",
       " 'getting',\n",
       " 'won',\n",
       " 'perhaps',\n",
       " 'stupid',\n",
       " 'terrible',\n",
       " 'recommend',\n",
       " 'understand',\n",
       " 'small',\n",
       " 'school',\n",
       " 'playing',\n",
       " 'face',\n",
       " 'sex',\n",
       " 'others',\n",
       " 'written',\n",
       " 'keep',\n",
       " 'perfect',\n",
       " 'episode',\n",
       " 'early',\n",
       " 'doing',\n",
       " 'definitely',\n",
       " 'human',\n",
       " 'gives',\n",
       " 'style',\n",
       " 'often',\n",
       " 'mother',\n",
       " 'person',\n",
       " 'felt',\n",
       " 'dialogue',\n",
       " 'name',\n",
       " 'finally',\n",
       " 'case',\n",
       " 'liked',\n",
       " 'head',\n",
       " 'supposed',\n",
       " 'become',\n",
       " 'lost',\n",
       " 'itself',\n",
       " 'lines',\n",
       " 'piece',\n",
       " 'couldn',\n",
       " 'live',\n",
       " 'picture',\n",
       " 'boy',\n",
       " 'entire',\n",
       " 'against',\n",
       " 'killer',\n",
       " 'children',\n",
       " 'absolutely',\n",
       " 'title',\n",
       " 'called',\n",
       " 'based',\n",
       " 'hope',\n",
       " 'sort',\n",
       " 'yes',\n",
       " 'waste',\n",
       " 'certainly',\n",
       " 'cinema',\n",
       " 'worse',\n",
       " 'went',\n",
       " 'mr',\n",
       " 'friend',\n",
       " 'problem',\n",
       " 'entertaining',\n",
       " 'evil',\n",
       " 'white',\n",
       " '5',\n",
       " 'overall',\n",
       " 'oh',\n",
       " 'beginning',\n",
       " 'loved',\n",
       " 'under',\n",
       " 'several',\n",
       " 'drama',\n",
       " 'sound',\n",
       " 'fans',\n",
       " 'direction',\n",
       " 'lives',\n",
       " 'dark',\n",
       " 'becomes',\n",
       " 'turn',\n",
       " 'throughout',\n",
       " 'son',\n",
       " 'fine',\n",
       " 'seemed',\n",
       " 'care',\n",
       " 'wanted',\n",
       " 'example',\n",
       " 'already',\n",
       " 'laugh',\n",
       " 'heart',\n",
       " 'unfortunately',\n",
       " '4',\n",
       " 'despite',\n",
       " 'history',\n",
       " 'guess',\n",
       " 'child',\n",
       " 'writing',\n",
       " 'lead',\n",
       " 'final',\n",
       " 'totally',\n",
       " 'wants',\n",
       " 'viewer',\n",
       " 'close',\n",
       " 'b',\n",
       " 'able',\n",
       " 'quality',\n",
       " 'humor',\n",
       " 'turns',\n",
       " 'behind',\n",
       " 'art',\n",
       " 'michael',\n",
       " 'side',\n",
       " 'guys',\n",
       " 'tries',\n",
       " 'town',\n",
       " 'game',\n",
       " 'act',\n",
       " 'days',\n",
       " 'works',\n",
       " 'amazing',\n",
       " 'flick',\n",
       " 'girls',\n",
       " 'sometimes',\n",
       " 'hand',\n",
       " 'run',\n",
       " 'past',\n",
       " 'gave',\n",
       " 'favorite',\n",
       " 'soon',\n",
       " 'late',\n",
       " 'etc',\n",
       " 'genre',\n",
       " 'directed',\n",
       " 'starts',\n",
       " 'actress',\n",
       " 'kill',\n",
       " 'hour',\n",
       " 'horrible',\n",
       " 'eyes',\n",
       " 'self',\n",
       " 'enjoyed',\n",
       " 'car',\n",
       " 'city',\n",
       " 'today',\n",
       " 'parts',\n",
       " 'themselves',\n",
       " 'brilliant',\n",
       " 'god',\n",
       " 'obviously',\n",
       " 'kid',\n",
       " 'stories',\n",
       " 'blood',\n",
       " 'daughter',\n",
       " 'writer',\n",
       " 'decent',\n",
       " 'type',\n",
       " 'feeling',\n",
       " 'voice',\n",
       " 'roles',\n",
       " 'fight',\n",
       " 'highly',\n",
       " 'except',\n",
       " 'thinking',\n",
       " 'slow',\n",
       " 'took',\n",
       " 'myself',\n",
       " 'brother',\n",
       " 'says',\n",
       " 'moment',\n",
       " 'matter',\n",
       " 'expect',\n",
       " 'leave',\n",
       " 'age',\n",
       " 'cannot',\n",
       " 'strong',\n",
       " 'stuff',\n",
       " 'told',\n",
       " 'heard',\n",
       " 'particularly',\n",
       " 'killed',\n",
       " 'involved',\n",
       " 'hit',\n",
       " 'extremely',\n",
       " 'attempt',\n",
       " 'alone',\n",
       " 'james',\n",
       " 'police',\n",
       " 'violence',\n",
       " 'living',\n",
       " 'happens',\n",
       " 'anyway',\n",
       " 'known',\n",
       " '!',\n",
       " 'obvious',\n",
       " 'lack',\n",
       " 'wouldn',\n",
       " 'coming',\n",
       " 'happened',\n",
       " 'murder',\n",
       " 'david',\n",
       " 'experience',\n",
       " 'chance',\n",
       " 'including',\n",
       " 'stop',\n",
       " 'simple',\n",
       " 'wonder',\n",
       " 'save',\n",
       " 'looked',\n",
       " 'interest',\n",
       " 'whose',\n",
       " 'group',\n",
       " 'career',\n",
       " 'happen',\n",
       " 'complete',\n",
       " 'none',\n",
       " 'light',\n",
       " 'number',\n",
       " 'husband',\n",
       " 'cut',\n",
       " 'hero',\n",
       " 'opening',\n",
       " 'song',\n",
       " 'running',\n",
       " 'english',\n",
       " 'gore',\n",
       " 'score',\n",
       " 'annoying',\n",
       " 'king',\n",
       " 'musical',\n",
       " 'relationship',\n",
       " 'serious',\n",
       " 'possible',\n",
       " 'ok',\n",
       " 'across',\n",
       " 'view',\n",
       " 'usually',\n",
       " 'released',\n",
       " 'opinion',\n",
       " 'yourself',\n",
       " 'taken',\n",
       " 'exactly',\n",
       " 'shown',\n",
       " 'hours',\n",
       " 'sad',\n",
       " 'cinematography',\n",
       " 'please',\n",
       " 'ago',\n",
       " 'shots',\n",
       " 'middle',\n",
       " 'somewhat',\n",
       " 'country',\n",
       " 'seriously',\n",
       " 'usual',\n",
       " 'robert',\n",
       " 'jokes',\n",
       " 'taking',\n",
       " 'started',\n",
       " 'finds',\n",
       " 'body',\n",
       " 'crap',\n",
       " 'reality',\n",
       " 'hell',\n",
       " 'cool',\n",
       " 'change',\n",
       " 'level',\n",
       " 'ridiculous',\n",
       " 'happy',\n",
       " 'mostly',\n",
       " 'scary',\n",
       " 'saying',\n",
       " 'novel',\n",
       " 'ends',\n",
       " 'huge',\n",
       " 'wish',\n",
       " 'order',\n",
       " 'turned',\n",
       " 'hilarious',\n",
       " 'episodes',\n",
       " 'call',\n",
       " 'supporting',\n",
       " 'ones',\n",
       " 'words',\n",
       " 'rating',\n",
       " 'talking',\n",
       " 'power',\n",
       " 'important',\n",
       " 'five',\n",
       " 'rock',\n",
       " 'female',\n",
       " 'single',\n",
       " 'jack',\n",
       " 'room',\n",
       " 'major',\n",
       " 'british',\n",
       " 'apparently',\n",
       " 'local',\n",
       " 'documentary',\n",
       " 'four',\n",
       " 'disappointed',\n",
       " 'strange',\n",
       " 'comic',\n",
       " 'modern',\n",
       " '7',\n",
       " 'future',\n",
       " 'earth',\n",
       " 'due',\n",
       " 'events',\n",
       " 'cheap',\n",
       " '8',\n",
       " 'word',\n",
       " 'basically',\n",
       " 'non',\n",
       " 'attention',\n",
       " 'television',\n",
       " 'talent',\n",
       " 'knew',\n",
       " 'aren',\n",
       " 'thriller',\n",
       " 'songs',\n",
       " 'paul',\n",
       " 'o',\n",
       " 'easily',\n",
       " 'clearly',\n",
       " 'tells',\n",
       " 'fast',\n",
       " 'romantic',\n",
       " 'problems',\n",
       " 'appears',\n",
       " 'oscar',\n",
       " 'miss',\n",
       " 'moving',\n",
       " 'class',\n",
       " 'bring',\n",
       " 'upon',\n",
       " 'theater',\n",
       " 'giving',\n",
       " 'similar',\n",
       " 'silly',\n",
       " 'george',\n",
       " 'falls',\n",
       " 'entertainment',\n",
       " 'straight',\n",
       " 'clich',\n",
       " 'needs',\n",
       " 'mystery',\n",
       " 'beyond',\n",
       " 'sets',\n",
       " 'predictable',\n",
       " 'lady',\n",
       " 'knows',\n",
       " 'ten',\n",
       " 'review',\n",
       " 'richard',\n",
       " 'talk',\n",
       " 'lee',\n",
       " 'nearly',\n",
       " 'eye',\n",
       " 'enjoyable',\n",
       " 'whether',\n",
       " 'named',\n",
       " 'within',\n",
       " 'near',\n",
       " 'tale',\n",
       " 'stand',\n",
       " 'message',\n",
       " 'sequence',\n",
       " 'french',\n",
       " 'above',\n",
       " 'working',\n",
       " 'red',\n",
       " 'points',\n",
       " 'theme',\n",
       " 'mention',\n",
       " 'storyline',\n",
       " 'add',\n",
       " 'feels',\n",
       " 'team',\n",
       " 'sister',\n",
       " 'herself',\n",
       " 'york',\n",
       " 'ways',\n",
       " '9',\n",
       " 'peter',\n",
       " 'hate',\n",
       " 'haven',\n",
       " 'bunch',\n",
       " 'begins',\n",
       " 'effort',\n",
       " 'surprised',\n",
       " 'release',\n",
       " 'lots',\n",
       " 'e',\n",
       " 'comments',\n",
       " 'actual',\n",
       " 'elements',\n",
       " 'animation',\n",
       " 'follow',\n",
       " 'using',\n",
       " 'somehow',\n",
       " 'doubt',\n",
       " 'typical',\n",
       " 'viewers',\n",
       " 'easy',\n",
       " 'clear',\n",
       " 'certain',\n",
       " 'feature',\n",
       " 'die',\n",
       " 'stay',\n",
       " 'material',\n",
       " 'weak',\n",
       " 'tried',\n",
       " 'sorry',\n",
       " 'sequel',\n",
       " 'fall',\n",
       " 'period',\n",
       " 'means',\n",
       " 'check',\n",
       " 'soundtrack',\n",
       " 'showing',\n",
       " 'leads',\n",
       " 'filmed',\n",
       " 'figure',\n",
       " 'dull',\n",
       " 'among',\n",
       " 'parents',\n",
       " 'gone',\n",
       " 'form',\n",
       " 'de',\n",
       " 'kept',\n",
       " 'minute',\n",
       " 'dialog',\n",
       " 'buy',\n",
       " 'brought',\n",
       " 'hear',\n",
       " 'editing',\n",
       " 'avoid',\n",
       " 'lame',\n",
       " 'crime',\n",
       " 'greatest',\n",
       " '?',\n",
       " 'viewing',\n",
       " 'indeed',\n",
       " 'japanese',\n",
       " 'general',\n",
       " 'zombie',\n",
       " 'imagine',\n",
       " 'fantastic',\n",
       " 'eventually',\n",
       " 'sequences',\n",
       " 'joe',\n",
       " 'space',\n",
       " 'realistic',\n",
       " 'believable',\n",
       " 'particular',\n",
       " 'famous',\n",
       " 'atmosphere',\n",
       " 'third',\n",
       " 'poorly',\n",
       " 'suspense',\n",
       " 'season',\n",
       " 'move',\n",
       " 'reviews',\n",
       " 'rent',\n",
       " 'average',\n",
       " 'tom',\n",
       " 'disney',\n",
       " 'baby',\n",
       " 'stage',\n",
       " 'difficult',\n",
       " 'whatever',\n",
       " 'sit',\n",
       " 'learn',\n",
       " 'street',\n",
       " 'okay',\n",
       " 'possibly',\n",
       " 'jane',\n",
       " 'decided',\n",
       " 'dance',\n",
       " 'forget',\n",
       " 'note',\n",
       " 'subject',\n",
       " 'sexual',\n",
       " 'wait',\n",
       " 'dr',\n",
       " 'truth',\n",
       " 'deal',\n",
       " 'emotional',\n",
       " 'premise',\n",
       " 'needed',\n",
       " 'expected',\n",
       " 'dog',\n",
       " 'america',\n",
       " 'screenplay',\n",
       " 'romance',\n",
       " 'meets',\n",
       " 'killing',\n",
       " 'nature',\n",
       " 'imdb',\n",
       " 'dramatic',\n",
       " 'unless',\n",
       " 'reading',\n",
       " '20',\n",
       " 'meet',\n",
       " 'free',\n",
       " 'nor',\n",
       " 'male',\n",
       " 'write',\n",
       " 'writers',\n",
       " 'became',\n",
       " 'surprise',\n",
       " 'directors',\n",
       " 'acted',\n",
       " 'leaves',\n",
       " 'badly',\n",
       " 'situation',\n",
       " 'dream',\n",
       " 'realize',\n",
       " 'question',\n",
       " 'society',\n",
       " 'shame',\n",
       " 'comment',\n",
       " 'weird',\n",
       " 'superb',\n",
       " 'fantasy',\n",
       " 'whom',\n",
       " 'credits',\n",
       " 'ask',\n",
       " 'total',\n",
       " 'sounds',\n",
       " 'older',\n",
       " 'directing',\n",
       " 'interested',\n",
       " 'forward',\n",
       " 'forced',\n",
       " 'brings',\n",
       " 'worked',\n",
       " 'otherwise',\n",
       " 'footage',\n",
       " 'box',\n",
       " 'earlier',\n",
       " 'result',\n",
       " 'beauty',\n",
       " 'open',\n",
       " 'keeps',\n",
       " 'air',\n",
       " 'creepy',\n",
       " 'ben',\n",
       " 'sci',\n",
       " 'personal',\n",
       " 'mess',\n",
       " 'towards',\n",
       " 'perfectly',\n",
       " 'leading',\n",
       " 'fi',\n",
       " 'crazy',\n",
       " 'laughs',\n",
       " 'hands',\n",
       " 'casting',\n",
       " 'previous',\n",
       " 'plus',\n",
       " 'memorable',\n",
       " 'hot',\n",
       " 'features',\n",
       " 'deep',\n",
       " 'c',\n",
       " 'boys',\n",
       " 'unique',\n",
       " 'quickly',\n",
       " 'christmas',\n",
       " 'begin',\n",
       " 'potential',\n",
       " 'plenty',\n",
       " 'setting',\n",
       " 'various',\n",
       " 'match',\n",
       " 'appear',\n",
       " 'apart',\n",
       " 'admit',\n",
       " 'twist',\n",
       " 'cheesy',\n",
       " 'rate',\n",
       " 'bill',\n",
       " 'development',\n",
       " 'powerful',\n",
       " 'mark',\n",
       " 'incredibly',\n",
       " 'fire',\n",
       " 'business',\n",
       " '30',\n",
       " 'western',\n",
       " 'telling',\n",
       " 'nudity',\n",
       " 'girlfriend',\n",
       " 'effect',\n",
       " 'present',\n",
       " 'married',\n",
       " 'dumb',\n",
       " 'outside',\n",
       " 'cop',\n",
       " 'meant',\n",
       " 'hardly',\n",
       " 'fighting',\n",
       " 'pay',\n",
       " 'joke',\n",
       " 'doctor',\n",
       " 'return',\n",
       " 'monster',\n",
       " 'expecting',\n",
       " '80',\n",
       " 'portrayed',\n",
       " 'background',\n",
       " 'front',\n",
       " 'attempts',\n",
       " 'battle',\n",
       " 'talented',\n",
       " 'inside',\n",
       " 'era',\n",
       " 'sweet',\n",
       " ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocabulary\n",
    "vocab = text_vectorization.get_vocabulary()\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d198769d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '',\n",
       " 1: '[UNK]',\n",
       " 2: 'the',\n",
       " 3: 'and',\n",
       " 4: 'a',\n",
       " 5: 'of',\n",
       " 6: 'to',\n",
       " 7: 'is',\n",
       " 8: 'it',\n",
       " 9: 'in',\n",
       " 10: 'i',\n",
       " 11: 'this',\n",
       " 12: 'that',\n",
       " 13: 's',\n",
       " 14: 'was',\n",
       " 15: 'as',\n",
       " 16: 'for',\n",
       " 17: 'with',\n",
       " 18: 'movie',\n",
       " 19: 'but',\n",
       " 20: 'film',\n",
       " 21: 't',\n",
       " 22: 'on',\n",
       " 23: 'you',\n",
       " 24: 'not',\n",
       " 25: 'he',\n",
       " 26: 'his',\n",
       " 27: 'are',\n",
       " 28: 'have',\n",
       " 29: 'be',\n",
       " 30: 'one',\n",
       " 31: 'all',\n",
       " 32: 'at',\n",
       " 33: 'they',\n",
       " 34: 'by',\n",
       " 35: 'who',\n",
       " 36: 'an',\n",
       " 37: 'so',\n",
       " 38: 'from',\n",
       " 39: 'like',\n",
       " 40: 'there',\n",
       " 41: 'her',\n",
       " 42: 'or',\n",
       " 43: 'just',\n",
       " 44: 'about',\n",
       " 45: 'out',\n",
       " 46: 'if',\n",
       " 47: 'has',\n",
       " 48: 'what',\n",
       " 49: 'some',\n",
       " 50: 'good',\n",
       " 51: 'can',\n",
       " 52: 'she',\n",
       " 53: 'very',\n",
       " 54: 'when',\n",
       " 55: 'more',\n",
       " 56: 'up',\n",
       " 57: 'would',\n",
       " 58: 'even',\n",
       " 59: 'time',\n",
       " 60: 'my',\n",
       " 61: 'no',\n",
       " 62: 'which',\n",
       " 63: 'story',\n",
       " 64: 'only',\n",
       " 65: 'really',\n",
       " 66: 'their',\n",
       " 67: 'see',\n",
       " 68: 'had',\n",
       " 69: 'we',\n",
       " 70: 'were',\n",
       " 71: 'me',\n",
       " 72: 'well',\n",
       " 73: 'than',\n",
       " 74: 'much',\n",
       " 75: 'been',\n",
       " 76: 'get',\n",
       " 77: 'people',\n",
       " 78: 'will',\n",
       " 79: 'also',\n",
       " 80: 'bad',\n",
       " 81: 'do',\n",
       " 82: 'other',\n",
       " 83: 'into',\n",
       " 84: 'first',\n",
       " 85: 'because',\n",
       " 86: 'great',\n",
       " 87: 'how',\n",
       " 88: 'don',\n",
       " 89: 'most',\n",
       " 90: 'him',\n",
       " 91: 'made',\n",
       " 92: 'its',\n",
       " 93: 'then',\n",
       " 94: 'make',\n",
       " 95: 'way',\n",
       " 96: 'could',\n",
       " 97: 'them',\n",
       " 98: 'too',\n",
       " 99: 'any',\n",
       " 100: 'after',\n",
       " 101: 'movies',\n",
       " 102: 'think',\n",
       " 103: 'characters',\n",
       " 104: 'character',\n",
       " 105: 'two',\n",
       " 106: 'watch',\n",
       " 107: 'films',\n",
       " 108: 'many',\n",
       " 109: 'being',\n",
       " 110: 'seen',\n",
       " 111: 'never',\n",
       " 112: 'plot',\n",
       " 113: 'life',\n",
       " 114: 'love',\n",
       " 115: 'acting',\n",
       " 116: 'little',\n",
       " 117: 'where',\n",
       " 118: 'best',\n",
       " 119: 'over',\n",
       " 120: 'did',\n",
       " 121: 'show',\n",
       " 122: 'know',\n",
       " 123: 'off',\n",
       " 124: 'man',\n",
       " 125: 'ever',\n",
       " 126: 'does',\n",
       " 127: 'your',\n",
       " 128: 'better',\n",
       " 129: 'here',\n",
       " 130: 'still',\n",
       " 131: 'end',\n",
       " 132: 'these',\n",
       " 133: 'scene',\n",
       " 134: 'say',\n",
       " 135: 'while',\n",
       " 136: 'scenes',\n",
       " 137: 'go',\n",
       " 138: 've',\n",
       " 139: 'such',\n",
       " 140: 'm',\n",
       " 141: 'should',\n",
       " 142: 'something',\n",
       " 143: 'why',\n",
       " 144: 'through',\n",
       " 145: 'back',\n",
       " 146: 'real',\n",
       " 147: 'those',\n",
       " 148: 'watching',\n",
       " 149: 'doesn',\n",
       " 150: 'though',\n",
       " 151: 'old',\n",
       " 152: 'thing',\n",
       " 153: 'now',\n",
       " 154: 'director',\n",
       " 155: 'actors',\n",
       " 156: 're',\n",
       " 157: 'years',\n",
       " 158: 'work',\n",
       " 159: 'didn',\n",
       " 160: 'another',\n",
       " 161: 'new',\n",
       " 162: '10',\n",
       " 163: 'nothing',\n",
       " 164: 'before',\n",
       " 165: 'makes',\n",
       " 166: 'funny',\n",
       " 167: 'look',\n",
       " 168: 'actually',\n",
       " 169: 'going',\n",
       " 170: 'find',\n",
       " 171: 'same',\n",
       " 172: 'few',\n",
       " 173: 'part',\n",
       " 174: 'every',\n",
       " 175: 'lot',\n",
       " 176: 'again',\n",
       " 177: 'cast',\n",
       " 178: 'world',\n",
       " 179: 'us',\n",
       " 180: 'quite',\n",
       " 181: 'down',\n",
       " 182: 'pretty',\n",
       " 183: 'want',\n",
       " 184: 'young',\n",
       " 185: 'things',\n",
       " 186: 'seems',\n",
       " 187: 'got',\n",
       " 188: 'around',\n",
       " 189: 'take',\n",
       " 190: 'horror',\n",
       " 191: 'however',\n",
       " 192: 'fact',\n",
       " 193: 'thought',\n",
       " 194: 'long',\n",
       " 195: 'big',\n",
       " 196: 'enough',\n",
       " 197: 'between',\n",
       " 198: 'series',\n",
       " 199: 'both',\n",
       " 200: 'own',\n",
       " 201: 'original',\n",
       " 202: 'may',\n",
       " 203: 'give',\n",
       " 204: 'd',\n",
       " 205: 'action',\n",
       " 206: 'always',\n",
       " 207: 'without',\n",
       " 208: 'must',\n",
       " 209: 'comedy',\n",
       " 210: 'isn',\n",
       " 211: 'family',\n",
       " 212: 'come',\n",
       " 213: 'times',\n",
       " 214: 'saw',\n",
       " 215: 'point',\n",
       " 216: 'gets',\n",
       " 217: 'role',\n",
       " 218: 'least',\n",
       " 219: 'whole',\n",
       " 220: 'right',\n",
       " 221: 'almost',\n",
       " 222: 'interesting',\n",
       " 223: 'done',\n",
       " 224: 'bit',\n",
       " 225: 'script',\n",
       " 226: 'music',\n",
       " 227: 'last',\n",
       " 228: 'making',\n",
       " 229: 'guy',\n",
       " 230: 'anything',\n",
       " 231: 'far',\n",
       " 232: 'll',\n",
       " 233: 'since',\n",
       " 234: 'might',\n",
       " 235: 'performance',\n",
       " 236: 'feel',\n",
       " 237: 'minutes',\n",
       " 238: 'tv',\n",
       " 239: '2',\n",
       " 240: 'am',\n",
       " 241: 'woman',\n",
       " 242: 'girl',\n",
       " 243: 'probably',\n",
       " 244: 'yet',\n",
       " 245: 'kind',\n",
       " 246: 'rather',\n",
       " 247: 'worst',\n",
       " 248: 'day',\n",
       " 249: 'away',\n",
       " 250: 'hard',\n",
       " 251: 'found',\n",
       " 252: 'sure',\n",
       " 253: 'fun',\n",
       " 254: 'anyone',\n",
       " 255: 'having',\n",
       " 256: 'each',\n",
       " 257: 'played',\n",
       " 258: 'although',\n",
       " 259: 'especially',\n",
       " 260: 'looking',\n",
       " 261: 'our',\n",
       " 262: 'trying',\n",
       " 263: 'course',\n",
       " 264: 'believe',\n",
       " 265: 'screen',\n",
       " 266: 'comes',\n",
       " 267: 'set',\n",
       " 268: 'looks',\n",
       " 269: 'goes',\n",
       " 270: 'book',\n",
       " 271: 'different',\n",
       " 272: 'place',\n",
       " 273: 'actor',\n",
       " 274: 'ending',\n",
       " 275: 'put',\n",
       " 276: 'year',\n",
       " 277: 'main',\n",
       " 278: 'true',\n",
       " 279: 'wasn',\n",
       " 280: 'everything',\n",
       " 281: 'three',\n",
       " 282: 'maybe',\n",
       " 283: 'once',\n",
       " 284: 'money',\n",
       " 285: 'worth',\n",
       " 286: 'dvd',\n",
       " 287: 'shows',\n",
       " 288: 'sense',\n",
       " 289: 'someone',\n",
       " 290: 'let',\n",
       " 291: 'reason',\n",
       " 292: '1',\n",
       " 293: 'american',\n",
       " 294: 'john',\n",
       " 295: 'job',\n",
       " 296: 'watched',\n",
       " 297: 'plays',\n",
       " 298: 'play',\n",
       " 299: 'together',\n",
       " 300: 'later',\n",
       " 301: 'said',\n",
       " 302: 'audience',\n",
       " 303: 'takes',\n",
       " 304: 'high',\n",
       " 305: 'house',\n",
       " 306: 'seem',\n",
       " 307: 'effects',\n",
       " 308: 'version',\n",
       " 309: 'everyone',\n",
       " 310: 'instead',\n",
       " 311: 'wife',\n",
       " 312: 'during',\n",
       " 313: 'father',\n",
       " 314: 'himself',\n",
       " 315: 'left',\n",
       " 316: 'night',\n",
       " 317: 'beautiful',\n",
       " 318: 'star',\n",
       " 319: 'half',\n",
       " 320: 'special',\n",
       " 321: 'seeing',\n",
       " 322: 'shot',\n",
       " 323: 'excellent',\n",
       " 324: 'war',\n",
       " 325: 'less',\n",
       " 326: 'black',\n",
       " 327: 'mind',\n",
       " 328: 'second',\n",
       " 329: 'idea',\n",
       " 330: 'simply',\n",
       " 331: 'nice',\n",
       " 332: 'else',\n",
       " 333: 'read',\n",
       " 334: '3',\n",
       " 335: 'help',\n",
       " 336: 'death',\n",
       " 337: 'men',\n",
       " 338: 'home',\n",
       " 339: 'hollywood',\n",
       " 340: 'fan',\n",
       " 341: 'used',\n",
       " 342: 'dead',\n",
       " 343: 'completely',\n",
       " 344: 'given',\n",
       " 345: 'poor',\n",
       " 346: 'short',\n",
       " 347: 'performances',\n",
       " 348: 'either',\n",
       " 349: 'line',\n",
       " 350: 'top',\n",
       " 351: 'need',\n",
       " 352: 'rest',\n",
       " 353: 'try',\n",
       " 354: 'budget',\n",
       " 355: 'low',\n",
       " 356: 'production',\n",
       " 357: 'camera',\n",
       " 358: 'boring',\n",
       " 359: 'use',\n",
       " 360: 'enjoy',\n",
       " 361: 'full',\n",
       " 362: 'classic',\n",
       " 363: 'truly',\n",
       " 364: 'friends',\n",
       " 365: 'women',\n",
       " 366: 'kids',\n",
       " 367: 'along',\n",
       " 368: 'until',\n",
       " 369: 'tell',\n",
       " 370: 'video',\n",
       " 371: 'wrong',\n",
       " 372: 'stars',\n",
       " 373: 'next',\n",
       " 374: 'moments',\n",
       " 375: 'came',\n",
       " 376: 'awful',\n",
       " 377: 'couple',\n",
       " 378: 'remember',\n",
       " 379: 'wonderful',\n",
       " 380: 'start',\n",
       " 381: 'mean',\n",
       " 382: 'getting',\n",
       " 383: 'won',\n",
       " 384: 'perhaps',\n",
       " 385: 'stupid',\n",
       " 386: 'terrible',\n",
       " 387: 'recommend',\n",
       " 388: 'understand',\n",
       " 389: 'small',\n",
       " 390: 'school',\n",
       " 391: 'playing',\n",
       " 392: 'face',\n",
       " 393: 'sex',\n",
       " 394: 'others',\n",
       " 395: 'written',\n",
       " 396: 'keep',\n",
       " 397: 'perfect',\n",
       " 398: 'episode',\n",
       " 399: 'early',\n",
       " 400: 'doing',\n",
       " 401: 'definitely',\n",
       " 402: 'human',\n",
       " 403: 'gives',\n",
       " 404: 'style',\n",
       " 405: 'often',\n",
       " 406: 'mother',\n",
       " 407: 'person',\n",
       " 408: 'felt',\n",
       " 409: 'dialogue',\n",
       " 410: 'name',\n",
       " 411: 'finally',\n",
       " 412: 'case',\n",
       " 413: 'liked',\n",
       " 414: 'head',\n",
       " 415: 'supposed',\n",
       " 416: 'become',\n",
       " 417: 'lost',\n",
       " 418: 'itself',\n",
       " 419: 'lines',\n",
       " 420: 'piece',\n",
       " 421: 'couldn',\n",
       " 422: 'live',\n",
       " 423: 'picture',\n",
       " 424: 'boy',\n",
       " 425: 'entire',\n",
       " 426: 'against',\n",
       " 427: 'killer',\n",
       " 428: 'children',\n",
       " 429: 'absolutely',\n",
       " 430: 'title',\n",
       " 431: 'called',\n",
       " 432: 'based',\n",
       " 433: 'hope',\n",
       " 434: 'sort',\n",
       " 435: 'yes',\n",
       " 436: 'waste',\n",
       " 437: 'certainly',\n",
       " 438: 'cinema',\n",
       " 439: 'worse',\n",
       " 440: 'went',\n",
       " 441: 'mr',\n",
       " 442: 'friend',\n",
       " 443: 'problem',\n",
       " 444: 'entertaining',\n",
       " 445: 'evil',\n",
       " 446: 'white',\n",
       " 447: '5',\n",
       " 448: 'overall',\n",
       " 449: 'oh',\n",
       " 450: 'beginning',\n",
       " 451: 'loved',\n",
       " 452: 'under',\n",
       " 453: 'several',\n",
       " 454: 'drama',\n",
       " 455: 'sound',\n",
       " 456: 'fans',\n",
       " 457: 'direction',\n",
       " 458: 'lives',\n",
       " 459: 'dark',\n",
       " 460: 'becomes',\n",
       " 461: 'turn',\n",
       " 462: 'throughout',\n",
       " 463: 'son',\n",
       " 464: 'fine',\n",
       " 465: 'seemed',\n",
       " 466: 'care',\n",
       " 467: 'wanted',\n",
       " 468: 'example',\n",
       " 469: 'already',\n",
       " 470: 'laugh',\n",
       " 471: 'heart',\n",
       " 472: 'unfortunately',\n",
       " 473: '4',\n",
       " 474: 'despite',\n",
       " 475: 'history',\n",
       " 476: 'guess',\n",
       " 477: 'child',\n",
       " 478: 'writing',\n",
       " 479: 'lead',\n",
       " 480: 'final',\n",
       " 481: 'totally',\n",
       " 482: 'wants',\n",
       " 483: 'viewer',\n",
       " 484: 'close',\n",
       " 485: 'b',\n",
       " 486: 'able',\n",
       " 487: 'quality',\n",
       " 488: 'humor',\n",
       " 489: 'turns',\n",
       " 490: 'behind',\n",
       " 491: 'art',\n",
       " 492: 'michael',\n",
       " 493: 'side',\n",
       " 494: 'guys',\n",
       " 495: 'tries',\n",
       " 496: 'town',\n",
       " 497: 'game',\n",
       " 498: 'act',\n",
       " 499: 'days',\n",
       " 500: 'works',\n",
       " 501: 'amazing',\n",
       " 502: 'flick',\n",
       " 503: 'girls',\n",
       " 504: 'sometimes',\n",
       " 505: 'hand',\n",
       " 506: 'run',\n",
       " 507: 'past',\n",
       " 508: 'gave',\n",
       " 509: 'favorite',\n",
       " 510: 'soon',\n",
       " 511: 'late',\n",
       " 512: 'etc',\n",
       " 513: 'genre',\n",
       " 514: 'directed',\n",
       " 515: 'starts',\n",
       " 516: 'actress',\n",
       " 517: 'kill',\n",
       " 518: 'hour',\n",
       " 519: 'horrible',\n",
       " 520: 'eyes',\n",
       " 521: 'self',\n",
       " 522: 'enjoyed',\n",
       " 523: 'car',\n",
       " 524: 'city',\n",
       " 525: 'today',\n",
       " 526: 'parts',\n",
       " 527: 'themselves',\n",
       " 528: 'brilliant',\n",
       " 529: 'god',\n",
       " 530: 'obviously',\n",
       " 531: 'kid',\n",
       " 532: 'stories',\n",
       " 533: 'blood',\n",
       " 534: 'daughter',\n",
       " 535: 'writer',\n",
       " 536: 'decent',\n",
       " 537: 'type',\n",
       " 538: 'feeling',\n",
       " 539: 'voice',\n",
       " 540: 'roles',\n",
       " 541: 'fight',\n",
       " 542: 'highly',\n",
       " 543: 'except',\n",
       " 544: 'thinking',\n",
       " 545: 'slow',\n",
       " 546: 'took',\n",
       " 547: 'myself',\n",
       " 548: 'brother',\n",
       " 549: 'says',\n",
       " 550: 'moment',\n",
       " 551: 'matter',\n",
       " 552: 'expect',\n",
       " 553: 'leave',\n",
       " 554: 'age',\n",
       " 555: 'cannot',\n",
       " 556: 'strong',\n",
       " 557: 'stuff',\n",
       " 558: 'told',\n",
       " 559: 'heard',\n",
       " 560: 'particularly',\n",
       " 561: 'killed',\n",
       " 562: 'involved',\n",
       " 563: 'hit',\n",
       " 564: 'extremely',\n",
       " 565: 'attempt',\n",
       " 566: 'alone',\n",
       " 567: 'james',\n",
       " 568: 'police',\n",
       " 569: 'violence',\n",
       " 570: 'living',\n",
       " 571: 'happens',\n",
       " 572: 'anyway',\n",
       " 573: 'known',\n",
       " 574: '!',\n",
       " 575: 'obvious',\n",
       " 576: 'lack',\n",
       " 577: 'wouldn',\n",
       " 578: 'coming',\n",
       " 579: 'happened',\n",
       " 580: 'murder',\n",
       " 581: 'david',\n",
       " 582: 'experience',\n",
       " 583: 'chance',\n",
       " 584: 'including',\n",
       " 585: 'stop',\n",
       " 586: 'simple',\n",
       " 587: 'wonder',\n",
       " 588: 'save',\n",
       " 589: 'looked',\n",
       " 590: 'interest',\n",
       " 591: 'whose',\n",
       " 592: 'group',\n",
       " 593: 'career',\n",
       " 594: 'happen',\n",
       " 595: 'complete',\n",
       " 596: 'none',\n",
       " 597: 'light',\n",
       " 598: 'number',\n",
       " 599: 'husband',\n",
       " 600: 'cut',\n",
       " 601: 'hero',\n",
       " 602: 'opening',\n",
       " 603: 'song',\n",
       " 604: 'running',\n",
       " 605: 'english',\n",
       " 606: 'gore',\n",
       " 607: 'score',\n",
       " 608: 'annoying',\n",
       " 609: 'king',\n",
       " 610: 'musical',\n",
       " 611: 'relationship',\n",
       " 612: 'serious',\n",
       " 613: 'possible',\n",
       " 614: 'ok',\n",
       " 615: 'across',\n",
       " 616: 'view',\n",
       " 617: 'usually',\n",
       " 618: 'released',\n",
       " 619: 'opinion',\n",
       " 620: 'yourself',\n",
       " 621: 'taken',\n",
       " 622: 'exactly',\n",
       " 623: 'shown',\n",
       " 624: 'hours',\n",
       " 625: 'sad',\n",
       " 626: 'cinematography',\n",
       " 627: 'please',\n",
       " 628: 'ago',\n",
       " 629: 'shots',\n",
       " 630: 'middle',\n",
       " 631: 'somewhat',\n",
       " 632: 'country',\n",
       " 633: 'seriously',\n",
       " 634: 'usual',\n",
       " 635: 'robert',\n",
       " 636: 'jokes',\n",
       " 637: 'taking',\n",
       " 638: 'started',\n",
       " 639: 'finds',\n",
       " 640: 'body',\n",
       " 641: 'crap',\n",
       " 642: 'reality',\n",
       " 643: 'hell',\n",
       " 644: 'cool',\n",
       " 645: 'change',\n",
       " 646: 'level',\n",
       " 647: 'ridiculous',\n",
       " 648: 'happy',\n",
       " 649: 'mostly',\n",
       " 650: 'scary',\n",
       " 651: 'saying',\n",
       " 652: 'novel',\n",
       " 653: 'ends',\n",
       " 654: 'huge',\n",
       " 655: 'wish',\n",
       " 656: 'order',\n",
       " 657: 'turned',\n",
       " 658: 'hilarious',\n",
       " 659: 'episodes',\n",
       " 660: 'call',\n",
       " 661: 'supporting',\n",
       " 662: 'ones',\n",
       " 663: 'words',\n",
       " 664: 'rating',\n",
       " 665: 'talking',\n",
       " 666: 'power',\n",
       " 667: 'important',\n",
       " 668: 'five',\n",
       " 669: 'rock',\n",
       " 670: 'female',\n",
       " 671: 'single',\n",
       " 672: 'jack',\n",
       " 673: 'room',\n",
       " 674: 'major',\n",
       " 675: 'british',\n",
       " 676: 'apparently',\n",
       " 677: 'local',\n",
       " 678: 'documentary',\n",
       " 679: 'four',\n",
       " 680: 'disappointed',\n",
       " 681: 'strange',\n",
       " 682: 'comic',\n",
       " 683: 'modern',\n",
       " 684: '7',\n",
       " 685: 'future',\n",
       " 686: 'earth',\n",
       " 687: 'due',\n",
       " 688: 'events',\n",
       " 689: 'cheap',\n",
       " 690: '8',\n",
       " 691: 'word',\n",
       " 692: 'basically',\n",
       " 693: 'non',\n",
       " 694: 'attention',\n",
       " 695: 'television',\n",
       " 696: 'talent',\n",
       " 697: 'knew',\n",
       " 698: 'aren',\n",
       " 699: 'thriller',\n",
       " 700: 'songs',\n",
       " 701: 'paul',\n",
       " 702: 'o',\n",
       " 703: 'easily',\n",
       " 704: 'clearly',\n",
       " 705: 'tells',\n",
       " 706: 'fast',\n",
       " 707: 'romantic',\n",
       " 708: 'problems',\n",
       " 709: 'appears',\n",
       " 710: 'oscar',\n",
       " 711: 'miss',\n",
       " 712: 'moving',\n",
       " 713: 'class',\n",
       " 714: 'bring',\n",
       " 715: 'upon',\n",
       " 716: 'theater',\n",
       " 717: 'giving',\n",
       " 718: 'similar',\n",
       " 719: 'silly',\n",
       " 720: 'george',\n",
       " 721: 'falls',\n",
       " 722: 'entertainment',\n",
       " 723: 'straight',\n",
       " 724: 'clich',\n",
       " 725: 'needs',\n",
       " 726: 'mystery',\n",
       " 727: 'beyond',\n",
       " 728: 'sets',\n",
       " 729: 'predictable',\n",
       " 730: 'lady',\n",
       " 731: 'knows',\n",
       " 732: 'ten',\n",
       " 733: 'review',\n",
       " 734: 'richard',\n",
       " 735: 'talk',\n",
       " 736: 'lee',\n",
       " 737: 'nearly',\n",
       " 738: 'eye',\n",
       " 739: 'enjoyable',\n",
       " 740: 'whether',\n",
       " 741: 'named',\n",
       " 742: 'within',\n",
       " 743: 'near',\n",
       " 744: 'tale',\n",
       " 745: 'stand',\n",
       " 746: 'message',\n",
       " 747: 'sequence',\n",
       " 748: 'french',\n",
       " 749: 'above',\n",
       " 750: 'working',\n",
       " 751: 'red',\n",
       " 752: 'points',\n",
       " 753: 'theme',\n",
       " 754: 'mention',\n",
       " 755: 'storyline',\n",
       " 756: 'add',\n",
       " 757: 'feels',\n",
       " 758: 'team',\n",
       " 759: 'sister',\n",
       " 760: 'herself',\n",
       " 761: 'york',\n",
       " 762: 'ways',\n",
       " 763: '9',\n",
       " 764: 'peter',\n",
       " 765: 'hate',\n",
       " 766: 'haven',\n",
       " 767: 'bunch',\n",
       " 768: 'begins',\n",
       " 769: 'effort',\n",
       " 770: 'surprised',\n",
       " 771: 'release',\n",
       " 772: 'lots',\n",
       " 773: 'e',\n",
       " 774: 'comments',\n",
       " 775: 'actual',\n",
       " 776: 'elements',\n",
       " 777: 'animation',\n",
       " 778: 'follow',\n",
       " 779: 'using',\n",
       " 780: 'somehow',\n",
       " 781: 'doubt',\n",
       " 782: 'typical',\n",
       " 783: 'viewers',\n",
       " 784: 'easy',\n",
       " 785: 'clear',\n",
       " 786: 'certain',\n",
       " 787: 'feature',\n",
       " 788: 'die',\n",
       " 789: 'stay',\n",
       " 790: 'material',\n",
       " 791: 'weak',\n",
       " 792: 'tried',\n",
       " 793: 'sorry',\n",
       " 794: 'sequel',\n",
       " 795: 'fall',\n",
       " 796: 'period',\n",
       " 797: 'means',\n",
       " 798: 'check',\n",
       " 799: 'soundtrack',\n",
       " 800: 'showing',\n",
       " 801: 'leads',\n",
       " 802: 'filmed',\n",
       " 803: 'figure',\n",
       " 804: 'dull',\n",
       " 805: 'among',\n",
       " 806: 'parents',\n",
       " 807: 'gone',\n",
       " 808: 'form',\n",
       " 809: 'de',\n",
       " 810: 'kept',\n",
       " 811: 'minute',\n",
       " 812: 'dialog',\n",
       " 813: 'buy',\n",
       " 814: 'brought',\n",
       " 815: 'hear',\n",
       " 816: 'editing',\n",
       " 817: 'avoid',\n",
       " 818: 'lame',\n",
       " 819: 'crime',\n",
       " 820: 'greatest',\n",
       " 821: '?',\n",
       " 822: 'viewing',\n",
       " 823: 'indeed',\n",
       " 824: 'japanese',\n",
       " 825: 'general',\n",
       " 826: 'zombie',\n",
       " 827: 'imagine',\n",
       " 828: 'fantastic',\n",
       " 829: 'eventually',\n",
       " 830: 'sequences',\n",
       " 831: 'joe',\n",
       " 832: 'space',\n",
       " 833: 'realistic',\n",
       " 834: 'believable',\n",
       " 835: 'particular',\n",
       " 836: 'famous',\n",
       " 837: 'atmosphere',\n",
       " 838: 'third',\n",
       " 839: 'poorly',\n",
       " 840: 'suspense',\n",
       " 841: 'season',\n",
       " 842: 'move',\n",
       " 843: 'reviews',\n",
       " 844: 'rent',\n",
       " 845: 'average',\n",
       " 846: 'tom',\n",
       " 847: 'disney',\n",
       " 848: 'baby',\n",
       " 849: 'stage',\n",
       " 850: 'difficult',\n",
       " 851: 'whatever',\n",
       " 852: 'sit',\n",
       " 853: 'learn',\n",
       " 854: 'street',\n",
       " 855: 'okay',\n",
       " 856: 'possibly',\n",
       " 857: 'jane',\n",
       " 858: 'decided',\n",
       " 859: 'dance',\n",
       " 860: 'forget',\n",
       " 861: 'note',\n",
       " 862: 'subject',\n",
       " 863: 'sexual',\n",
       " 864: 'wait',\n",
       " 865: 'dr',\n",
       " 866: 'truth',\n",
       " 867: 'deal',\n",
       " 868: 'emotional',\n",
       " 869: 'premise',\n",
       " 870: 'needed',\n",
       " 871: 'expected',\n",
       " 872: 'dog',\n",
       " 873: 'america',\n",
       " 874: 'screenplay',\n",
       " 875: 'romance',\n",
       " 876: 'meets',\n",
       " 877: 'killing',\n",
       " 878: 'nature',\n",
       " 879: 'imdb',\n",
       " 880: 'dramatic',\n",
       " 881: 'unless',\n",
       " 882: 'reading',\n",
       " 883: '20',\n",
       " 884: 'meet',\n",
       " 885: 'free',\n",
       " 886: 'nor',\n",
       " 887: 'male',\n",
       " 888: 'write',\n",
       " 889: 'writers',\n",
       " 890: 'became',\n",
       " 891: 'surprise',\n",
       " 892: 'directors',\n",
       " 893: 'acted',\n",
       " 894: 'leaves',\n",
       " 895: 'badly',\n",
       " 896: 'situation',\n",
       " 897: 'dream',\n",
       " 898: 'realize',\n",
       " 899: 'question',\n",
       " 900: 'society',\n",
       " 901: 'shame',\n",
       " 902: 'comment',\n",
       " 903: 'weird',\n",
       " 904: 'superb',\n",
       " 905: 'fantasy',\n",
       " 906: 'whom',\n",
       " 907: 'credits',\n",
       " 908: 'ask',\n",
       " 909: 'total',\n",
       " 910: 'sounds',\n",
       " 911: 'older',\n",
       " 912: 'directing',\n",
       " 913: 'interested',\n",
       " 914: 'forward',\n",
       " 915: 'forced',\n",
       " 916: 'brings',\n",
       " 917: 'worked',\n",
       " 918: 'otherwise',\n",
       " 919: 'footage',\n",
       " 920: 'box',\n",
       " 921: 'earlier',\n",
       " 922: 'result',\n",
       " 923: 'beauty',\n",
       " 924: 'open',\n",
       " 925: 'keeps',\n",
       " 926: 'air',\n",
       " 927: 'creepy',\n",
       " 928: 'ben',\n",
       " 929: 'sci',\n",
       " 930: 'personal',\n",
       " 931: 'mess',\n",
       " 932: 'towards',\n",
       " 933: 'perfectly',\n",
       " 934: 'leading',\n",
       " 935: 'fi',\n",
       " 936: 'crazy',\n",
       " 937: 'laughs',\n",
       " 938: 'hands',\n",
       " 939: 'casting',\n",
       " 940: 'previous',\n",
       " 941: 'plus',\n",
       " 942: 'memorable',\n",
       " 943: 'hot',\n",
       " 944: 'features',\n",
       " 945: 'deep',\n",
       " 946: 'c',\n",
       " 947: 'boys',\n",
       " 948: 'unique',\n",
       " 949: 'quickly',\n",
       " 950: 'christmas',\n",
       " 951: 'begin',\n",
       " 952: 'potential',\n",
       " 953: 'plenty',\n",
       " 954: 'setting',\n",
       " 955: 'various',\n",
       " 956: 'match',\n",
       " 957: 'appear',\n",
       " 958: 'apart',\n",
       " 959: 'admit',\n",
       " 960: 'twist',\n",
       " 961: 'cheesy',\n",
       " 962: 'rate',\n",
       " 963: 'bill',\n",
       " 964: 'development',\n",
       " 965: 'powerful',\n",
       " 966: 'mark',\n",
       " 967: 'incredibly',\n",
       " 968: 'fire',\n",
       " 969: 'business',\n",
       " 970: '30',\n",
       " 971: 'western',\n",
       " 972: 'telling',\n",
       " 973: 'nudity',\n",
       " 974: 'girlfriend',\n",
       " 975: 'effect',\n",
       " 976: 'present',\n",
       " 977: 'married',\n",
       " 978: 'dumb',\n",
       " 979: 'outside',\n",
       " 980: 'cop',\n",
       " 981: 'meant',\n",
       " 982: 'hardly',\n",
       " 983: 'fighting',\n",
       " 984: 'pay',\n",
       " 985: 'joke',\n",
       " 986: 'doctor',\n",
       " 987: 'return',\n",
       " 988: 'monster',\n",
       " 989: 'expecting',\n",
       " 990: '80',\n",
       " 991: 'portrayed',\n",
       " 992: 'background',\n",
       " 993: 'front',\n",
       " 994: 'attempts',\n",
       " 995: 'battle',\n",
       " 996: 'talented',\n",
       " 997: 'inside',\n",
       " 998: 'era',\n",
       " 999: 'sweet',\n",
       " ...}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vocabulary as a dictionary\n",
    "\n",
    "vocab_dict = {i:word for i,word in enumerate(vocab)}\n",
    "vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0918e2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '' is used to represent something which is not really a word. \n",
    "#...Like for example padding or masking. Its index is 0\n",
    "\n",
    "# [UNK]' is used to represent an Out of Vocabular token OOV token. Its index is typically 1\n",
    "# rest of the indices are used for all other words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a72b6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each review is standardized, tokenized, and convered into an integer sequence\n",
    "\n",
    "train_int = train_data.map(lambda x,y: (text_vectorization(x),y)) \n",
    "val_int = val_data.map(lambda x,y: (text_vectorization(x),y)) \n",
    "test_int = test_data.map(lambda x,y: (text_vectorization(x),y)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "317ac6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews shape (32, 300)\n",
      "Labels shape (32,) \n",
      "\n",
      "Encoded 7th Review: tf.Tensor(\n",
      "[ 855   23   28 6875 4650   15  711    1    1  485  485  773    1    5\n",
      " 1850   52   13  561  123    9    2   84  133   12   13  220 1652   11\n",
      "  121   47   61    1  764  702 8773   15 7112 3806    1   38    2   84\n",
      "  324    3  153    2    1 1514    5    2    1 9279    1   15    2    1\n",
      "  730    5    2    1  883  157 1135   73    2 3806    3  883  157  507\n",
      "   41  200 2456   19  130    1 8181 8578   24 1937  196    6   28    4\n",
      " 2940  424   22    2  493    8   13 2796   33   28    1    1   13  361\n",
      " 1810    3    1   33  229   58  266 2199   16    1  130   52   13    1\n",
      "    5    2 3806  196    6   28  301 2940  424   41  200  554   48    4\n",
      "    1  581    1   15  301 2940  424 1284   15    1    1   15   26 9198\n",
      "    1  421   21  212   56   17   99 1134   16   90  742    2   63   37\n",
      "   52  508   90    4    1    1  295   32    2 5508    3  411 2507    1\n",
      "   15  711    1    1    1    1    5    2    1  503 9844  390 4636 3255\n",
      "    9   41 1035   17    4 7686    5  945 2910 2043   16  113    9    2\n",
      "  979  178   43  164    1   41  118  224    5 2043   10  138   64   75\n",
      "    6    1    2  677    1  338    5 1850  283   10  193    8   14   53\n",
      "  317   19  780   24  173    5    2  146  178   72   69   51   21  134\n",
      "   33  159   21 3086  179 4090 2507   59   14  127  104   57   28   75\n",
      "  604    2  219  121   33   88   21  888 2898   39   12   99   55  261\n",
      " 1897   24 6208   37   17    4  177    3  954   39   11   23   28    2\n",
      "  156    1    5    1    1 3123 6948   33  546  132  292 2105  661  540\n",
      "   85   33 1478   37   72  100], shape=(300,), dtype=int64) \n",
      "\n",
      "Encoded 16th Review: tf.Tensor(\n",
      "[   8   14 2432    3 1357   48    4  436    5    4    1   10 3816    2\n",
      "  302  292  239  361 1493 1430    9 1070  237   11    7   24    4 2757\n",
      "   81   24   58  844    8 8750 1365   14   43   98  381    6   29  834\n",
      " 3217   14  614   43  614   10  408  793   16   90    2  273    9  412\n",
      "   77  378   11  931 3611   14    2  171   15   52  206    7 1207    3\n",
      "  999   19   17   61 1281    2  875   17  294   14  343 1302    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0], shape=(300,), dtype=int64) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lets look again at the first batch of data, now that reviews are converted into integers.\n",
    "\n",
    "for reviews, labels in train_int.take(1):\n",
    "    print (\"Reviews shape\", reviews.shape)\n",
    "    print (\"Labels shape\", labels.shape, \"\\n\")\n",
    "        \n",
    "    print ('Encoded 7th Review:', reviews[6], \"\\n\")    \n",
    "    print ('Encoded 16th Review:', reviews[15], \"\\n\")\n",
    "\n",
    "# Each batch has 32 reviews. Each review is converted into a 300 length integer sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01bd6da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zeros in the above review are for padding so that each review can reach max length= 300 words\n",
    "\n",
    "# 1's in above review represent unknown words. \n",
    "#...meaning these words in the review were not there in the vocabulary\n",
    "#.. since we have chosen max tokens = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2290fdb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 7th Review: tf.Tensor(\n",
      "[ 855   23   28 6875 4650   15  711    1    1  485  485  773    1    5\n",
      " 1850   52   13  561  123    9    2   84  133   12   13  220 1652   11\n",
      "  121   47   61    1  764  702 8773   15 7112 3806    1   38    2   84\n",
      "  324    3  153    2    1 1514    5    2    1 9279    1   15    2    1\n",
      "  730    5    2    1  883  157 1135   73    2 3806    3  883  157  507\n",
      "   41  200 2456   19  130    1 8181 8578   24 1937  196    6   28    4\n",
      " 2940  424   22    2  493    8   13 2796   33   28    1    1   13  361\n",
      " 1810    3    1   33  229   58  266 2199   16    1  130   52   13    1\n",
      "    5    2 3806  196    6   28  301 2940  424   41  200  554   48    4\n",
      "    1  581    1   15  301 2940  424 1284   15    1    1   15   26 9198\n",
      "    1  421   21  212   56   17   99 1134   16   90  742    2   63   37\n",
      "   52  508   90    4    1    1  295   32    2 5508    3  411 2507    1\n",
      "   15  711    1    1    1    1    5    2    1  503 9844  390 4636 3255\n",
      "    9   41 1035   17    4 7686    5  945 2910 2043   16  113    9    2\n",
      "  979  178   43  164    1   41  118  224    5 2043   10  138   64   75\n",
      "    6    1    2  677    1  338    5 1850  283   10  193    8   14   53\n",
      "  317   19  780   24  173    5    2  146  178   72   69   51   21  134\n",
      "   33  159   21 3086  179 4090 2507   59   14  127  104   57   28   75\n",
      "  604    2  219  121   33   88   21  888 2898   39   12   99   55  261\n",
      " 1897   24 6208   37   17    4  177    3  954   39   11   23   28    2\n",
      "  156    1    5    1    1 3123 6948   33  546  132  292 2105  661  540\n",
      "   85   33 1478   37   72  100], shape=(300,), dtype=int64) \n",
      "\n",
      "Decoded 7th Review: okay you have penelope keith as miss [UNK] [UNK] b b e [UNK] of england she s killed off in the first scene that s right folks this show has no [UNK] peter o toole as ol colonel [UNK] from the first war and now the [UNK] lord of the [UNK] joanna [UNK] as the [UNK] lady of the [UNK] 20 years younger than the colonel and 20 years past her own prime but still [UNK] brit spelling not mine enough to have a toy boy on the side it s alright they have [UNK] [UNK] s full knowledge and [UNK] they guy even comes round for [UNK] still she s [UNK] of the colonel enough to have said toy boy her own age what a [UNK] david [UNK] as said toy boy equally as [UNK] [UNK] as his squeeze [UNK] couldn t come up with any cover for him within the story so she gave him a [UNK] [UNK] job at the circus and finally susan [UNK] as miss [UNK] [UNK] [UNK] [UNK] of the [UNK] girls boarding school serving tea in her office with a dash of deep poignant advice for life in the outside world just before [UNK] her best bit of advice i ve only been to [UNK] the local [UNK] home of england once i thought it was very beautiful but somehow not part of the real world well we can t say they didn t warn us ah susan time was your character would have been running the whole show they don t write em like that any more our loss not yours so with a cast and setting like this you have the re [UNK] of [UNK] [UNK] right? wrong! they took these 1 dimensional supporting roles because they paid so well after \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lets see the encoded review and the review decoded back to text\n",
    "for reviews, labels in train_int.take(1):     \n",
    "    print ('Encoded 7th Review:', reviews[6], \"\\n\")    \n",
    "    \n",
    "    decoded_review =  \" \".join([vocab_dict[int_token] for int_token in reviews[6].numpy()])\n",
    "    print ('Decoded 7th Review:', decoded_review, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806996ce",
   "metadata": {},
   "source": [
    "## 1. Build the Model with One-hot Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd437772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the seeds for model initialization/training for reproducibility\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f35e495",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"./images/onehot2.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18069ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_units = 16\n",
    "\n",
    "def model_BiLSTM(): \n",
    "    # Define Input shape\n",
    "    inputs = keras.Input(shape = (max_length,), dtype=\"int64\") \n",
    "    \n",
    "    # each token in input sequence will be converted to one-hot vector\n",
    "    onehot = tf.one_hot(inputs, depth=max_tokens) \n",
    "    \n",
    "    # Bidirectional LSTM layer\n",
    "    x = layers.Bidirectional(layers.LSTM(hidden_units))(onehot)  \n",
    "\n",
    "    # Dropout Layer\n",
    "    x = layers.Dropout(0.5)(x) \n",
    "\n",
    "    # Dense output Layer\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x) \n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26a55858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 300)]             0         \n",
      "                                                                 \n",
      " tf.one_hot (TFOpLambda)     (None, 300, 10000)        0         \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 32)               1282176   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,282,209\n",
      "Trainable params: 1,282,209\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = model_BiLSTM()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d01988cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when we use bidirectional RNN or LSTM, we will have an extra weight matrices \n",
    "#...for the backward RNN or LSTM. \n",
    "# Extra W for x, a and bias term. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "22581f34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df4ff324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([10000, 64])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weights[0].shape # weight matrix that will be multiplied with Xt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a588db80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([16, 64])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weights[1].shape # weight matrix that will be multiplied with at-1 (activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6b40bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weights[2].shape # bias term ba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5409b48e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([10000, 64])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weights[3].shape # weight matrix that will be multiplied with Xt - for backward lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fbbed6d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([16, 64])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weights[4].shape # weight matrix that will be multiplied with at-1 (activation) - for backward lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cafbe858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weights[5].shape # bias term ba - for backward lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7505fb66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weights[6].shape # weight matrix that will be multiplied with at to calculate dense layer output (yt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dee4cc20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weights[7].shape # bia term by-  to calculate dense layer output (yt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56df0fbd",
   "metadata": {},
   "source": [
    "### Compile & Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b857896b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_BiLSTM()\n",
    "path = Path(\"./models/model_bilstm.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3410c4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will reuse this function to train and evaluate for convenience\n",
    "\n",
    "def train_evaluate(model,path,train,val,test):\n",
    "    \n",
    "    # Call backs\n",
    "    checkpoint_cb = keras.callbacks.ModelCheckpoint(filepath = path,\n",
    "                                                       save_best_only=True) # Save only best model\n",
    "    \n",
    "    earlystop_cb = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    callbacks = [checkpoint_cb,earlystop_cb]\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=\"rmsprop\", loss='binary_crossentropy',  metrics = [\"accuracy\"])\n",
    "    \n",
    "    #Train the model\n",
    "    history = model.fit(train, validation_data = val, callbacks=callbacks, epochs=20)\n",
    "    \n",
    "    #Evaluate the model on test data\n",
    "    test_loss, test_accuracy = model.evaluate(test)\n",
    "    \n",
    "    return (history,test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74003c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "(history_bilstm,test_accuracy_bilstm) = train_evaluate(model,path,\n",
    "                                                       train_int,\n",
    "                                                       val_int,\n",
    "                                                       test_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd2166b",
   "metadata": {},
   "source": [
    "**Plot History**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2764c6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history_bilstm,\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc12ddbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean absolute error with this simple 1 neuron dense model \n",
    "print (f\"Accuracy on the test data set is {test_accuracy_bilstm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaa282c",
   "metadata": {},
   "source": [
    "## 2. Build the Model with Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db373acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot vectors are sparse and very high dimensional - Vocab length\n",
    "# word embeddings can help with these in that they are more meaningul, and \n",
    "#....are typically smaller dimensions like 128, 256, 512\n",
    "\n",
    "# words with similar meaning have embedding vectors are that are closer and similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb2161a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the seeds for model initialization/training for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7466a44d",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"./images/wordembed3.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae58cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_units = 16\n",
    "\n",
    "def model_BiLSTM_wembeddings(): \n",
    "    \n",
    "    # Define Input shape\n",
    "    inputs = keras.Input(shape = (max_length,), dtype=\"int64\") \n",
    "    \n",
    "    # Each token in input sequence will be converted to a 128 dimensional word embedding vector.\n",
    "    embeddings = layers.Embedding(input_dim = max_tokens, output_dim=128, mask_zero=True)(inputs) \n",
    "     \n",
    "    # Bidirectional LSTM layer \n",
    "    x = layers.Bidirectional(layers.LSTM(hidden_units))(embeddings) \n",
    "\n",
    "    # Dropout Layer\n",
    "    x = layers.Dropout(0.5)(x) \n",
    "\n",
    "    # Dense output Layer\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x) \n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# what makes a good word embedding depends heavily on the particular task.\n",
    "# If we use an embedding layer like above, word embeddings are calculated automatically \n",
    "#...as part of learning process\n",
    "# mask_Zero = True, above ensures extra zeros added for padding shorter sequences are ignored "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddfd3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_BiLSTM_wembeddings()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460a6887",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c5c4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weights[0].shape # Word Embedding matrix weights that will be calculated as part of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4401bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weights[1].shape  # weight matrix that will be multiplied with Xt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a61ed20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weights[2].shape # weight matrix that will be multiplied with at-1 (activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f19a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weights[3].shape # bias term ba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98981a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weights[4].shape # weight matrix that will be multiplied with Xt -  backward lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf3bf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weights[5].shape # weight matrix that will be multiplied with at-1 (activation) - backward lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed9a7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weights[6].shape # bias term ba -  backward lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864746a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weights[7].shape # # weight matrix that will be multiplied with at to calculate dense layer output (yt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4591cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weights[8].shape # bias term by -  to calculate dense layer output (yt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4590540",
   "metadata": {},
   "source": [
    "### Compile & Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a81c06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_BiLSTM_wembeddings()\n",
    "path = Path(\"./models/model_bilstm_wembeddings.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fdf2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will reuse this function to train and evaluate for convenience\n",
    "def train_evaluate(model,path,train,val,test):\n",
    "    \n",
    "    #call backs\n",
    "    checkpoint_cb = keras.callbacks.ModelCheckpoint(filepath = path,\n",
    "                                                       save_best_only=True) # Save only best model\n",
    "    \n",
    "    earlystop_cb = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    callbacks = [checkpoint_cb,earlystop_cb]\n",
    "\n",
    "    #Compile the model\n",
    "    model.compile(optimizer=\"rmsprop\", loss='binary_crossentropy',  metrics = [\"accuracy\"])\n",
    "    \n",
    "    #Train the model\n",
    "    history = model.fit(train, validation_data = val, callbacks=callbacks, epochs=10)\n",
    "    test_loss, test_accuracy = model.evaluate(test)\n",
    "    \n",
    "    return (history,test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22078fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "(history_bilstm_wembeddings,test_accuracy_bilstm_wembeddings) = train_evaluate(model,path,train_int,val_int,test_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f044295",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f\"Accuracy on the test data set is {test_accuracy_bilstm_wembeddings}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d58c71c",
   "metadata": {},
   "source": [
    "## 3. Pretrained Word Embeddings (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f7e0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When we dont have enough training data to  learn task specific word embeddings\n",
    "# when we think pretrained word embeddings on some other task are fairly generic to apply to our task as well\n",
    "# similar to pretrained features in convnets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa85f3a2",
   "metadata": {},
   "source": [
    "# Many different pretrained word embeddings. some generic and some domain specific\n",
    "* Word2vec\n",
    "* Glove"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266d0209",
   "metadata": {},
   "source": [
    "### Download Pretrained Glove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633e3084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://nlp.stanford.edu/data/glove.6B.zip\n",
    "#unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cce2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "file_path = Path(\"./data/glove.6B/glove.6B.50d.txt\")\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "# Format the loaded word embeddings into an embedding index which will be a dictionary \n",
    "# whose keys will be words and values will be 50-dimensional wordembeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ada8816",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embeddings_index) # Pretrained word embeddings for around 400000 words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51257bb0",
   "metadata": {},
   "source": [
    "### Create Pretrained Glove Word Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f08f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the pretrain word embedding matrix from the embeddings_index. Dimensions will be of size (vocab_size,embedding_dim)\n",
    "input_dim = max_tokens\n",
    "output_dim = 50\n",
    "embedding_matrix = np.zeros((input_dim,output_dim))\n",
    "\n",
    "for i, word in enumerate(vocab):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60873b2c",
   "metadata": {},
   "source": [
    "### Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ad3f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_units = 16\n",
    "\n",
    "def model_BiLSTM_wembeddings_pretrained(): \n",
    "    \n",
    "    # Define Input shape\n",
    "    inputs = keras.Input(shape = (max_length,), dtype=\"int64\") \n",
    "    \n",
    "    # Each token in input sequence will be converted to a 50-dim pretrained word embedding vector. \n",
    "    embeddings =layers.Embedding(input_dim,output_dim,weights=[embedding_matrix],\n",
    "                                 trainable=False, mask_zero=True)(inputs) \n",
    "    \n",
    "    # Bidirectional LSTM layer \n",
    "        x = layers.Bidirectional(layers.LSTM(hidden_units))(embeddings) \n",
    "\n",
    "    # Dropout Layer\n",
    "    x = layers.Dropout(0.5)(x) \n",
    "\n",
    "    # Dense output Layer\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x) \n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d326b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_BiLSTM_wembeddings_pretrained()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10235c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will reuse this function to train and evaluate for convenience\n",
    "def train_evaluate(model,path,train,val,test):\n",
    "    \n",
    "    #call backs\n",
    "    checkpoint_cb = keras.callbacks.ModelCheckpoint(filepath = path,\n",
    "                                                       save_best_only=True) # Save only best model\n",
    "    \n",
    "    earlystop_cb = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, \n",
    "                                                 restore_best_weights=True)\n",
    "    callbacks = [checkpoint_cb,earlystop_cb]\n",
    "\n",
    "    #Compile the model\n",
    "    model.compile(optimizer=\"rmsprop\", loss='binary_crossentropy',  metrics = [\"accuracy\"])\n",
    "    \n",
    "    #Train the model\n",
    "    history = model.fit(train, validation_data = val, callbacks=callbacks, epochs=10)\n",
    "    test_loss, test_accuracy = model.evaluate(test)\n",
    "    \n",
    "    return (history,test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40824c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(history_bilstm_wembeddings_pt,test_accuracy_bilstm_wembeddings_pt) = train_evaluate(model,path,\n",
    "                                                                                     train_int,\n",
    "                                                                                     val_int,test_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f352205e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f\"Accuracy on the test data set is {test_accuracy_bilstm_wembeddings_pt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9ec99b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
